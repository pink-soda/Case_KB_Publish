[
  {
    "content": "改造 Dify 实现生产可用的\nAI Agent 应用落地\n陈迪豪 | 顺丰科技\n演讲嘉宾\n陈迪豪\n顺丰科技 AI 技术平台高级工程师\n目前担任顺丰科技 AI 技术平台高级工程师，负责顺丰集团 AI 和大模型\n基础架构功能，曾任第四范式平台架构师和 OpenMLDB 项目 PMC，过\n去在小米担任云深度学习平台架构师以及优思德云计算公司存储和容器团\n队负责人。活跃于分布式系统、机器学习相关的开源社区，也是HBase、\nOpenStack、TensorFlow、TVM 等开源项目贡献者。\n目 录\nCONTENTS\n1.\n介绍 Dify 开发平台\n2.\n改造 Dify 开发平台\n3.\n落地软件机器人 Agent 场景\n4.\n落地数据平台 Agent 场景\n5.\nDify 实践总结\n介绍 Dify 开发平台\nPART 01\n介绍 Dify 开发平台 - What\n• 简介：Dify是一个低代码AI平台，旨在帮助用户更快捷地构建AI驱动的应用程序。\n• 特点：模块化设计、易于集成、提供多种AI服务接口。\n• 优势：低代码开发、高效部署、面向业务需求的AI支持。\n介绍 Dify 开发平台 - Why\n1. 低代码开发，缩短开发周期\n•\nDify支持低代码开发，用户只需少量编码即可构建复杂的AI应用，极大地缩短了开发周期。\n•\n可视化的界面设计和API集成工具使得开发流程更为简洁，即便没有深厚的编程背景也可以快速上手。\n2. 模块化架构，支持灵活扩展\n•\nDify的模块化设计使平台更具扩展性，用户可以按需选择和组合AI模块，适应各种应用场景。\n•\n平台支持不同AI模型的接入和切换，能满足不同类型的任务需求，如文本处理、图像分析、数据预测等。\n3. 高效部署，快速上线\n•\nDify集成了简化的部署工具，使得AI应用可以更快速地上线。\n•\n提供云端部署和本地部署选项，支持企业根据需求灵活选择部署环境，有效应对数据安全和隐私要求。\n4. 多模型支持，满足多样化需求\n•\nDify兼容主流的AI模型，并支持自定义模型的集成，用户可以在平台内调取不同的模型服务，以实现从数据分析到自然语言处理的多样化需求。\n•\n模型的灵活替换和升级使平台更具适应性和前瞻性。\n5. 节省成本，提高效率\n•\n低代码的开发模式和便捷的模型管理工具大幅降低了开发和维护成本。\n•\n通过自动化流程和AI驱动的智能应用，Dify帮助企业提高了工作效率，减少了人工干预，节省了运营成本。\n介绍 Dify 开发平台 - How\n介绍 Dify 开发平台 - But？\n• 元数据存储实现方式单一\n• RAG能力有限，不支持最新检索算法\n• 向量数据库支持不全\n• 原生部署不支持高可用\n• 不支持删除 message 接口\n• 社区版不支持模型服务负载均衡\n• ……\n介绍 Dify 开发平台 - 顺丰集成版\n改造 Dify 开发平台\nPART 02\n改造 Dify 开发平台 - 部署增强\nDify\nPostgreSQL\nDify\nPostgreSQL\nMySQL\n(新增 MySQL 配置项，需改数\n据库和表结构，SQL语句修改)\n改造 Dify 开发平台 - 部署增强\nDify\nRedis\nDify\nRedis Sentinel\nRedis Sentinel\nRedis Sentinel\n改造 Dify 开发平台 - 检索增强\n• 新增 ElasticSearch 8支持，支持混合检索\n• 新增 ES 相关配置，支持 ES Vector 组件\n• 支持 Contextual Retrieval (#8776)\n• 支持 GraphRAG / LightRAG (#6019)\n改造 Dify 开发平台 - 集成内部服务\n集成私有化大模型\n• 开源大模型\n• 微调领域模型\n• 商业模型服务\nDify 内置工具\n• 向量混合检索工具\n• 数据平台表接口查询服务\nDify 接口改造\n• 修改 LLM 节点输入获取方法\n• 新增删除 messages API\n改造 Dify 开发平台 - Ongoing\n• 更多 RAG 算法集成：GraphRAG、LightRAG 等\n• 多 Agent 调度系统集成：OpenAI Swarm、MetaGPT 等\n• 生产特性开发：高可用、负载均衡的模型服务支持等\n• 产品化集成：对标 HiAgent 和 Coze 等商业产品\n• 更多应用和流程集成：\n‣  数据增强工作流\n‣  全平台软件助手\n‣  混合云推理服务\n‣  ……\n落地软件机器人 Agent 场景\nPART 03\n落地软件机器人 Agent 场景 - 背景\nRPA（Robotic Process Automation，机器人流程自动化）是一种\n通过软件机器人（bot）来模拟人类用户在应用程序中执行规则\n化任务的技术，旨在简化流程、提高效率并减少人为错误。随着\n大语言模型（LLM）的快速发展，将 RPA 与大模型集成，能够\n进一步扩展其应用场景，实现更智能的自动化。\n落地软件机器人 Agent 场景 - RPA vs Workflow\n特性\nRPA (机器⼈流程⾃动化)\n⼯作流 (Workflow)\n定义\n模拟⼈类⽤户操作的⾃动化技术\n⼀系列任务或活动的⾃动化处理\n应⽤场景\n⾼度重复、标准化的任务，如数据输⼊和迁移\n复杂的业务流程，如审批流程、项⽬管理\n技术实现\n通过软件机器⼈与⽤户界⾯交互\n通过规则和逻辑构建⾃动化流程\n灵活性与可扩展性\n灵活性较低，主要适⽤于标准化任务\n更具灵活性和可扩展性，适应动态业务需求\n⽤户参与\n通常是完全⾃动化，⽤户⼲预较少\n涉及多⽅参与者，⽀持协作和沟通\n监控与分析\n关注机器⼈的性能和执⾏状态\n提供全⾯的监控和分析能⼒，对整个业务流程进⾏分析\n集成能⼒\n能够集成现有应⽤程序和系统，不需要重构\n通常需要与业务系统、数据库、API 等集成\n落地软件机器人 Agent 场景 - RPA with Dify\n• 相比于传统工作流，Dify 大模型能力可以实现意图识别、代码生成等功能\n• 通过知识库引入让大模型生成结果更加可用\n• 通过微调模型让意图识别准确率和性能更好\n• 通过流程拆解、步骤拆解、代码生成等多步实现人工参与调优\n• 最终实现端到端自然语言到 RPA 流程的 AI 应用落地\n落地软件机器人 Agent 场景 - RPA with Dify\n• 知识库搭建，引入 RPA 领域知识\n落地软件机器人 Agent 场景 - RPA with Dify\n• 多场景工作流，实现多步任务拆解\n落地数据平台 Agent 场景\nPART 04\n落地数据平台Agent场景 - 背景\n1. 数据处理与分析\n• 数据清洗与预处理：利用 Dify 的 RAG（检索增强\n生成）功能，从各种文档格式（如 PDF 和 PPT）中\n提取文本，进行数据清洗和预处理。 \n• 数据分析与报告生成：通过 Dify 的模型集成和提示\n词设计，自动分析大规模数据集，生成结构化的分\n析报告，辅助决策。\n2. 智能问答与知识库构建\n• 企业知识库问答：将企业内部知识库与 Dify 集成，\n构建智能问答系统，提升员工和客户的查询效率。 \n• 客户服务自动化：利用 Dify 的 AI 智能体功能，自\n动处理客户咨询，提供实时、准确的服务。\n3. 数据驱动的业务流程自动化\n• 自动化决策支持：结合 Dify 的 AI 智能体和工作流\n编排功能，实现基于数据的自动化决策流程，提升\n业务效率。\n• 业务流程优化：通过 Dify 的可视化工具，分析和\n优化业务流程，减少人工干预，降低成本。\n4. 多模态数据处理\n• 文本与图像数据融合：利用 Dify 的模型支持，处\n理文本和图像等多模态数据，实现综合分析。\n• 多语言数据处理：Dify 支持多语言模型，适用于全\n球化业务的数据处理需求。\n落地数据平台Agent场景 - 智能错误归因\n• 智能错误归因分析\n落地数据平台Agent场景 - 智能错误归因\n落地数据平台Agent场景 - SQL 生成和自动查表\nSQL 生成\n• 自然语言转 SQL：用户只需以自然语言描述所需的数据查询，大模型会解析该描述并生成相应的 SQL 查询。\n• 复杂查询支持：大模型能够处理复杂的查询逻辑，包括多表联接、聚合函数、条件过滤等，用户只需提供业\n务逻辑即可。\n• 动态查询生成：根据输入的参数动态生成 SQL 查询，支持灵活的查询需求，如排序、分组和限制结果集。\n自动查表功能\n• 表结构理解：大模型可以解析数据库的表结构信息（如表名、字段名及其数据类型），帮助用户更好地理解\n数据库 schema。\n• 海量表查询：根据数据库的表属性，从数十万的表中查找业务关联的表内容，通过粗排和精排实现查表准确\n率和召回率。\n• 智能建议：在用户输入查询时，模型可以根据当前数据库结构提供相关表和字段的智能建议，提高查询的准\n确性和效率。\n落地数据平台Agent场景 - SQL 生成和自动查表\nDify 实践总结\nPART 05\nDify 实践总结\n• Dify 在顺丰内部有海量的应用场景（数百个在线应用，几十万对话数）\n开发流程\n未使⽤ Dify 平台\n使⽤ Dify 平台\n性能提升\n开发应⽤前&后端\n集成和封装 LLM 能⼒，花费较多时间开发前端应⽤\n直接使⽤ Dify 的后端服务，可基于 WebApp 脚⼿架开发\n80%\nPrompt Engineering\n仅能通过调⽤ API 或 Playground 进⾏\n结合⽤户输⼊数据所⻅即所得完成调试\n25%\n数据准备与嵌⼊\n编写代码实现⻓⽂本数据处理、嵌⼊\n在平台上传⽂本或绑定数据源即可\n80%\n应⽤⽇志与分析\n编写代码记录⽇志，访问数据库查看\n平台提供实时⽇志与分析\n70%\n数据分析与微调\n技术⼈员进⾏数据管理和创建微调队列\n⾮技术⼈员可协同，可视化模型调整\n60%\nAI 插件开发与集成\n编写代码创建、集成 AI 插件\n平台提供可视化⼯具创建、集成插件能⼒\n50%\nDify 实践总结\n• 未来将集成顺丰统一的模型广场\n• 通过插件增加顺丰内部所有 API 市场\n• 打破公有云和私有云限制，充分利用 vGPU 池化技术\n• 多 Agent 调度支持，尤其是 OpenAI Swarm 深度集成\n• ……\n利用AI技术深化计算机对现实世界的理解\n推动研发进入智能化时代\n",
    "file_path": "./emails\\CAS-11358-S054FG.pdf",
    "classification": {
      "level1": "软件问题",
      "level2": "平台改造",
      "level3": "数据库集成和检索增强"
    },
    "category_score": {
      "level1": 0.0,
      "level2": 0.0,
      "level3": 0.0
    },
    "reasoning": "该案例涉及对Dify开发平台的改造，包括数据库集成、检索增强、内部服务集成等多个方面，属于软件问题中的平台改造范畴。具体的技术问题包括数据库集成（如新增MySQL配置）、检索增强（如新增ElasticSearch支持）等，因此归类为数据库集成和检索增强。"
  },
  {
    "content": "1.OZON 账户注册\n打开浏览器，在浏览器输入https://seller.ozon.ru/ch\n2.转到登录页面，并单击注册。\n3.输入电话号码\n4.输入邮箱，然后单击获取验证码。将收到发送的验证码。复制验证码并粘贴到相应的字\n段。\n5.注册国家选择中国\n6. 填写完整的信息( 可以去企查查复制营业执照信息）\n7.上传营业执照扫描件\n8.绑定实名通过的连连\n资料没有问题联系连连经理反馈加急当天下店\n",
    "file_path": "./emails\\CAS-12551-Y9S4X3.pdf",
    "classification": {
      "level1": "软件问题",
      "level2": "账户注册问题",
      "level3": "OZON平台账户注册"
    },
    "category_score": {
      "level1": 0.95,
      "level2": 0.0,
      "level3": 0.0
    },
    "reasoning": "该案例描述了在OZON平台上进行账户注册的具体步骤，涉及到浏览器操作、电话号码和邮箱的输入、验证码的获取和输入、国家选择、信息填写和营业执照上传等内容。这些操作都属于软件使用过程中的问题，因此归类为软件问题。现有分类结构中没有适合的二级和三级分类，因此创建新的二级分类'账户注册问题'和三级分类'OZON平台账户注册'。"
  },
  {
    "content": "Building Knowledge \nGraph RAG with \nLlamaIndex\n使用 LlamaIndex 构建\n知识图谱 RAG\nBasic Steps for Building the \nKnowledge Graph\n在 LlamaIndex 中构建知识图谱 RAG \n的基本步骤\nImport Packages(导入软件包)\nLoad documents(加载文件)\nConfigure service_context and \nstorage_context(配置服务上下文和存储上下文)\nservice_context\nstorage_context\nSimpleGraphStore\nNeo4jGraphStore\nNebulaGraphStore\n本期焦点\nCreate Index(创建索引)\nCreate query_engine(创建查询引擎)\nQuery with query_engine(使用查询引擎进行查\n询)\nExtending the Knowledge Graph \nRAG based on graph-store\n基于graph-store扩展知识图谱RAG\nSimpleGraphStore\ngraph_store = SimpleGraphStore() \nCode + Doc\n\nNeo4jGraphStore\nCreate Neo4jGraph Database\n\nReference Notebook\n\nDoc\n\nNebulaGraphStore\nCreate NebulaGraph Database\n\nReference Notebook\n\nDoc\n\n代码实现\nOfficial Github官方\n\n可乐i_Klay版本\n\n",
    "file_path": "./emails\\CAS-12579-S0T2G6.pdf",
    "classification": {
      "level1": "软件问题",
      "level2": "平台改造",
      "level3": "知识图谱构建与查询"
    },
    "category_score": {
      "level1": 1.0,
      "level2": 0.9,
      "level3": 0.0
    },
    "reasoning": "邮件内容主要涉及使用 LlamaIndex 构建知识图谱 RAG 的步骤和相关配置，属于软件问题中的平台改造类问题。现有分类中没有具体的‘知识图谱构建与查询’分类，因此需要在三级分类中新增该分类。"
  },
  {
    "content": "88-NETLOGON\n1.\nNETLOGON DNS NETLOGONNETLOGON\n2.\nNETLOGON\nPass-Through\nNETLOGON\nPass-Through\n,NETLOGONDC\nDC\nNETLOGON\nNETLOGON\nNETLOGONDC\nNETLOGONNETLOGONADADNETLOGONWindows 10 30NETLOGONNETLOGON3030NETLOGONAD\nCMGE V2020-LNETLOGON\nCMGE V2020-LCMGE V2020-L\nNetlogon\nCMGE V2020-L\nCMGE V2020-L30NETLOGON\nNETLOGON\nNetlogon\n “Netlogon Remote Protocol” “The Netlogon Remote Protocol is a remote procedure call (RPC) interface that is used for user and \nmachine authentication on domain-based networks.” Netlogon/\n3.\nNetlogon Remote Protocol: https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-nrpc/ff8f970f-3e37-40f7-bd4b-\naf7336e4792f\nMachine Account Password Process: \n \nhttps://docs.microsoft.com/zh-cn/archive/blogs/askds/machine-account-password-process-2\n",
    "file_path": "./emails\\CAS-12653-SSDFB6.pdf",
    "classification": {
      "level1": "软件问题",
      "level2": "网络问题",
      "level3": "Netlogon服务问题"
    },
    "category_score": {
      "level1": 1.0,
      "level2": 0.0,
      "level3": 0.0
    },
    "reasoning": "邮件内容中反复提到Netlogon服务相关问题，属于软件问题中的网络问题范畴。现有分类中没有匹配的二级和三级分类，因此创建新的二级和三级分类。"
  },
  {
    "content": "2024-9-20\nTraining Language Models to Self-Correct via\nReinforcement Learning\nAviral Kumar*+,1, Vincent Zhuang*+,1, Rishabh Agarwal*,1, Yi Su*,1, JD Co-Reyes1, Avi Singh1, Kate Baumli1,\nShariq Iqbal1, Colton Bishop1, Rebecca Roelofs1, Lei M Zhang1, Kay McKinney1, Disha Shrivastava1, Cosmin\nPaduraru1, George Tucker1, Doina Precup1, Feryal Behbahani†,1 and Aleksandra Faust†,1\n1Google DeepMind, *Equal Contribution, +Randomly ordered via coin flip, †Jointly supervised.\nSelf-correction is a highly desirable capability of large language models (LLMs), yet it has consistently\nbeen found to be largely ineffective in modern LLMs. Existing approaches for training self-correction either\nrequire multiple models or rely on a more capable model or other forms of supervision. To this end, we\ndevelop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an\nLLM’s self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants\nof supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling\nself-correction behavior. In particular, we observe that training via SFT either suffers from a distribution\nmismatch between the training data and the model’s own responses or implicitly prefers only a certain mode\nof correction behavior that is often not effective at test time. SCoRe addresses these challenges by training\nunder the model’s own distribution of self-generated correction traces and using appropriate regularization\nto steer the learning process into learning a self-correction strategy that is effective at test time as opposed\nto simply fitting high-reward responses for a given prompt. This regularization prescribes running a first\nphase of RL on a base model to generate a policy initialization that is less susceptible to collapse and then\nusing a reward bonus to amplify self-correction during training. When applied to Gemini 1.0 Pro and 1.5\nFlash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base\nmodels’ self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks.\n1. Introduction\nLarge language models (LLMs) have proven to be a useful tool in reasoning and scientific domains such\nas mathematical problem-solving and coding (Lozhkov et al., 2024; Shao et al., 2024; Team, 2024). An\naspirational property of LLMs in such settings is to able to implement algorithms: strategies that help\nthe LLM to use computation and interaction to improve its response on the test-time query. Modern\nLLMs largely do not implement algorithms reliably: for instance, consider a problem setting that requires\nmodels to detect and revise (or “self-correct”) their own responses to a given test-time query, so as to\nbe able to eventually arrive at the best-possible final response. This sort of self-correction capability has\nbeen shown by several recent works to be severely lacking in current LLMs, especially in the absence of\nexternal input (also referred to as intrinsic self-correction) (Huang et al., 2023; Kamoi et al., 2024).\nTo make progress towards the eventual goal of teaching LLMs to implement algorithms to handle\nchallenging inputs, we study a special instance of training LLMs to implement self-correction strategies\nto fix their mistakes “on-the-fly”. This should be possible: on many queries where current LLMs fail,\nthey still contain the underlying “knowledge” needed to arrive at the correct response but are unable\nto correctly elicit and draw inferences about their own knowledge when needed (Snell et al., 2024).\nFor example, strong LLMs can often successfully complete a sub-part of a math proof when prompted\nwith the remainder, but may not be able to complete it from scratch. In a similar vein, leveraging their\nprevious responses should, in principle, enable LLMs to improve their subsequent ones. Nevertheless,\nCorresponding author(s): [vincentzhuang, aviralkumar, rishabhagarwal, yisumtv]@google.com\narXiv:2409.12917v1  [cs.LG]  19 Sep 2024\nTraining Language Models to Self-Correct via Reinforcement Learning\nBase Model\nSTaR\nSFT\nSCoRe\n40\n45\n50\n55\n60\n65\n70\nTest Accuracy (%)\n-11.2%\n+0.4%\n+1.8%\n+4.4%\nGemini 1.5 Flash: MATH\nDirect Generation\nSelf-Correction (SC)\n (SC - Direct)\n21\n22\n23\n24\n25\nNumber of samples (K)\n60%\n62%\n64%\n66%\n68%\n70%\nSelf-Consistency@K\nScaling Inference Compute: MATH\nParallel Samples\nSequential (Self-Correct)\nFigure 1 ∣Left: SCoRe achieves state-of-the-art self-correction performance on MATH; Right: SCoRe inference-time scaling:\nspending samples on sequential self-correction becomes more effective than only on parallel direct samples (Section 6.2).\nself-correction has remained elusive, highlighting the need for going beyond existing training paradigms.\nHow can we instill LLMs with self-correction abilities? Prior attempts toward self-correcting LLMs either\nrely on prompt-engineering (Kim et al., 2023; Madaan et al., 2023) or fine-tuning models specifically for\nself-correction (Havrilla et al., 2024b; Qu et al., 2024; Welleck et al., 2023; Yuan et al., 2024). While the\nformer class of approaches often fail to effectively perform meaningful intrinsic self-correction, existing\nfine-tuning based approaches require running multiple models upon inference, e.g., a separate verifier or\nrefinement model (Havrilla et al., 2024b; Welleck et al., 2023), or require oracle “teacher” supervision to\nguide the process of self-correction (Qu et al., 2024), without which self-correction does not necessarily\noutperform independent uncorrelated attempts at the problem. We develop an approach that is effective\nat self-correction without any of the aforementioned requirements. Our approach, Self-Correction via\nReinforcement Learning (SCoRe), trains only a single model that can both produce a response to a\nreasoning problem and also correct errors despite not receiving any oracle feedback. More importantly,\nSCoRe teaches this ability to models entirely by training on self-generated data, without any oracle.\nWe begin by studying the failure modes of existing fine-tuning based strategies in this setting. We observe\nthat running supervised fine-tuning on multi-turn self-correction traces coupled with rejection sampling\n(i.e., a “multi-turn” variant of STaR (Zelikman et al., 2022)) often amplifies the model’s bias to not\nmake any error corrections. A minimal edit strategy appears somewhat optimal as it inhibits the model\nfrom learning to make correct responses worse in the second attempt, even though it does not instill\nself-correction abilities to the model. If the training dataset for SFT is altered to explicitly down-weight\ncertain correction traces that only make minor edits, then the resulting training is able to avoid collapse.\nHowever, it suffers from the curse of distributional shift: a correction strategy learned by training on\noff-policy data does not necessarily enable the model to be succeed at correcting its own mistakes.\nHow does SCoRe work? SCoRe addresses the aforementioned challenges with SFT by utilizing online\nmulti-turn reinforcement learning (RL). Concretely, SCoRe runs multi-turn RL on self-generated data to\navoid challenges with distribution mismatch between training and inference. To avoid the failure mode\nof learning a minimal edit strategy when training on on-policy data, we train SCoRe in two stages, with\neach stage regularizing the learning process to not collapse its behavior. The first stage replaces SFT\nin conventional LLM fine-tuning workflows by training a model initialization that optimizes correction\n2\nTraining Language Models to Self-Correct via Reinforcement Learning\nperformance while constraining the first attempt to be close to the base model. The second stage runs\nmulti-turn RL to optimize reward at both attempts, while using a reward bonus term that encourages\nimproving responses from the first attempt to the second. Both the initialization and the reward bonus\nensure that the model cannot simply learn to produce the best first-attempt response and only minorly\nedit it. Overall, SCoRe is able to elicit knowledge from the base model to enable positive self-correction.\nOur main contribution is SCoRe, a multi-turn RL approach for teaching LLMs how to correct their own\nmistakes. To the best of our knowledge, SCoRe is the first approach to attain significantly positive\nintrinsic self-correction: relative to base Gemini models, our method attains an absolute 15.6% gain\non self-correction for reasoning problems from MATH (Hendrycks et al., 2021) and an absolute 9.1%\ngain on coding problems from HumanEval (Chen et al., 2021). We additionally motivate the design of\nSCoRe by extensively studying the failure modes of baseline approaches, which broadly indicate that\nreinforcement learning may play an essential role in self-learned self-correction.\n2. Related Work\nPrior works study self-correction for LLMs under a variety of assumptions and problem settings. The\nmost prominent problem settings include problems where external input tokens from an environment is\navailable, for e.g., agentic tasks (Liu et al., 2023), code repair (Jain et al., 2024), and tool use (Chen\net al., 2023). While self-correction with external feedback is possible with strong proprietary models (Pan\net al., 2023), even the strongest models struggle in the substantially more challenging setting when no\nexternal input is available (Kamoi et al., 2024). This setting is called intrinsic self-correction. Prior work\nthat attempts to amplify intrinsic correction abilities are largely based on prompting and fine-tuning.\nPrompting for intrinsic self-correction. Recent work demonstrates that LLMs struggle to self-correct\ntheir reasoning errors without external feedback and naïvely running self-correction can degrade perfor-\nmance (Huang et al., 2023; Qu et al., 2024; Tyen et al., 2024; Zheng et al., 2024). These experimental\nstudies are at odds with prior work (Kim et al., 2023; Madaan et al., 2023; Shinn et al., 2023) and largely\nstem from mismatched assumptions on the setting (Kamoi et al., 2024). For example, Kim et al. (2023);\nShinn et al. (2023) use oracle ground-truth answers during self-correction that may not be available\ngenerally. Madaan et al. (2023) use weak prompts for initial responses, thereby perhaps overestimate the\nimprovement possible by self-correction. This indicates that there is no major work showing successful\nintrinsic self-correction via prompting alone. In the context of code self-repair, Olausson et al. (2023)\nshow that even when strong models are prompted with some form of partial feedback, e.g., showing\ntest-cases but not the desired outcomes on those test-cases, they are often unable to correct their mistakes.\nSampling multiple responses in parallel attains much better results in Olausson et al. (2023).\nFine-tuning for intrinsic self-correction. To address the issues with prompting off-the-shelf models\nalone, several works run supervised fine-tuning (SFT) or weighted SFT on the LLM to generate a revision\ngiven an initial response. Nonetheless typical works in this literature rely on oracle feedback: e.g.,\nobtaining revisions directly from human annotators (Saunders et al., 2022) or stronger models (Qu\net al., 2024; Ye et al., 2023). Our work aims to train for self-correction entirely without the use of bigger\nmodels or humans, when the learner itself is asked to generate its own training data. Similar to these\nprior works, we assume access to a reward function for evaluating model-generated outputs, similar\nto Akyürek et al. (2023); Welleck et al. (2023); Zhang et al. (2024). Other approaches build pipelines\nwith multiple models for self-correction (e.g., GLoRE (Havrilla et al., 2024b), Self-correct (Welleck et al.,\n2023), or Akyürek et al. (2023); Paul et al. (2023)). While this can lead to good results, these pipelines\ndo not quite tackle self-correction and require system design for serving multiple models at deployment.\n3\nTraining Language Models to Self-Correct via Reinforcement Learning\nProblem. If 𝑛≡2 (mod 7), then find the remain-\nder when (𝑛+ 2)(𝑛+ 4)(𝑛+ 6) is divided by 7.\nSCoRe turn 1 solution (incorrect). Since 𝑛≡2\n(mod 7), we can write 𝑛= 7𝑘+2 for some integer\n𝑘. Substituting, we get\n(𝑛+ 2)(𝑛+ 4)(𝑛+ 6) = (7𝑘+ 4)(7𝑘+ 6)(7𝑘+ 8)\n≡4 ⋅6 ⋅8\n(mod 7)\n≡192\n(mod 7)\n≡1\n(mod 7).\nFinal Answer: The final answer is 1. I hope it is\ncorrect.\nSelf-correction instruction. There might be an\nerror in the solution above because of lack of un-\nderstanding of the question. Please correct the\nerror, if any, and rewrite the solution.\nSCoRe turn 2 solution (correct). Since 𝑛≡2\n(mod 7), we can write 𝑛= 7𝑘+2 for some integer\n𝑘. Substituting, we get\n(𝑛+ 2)(𝑛+ 4)(𝑛+ 6) = (7𝑘+ 4)(7𝑘+ 6)(7𝑘+ 8)\n≡4 ⋅6 ⋅8\n(mod 7)\n≡4 ⋅6 ⋅1\n(mod 7)\n≡24\n(mod 7)\n≡3\n(mod 7).\nFinal Answer: The final answer is 3. I hope it is\ncorrect.\nFoundation model\nFoundation model trained with ScoRe\nprompt (x)\nresponse (y)\nfirst attempt (y1)\nImproved \nresponse (y2)\nprompt (x)\nOptional external \ninput (p) \nStandard training\nTraining via SCoRe (Ours)\nLoss\nLoss\nThe problem setting of self-correction. SCoRe trains a\nmodel to not just produce the best possible response, but\ninstead aims to train the model to produce the best final\nresponse in the final attempt. In the second turn, extra\ninput in the form of an instruction asking the model to\ncorrect itself or model-generated may be provided.\nFigure 2 ∣An example trace and the problem setting of self-correction.\nMulti-turn RL for LLMs. Our approach utilizes a multi-turn policy gradient approach for training for\nself-correction, which extends the single-turn approach of Ahmadian et al. (2024) and can be viewed\nas an instantiation of the hierarchical RL framework from Zhou et al. (2024). Generally, prior work at\nthe intersection of LLMs and multi-turn RL builds value-based (Farebrother et al., 2024; Shani et al.,\n2024; Snell et al., 2022; Zhou et al., 2024), policy-based (Shao et al., 2024; Xiong et al., 2024), and\nmodel-based (Hong et al., 2024) approaches. While this line of work builds machinery to do RL (i.e.,\noptimize rewards) in a multi-turn Markov decision process (MDP), our primary contribution in this paper\nis to devise a formalization, for learning self-correction behavior instead of the RL machinery itself.\nSelf-correction with external feedback. Many works study self-correction with additional feedback from\nthe environment, most commonly in the setting of code generation, where unit test results or compiler\nexecution feedback are available (Chen et al., 2024; Jain et al., 2024; Olausson et al., 2023). Largely\nthese works prompt models to reason about code execution; Ni et al. (2024) propose a self-training\nmethod that leverages execution traces, though only evaluate it on correcting a fixed dataset of errors.\n4\nTraining Language Models to Self-Correct via Reinforcement Learning\n3. Preliminaries and Problem Setup\nOur goal is to develop an approach for training LLMs to improve their own predictions by entirely\ntraining on self-generated data. As discussed so far, we situate ourselves in the intrinsic self-correction\nsetting (Huang et al., 2023), where models attempt to correct their initial responses without any external\nfeedback. Concretely, given a dataset 𝒟= {(𝒙𝑖, 𝒚∗\n𝑖)}𝑁\n𝑖=1 of problems 𝒙𝑖and oracle responses 𝒚∗\n𝑖, we will\ntrain an LLM policy 𝜋𝜃(⋅∣[𝒙, ˆ𝒚1∶𝑙, 𝑝1∶𝑙]) that, given the problem 𝒙, previous 𝑙model attempts ˆ𝒚1∶𝑙at the\nproblem, and auxiliary instructions 𝑝1∶𝑙(e.g., instruction to find a mistake and improve the response),\nsolves the problem 𝒙as correctly as possible. This formalism is akin to the multi-turn MDP in Qu et al.\n(2024). Moreover, we assume access to a reward function / verifier ̂𝑟(𝒚, 𝒚∗), such as a string-matching\nbased answer checking function) that evaluates correctness of response 𝒚by comparing with the oracle\nresponse 𝒚∗. Critically, we do not assume access to such a function at test-time and the model itself learns\nto deduce whether there was a mistake and corrects it, as is often the case in e.g. mathematical reasoning\nproblems. An example and overview of our problem setting is given in Figure 2.\nWe aim to find a model 𝜋(□∣◦) (which we will also refer to as a policy) mapping a sequence of input\ntokens ◦to a sequence of output tokens □that maximizes the correctness reward obtained from the\nverifier at the end of 𝑙+ 1 turns. Formally, this can be written as the following multi-step RL objective:\nmax\n𝜋𝜃\nℰ𝒙,𝒚∗∼𝒟,ˆ𝒚𝑙+1∼𝜋𝜃(⋅∣[𝒙,ˆ𝒚0∶𝑙,𝑝1∶𝑙]) [̂𝑟(ˆ𝒚𝑙+1, 𝒚∗)] .\n(1)\nCrucially, note that unlike standard SFT or prevalent RL fine-tuning workflows that train the policy 𝜋to\ndirectly produce an optimal response ˆ𝒚for an input 𝒙, Equation 1 trains 𝜋over multiple turns / attempts\nsimultaneously, where intermediate turn responses ˆ𝒚1∶𝑙are supervised indirectly with the final rewards.\nA base RL approach for fine-tuning LLMs. Our RL toolkit is based on on-policy policy gradient. These\nmethods, such as REINFORCE with a KL-divergence penalty against a fixed model (Ahmadian et al.,\n2024), are widely used in RL fine-tuning of LLMs, primarily in setting of single-turn RL from human\nfeedback. Formally, such policy gradient approaches train a policy 𝜋𝜃(⋅∣𝒙) to optimize:\nmax\n𝜃\nE𝒙𝑡,𝒚𝑡∼𝜋𝜃(⋅∣𝒙𝑡) [̂𝑟(𝒚𝑡, 𝒚∗) −𝛽1𝐷𝐾𝐿(𝜋𝜃(⋅∣𝒙𝑡)∣∣𝜋ref(⋅∣𝒙𝑡))] ,\n(2)\nwhere 𝜋ref is a reference anchor policy, typically chosen to be a pre-trained or SFT policy.\nMetrics. For measuring self-correction performance, we report and analyze the following metrics: (1)\nAccuracy@t1: the model’s accuracy at the first attempt; (2) Accuracy@t2: the model’s accuracy at the\nsecond attempt, (3) Δ(t1, t2): the net improvement in model accuracy between the first and second\nattempts, which measures the efficacy of self-correction, (4) Δi→c(t1, t2): the fraction of problems that\nare incorrect in the first attempt but become correct at the second attempt, which measures how many\nnew problems can self-correction solve; and (5) Δc→i(t1, t2): the fraction of problems that are correct in\nthe first attempt but become incorrect at the second attempt, which measures how well the model is able\nto understand what makes a response correct.\n4. Supervised Fine-Tuning on Self-Generated Data is Insufficient for Self-Correction\nPerhaps a natural approach to train for self-correction is to utilize some form of supervised fine-tuning\non data collected from a base model. Variants of this recipe have been shown to scale well in single-\nturn reasoning problems (Havrilla et al., 2024a; Singh et al., 2023; Zelikman et al., 2022). Can such\nSFT-based approaches be effective for self-correction as well?\n5\nTraining Language Models to Self-Correct via Reinforcement Learning\nTable 1 ∣Self-correction performance after training on 𝒟STaR and 𝒟SFT. For both approaches, we find that the gap between\nsecond-attempt and first-attempt performance (Δ(𝑡1, 𝑡2)) is either overly negative or very small. In addition, both approaches\nerroneously modify a correct response to be incorrect, i.e., reflected in a high Δc→i(𝑡1, 𝑡2) and a low Δi→c(𝑡1, 𝑡2).\nMethod\nAccuracy@t1\nAccuracy@t2\nΔ(t1, t2)\nΔi→c(t1, t2)\nΔc→i(t1, t2)\nBase model\n52.6%\n41.4%\n-11.2%\n4.6%\n15.8%\nSTaR 𝒟StaR\n55.4%\n41.2%\n-14.2%\n5.4%\n19.6%\nPair-SFT 𝒟SFT\n52.4%\n54.2%\n1.8%\n5.4%\n3.6%\nTable 2 ∣Self-correction performance after training on 𝒟+\nSTaR and 𝒟+\nSFT. Performance improves for STaR indicating that a\nhigher coverage dataset helps improve performance, but not for SFT where traning on traces where both responses are correct\nforces the model to simply not make any changes to its first-attempt response, no matter how correct or incorrect that is.\nMethod\nAccuracy@t1\nAccuracy@t2\nΔ(t1, t2)\nΔi→c(t1, t2)\nΔc→i(t1, t2)\nBase model\n52.6%\n41.4%\n-11.2%\n4.6%\n15.8%\nSTaR 𝒟+\nStaR\n53.6%\n54.0%\n0.4%\n2.6%\n2.2%\nPair-SFT 𝒟+\nSFT\n55.0%\n55.0%\n0%\n0%\n0%\nIn this section, we perform an empirical study to answer this question. We study two approaches:\nSTaR (Zelikman et al., 2022) and an approach akin to Welleck et al. (2023) that trains only one model.\nWe do not use learned process or outcome verifiers to guide correction traces, so our setup differs from\nSFT in Snell et al. (2024). We find that such methods improve substantially compared to the base model’s\nself-correction behavior, but still fail to attain a positive self-correction rate and produce a worse second\nattempt compared to their first attempt. By probing trained models, we find that these failures largely\nstem from supervised fine-tuning amplifying the initial bias of the base model resulting in only minor\nchanges to its first-attempt response. While these failures can be addressed if a different distribution over\ninitial responses is used for training, doing so fails to induce effective self-correction behavior under the\nmodel’s own response distribution. Either way, learning is affected by distribution shift or amplification\nof the base model’s bias. These observations motivate the design of our method in Section 5.\n4.1. Analysis Setup: Methods and Dataset Construction\nMethods. We prompt off-the-shelf models to obtain a large number of two-turn self-correction traces.\nThe STaR approach, analogous to ReST𝐸𝑀(Singh et al., 2023), filters these trajectories to only retain\nthose that successfully revise incorrect responses and runs SFT on the resulting dataset. In contrast,\nWelleck et al. (2023) use the base model data from above to construct sets of correct and incorrect\nresponses and then generates “synthetic” repair traces by pairing incorrect responses with correct ones.\nWe study a variant of their method we call Pair-SFT, which does not train a separate corrector model\nand does not augment this initial dataset with multi-turn traces.\nDataset construction. We perform our study on the MATH dataset, and generate self-correction traces\nby prompting the Gemini 1.5 Flash (Reid et al., 2024) using temperature 1.0. We construct datasets for\nSTaR and Pair-SFT as follows: (1) 𝒟STaR ∶= {(𝒙𝑖, ˆ𝒚−\n𝑖, ˆ𝒚+\n𝑖)}𝑁\n𝑖=1, where ˆ𝒚−\n𝑖and ˆ𝒚+\n𝑖correspond to incorrect\n6\nTraining Language Models to Self-Correct via Reinforcement Learning\n(a) Histograms of edit distance ratios on MATH 500.\n(b) STaR edit distance ratios.\n(c) Pair-SFT edit distance ratios.\nFigure 3 ∣Edit distance between first-attempt and second-attempt responses obtained from fine-tuned models, our\napproach (SCoRe) and the base model. Observe that while training on self-generated error correction traces inherits the\nbi-modal distribution of edits as the base model, SFT tends to be quite conservative.\nand correct responses appearing within a single sequence of attempts from the current model, and (2)\n𝒟SFT ∶= {(𝒙𝑖, ˆ𝒚−\n𝑖, ˜𝒚+\n𝑖)}𝑁\n𝑖=1, where ˜𝒚+\n𝑖is a random correct response for problem 𝒙, randomly sampled\nfrom the set of all first-turn and second-turn responses produced by the model. We then ran supervised\nfine-tuning on both of these datasets: following Singh et al. (2023), we repeat 3 iterations of collecting\nand running SFT on 𝒟STaR, but only 1 epoch on 𝒟SFT given the large dataset size.\n4.2. Empirical Findings\nWe plot the self-correction performance of the Gemini 1.5 Flash before and after running fine-tuning\non 𝒟STaR (3 iterations) and 𝒟SFT in Table 1. We find that although Δ(t1, t2) is substantially higher for\nPair-SFT relative to the base model, there is still little benefit to doing self-correction (1.8% gain). By\nconsidering Δi→c and Δc→i, we find that SFT mainly helps by reducing the number of correct problems\nthat are mistakenly changed to incorrect after revision, and does not significantly increase the fraction of\nincorrect first attempts that are correctly repaired. This result is consistent with prior studies on intrinsic\nself-correction that have found negligible or even negative Δ(t1, t2) (Huang et al., 2023; Qu et al., 2024).\nWe also find that unlike Pair-SFT, training on 𝒟STaR does not reduce Δc→i, indicating that the STaR policy\ndoes not have a clear understanding of when and when not to make modifications. We hypothesize\nthat this discrepancy is due to the data distributions of 𝒟𝑆𝐹𝑇and 𝒟𝑆𝑇𝑎𝑅: the former covers a much more\ndiverse space of revision trajectories due to the nature of random pairing. Observing this, we also\ntrained on an extended version of 𝒟+\nSTaR (and also 𝒟+\nSFT), which additionally presents more tuples with\nboth correct responses. We would expect the addition of such “correct-to-correct” data to prevent the\nmodel from erroneously revising a correct response and, at the very least, restrict the modification of a\ncorrect response into only another correct response. As shown in Table 2, perhaps interestingly, we find\nthat including such data has opposite effects on STaR and SFT: for STaR, inclusion of this data helps\nsubstantially, though it still results in barely any meaningful self-correction performance. On the other\nhand, for SFT, inclusion of this data overly biases the model to not change its answer at all.\n7\nTraining Language Models to Self-Correct via Reinforcement Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEpoch fraction\n0.02\n0.04\n0.06\n0.08\n0.10\nValidation \ni\nc(t1, t2)}\n0.1\n0.2\n0.3\n0.4\nTrain \ni\nc(t1, t2)}\nFixed validation\nSelf-generated validation\nFixed train\nFigure 4 ∣Tracking self-correction performance on a different sets of\nfirst-attempt responses: (a) “fixed validation”: first response is distributed\nidentically as the training set, (b) “self-generated”: first response is\ngenerated by the learner itself. Observe that over the course of training,\nwhile accuracy of correcting a fixed validation set of responses largely\nstays constant (and perhaps even slightly improves) and the accuracy on\nthe training data improves substantially (note the different axis for the\ntraining set accuracy on the right), the model’s correction abilities its own\nfirst-attempt response degrade substantially. This indicates that training\non a fixed offline dataset may not be effective at inducing self-correction\nabilities due to distribution shift.\nDiving deeper: analyzing self-correction\nbehavior.\nWe also visualized how the\nSTaR and SFT models edit their responses.\nIn particular, we measured edit distance\nratio, defined as the edit distance be-\ntween the responses normalized by the\ntotal length of both the responses, to sum-\nmarize the extent to which models mod-\nify their first-attempt response. As shown\nin Figure 3a, while the base model some-\ntimes makes substantially large edits to\nthe original response, models fine-tuned\non 𝒟STaR and 𝒟SFT are overly conserva-\ntive, and often make no edits at all. We\nwill show in Section 5 that our proposed\nmethod SCoRe is able to avoid amplifying\nthis bias of not making changes, without\nany explicit training for controlling how\nmuch to edit solutions.\nWe also plotted edit distance ratios in cor-\nrection traces appearing in the training data and compared it against the ratios in self-correction traces\ngenerated by STaR and Pair-SFT on training and validation problems in Figures 3b and 3c. While\nSTaR produces qualitatively similar edit distance ratios on both train and validation problems (meaning\nthat it performs within the training distribution very well), we still observe somewhat of a discrepancy\nbetween train and validation edit distance ratios for SFT. This means that Pair-SFT is not very effective at\ngeneralizing to new problems from the same distribution.\nSeeing the discrepancy in the edit distance ratios between train and validation problems for Pair-SFT,\nwe also analyzed the self-correction accuracy of the SFT model on a fixed set of first-attempt responses\nand self-generated first-attempt responses in Figure 4. We observe clearly different behaviors on both\ntraining vs. validation as well as static vs self-generated first-attempt distributions: while the model is able\nto optimize training correction accuracy well and also maintains its initial correction accuracy on first\nattempts appearing in the validation set (distributed i.i.d. to the training distribution), its self-correction\naccuracy degrades with more training.\nTakeaways: Insufficiency of SFT\nWe showed two distinct sources of failure of SFT methods: STaR latched onto only one mode\nof correction behavior that made minor changes, and training via Pair-SFT on data with wider\ncoverage resulted in a degradation in self-correction abilities on responses from the model’s\ndistribution of initial responses, due to distribution shift. This implies that an effective approach\nmust satisfy two desiderata: [D1] it should directly train on self-generated traces to alleviate\ndistribution mismatch that affected SFT (Figure 4), and [D2] self-generated traces employed\nshould prevent a collapse to making minor edits during learning. We will next develop an online\nRL approach that addresses these challenges with a careful initialization and reward shaping.\n8\nTraining Language Models to Self-Correct via Reinforcement Learning\nBroader Implications of these Results\nThese results more generally suggest that offline supervised fine-tuning is likely not effective\nat making use of additional in-context tokens to learn nuanced algorithmic behaviors, due to\nchallenges of distribution shift in training data and amplification of certain pathological behaviors\nthat seem promising on the training data but do not learn the right strategy.\n5. SCoRe: Self-Correction via Multi-Turn Reinforcement Learning\nTo develop an effective approach for teaching LLMs to self-correct by training entirely on self-generated\ndata, we have to satisfy the two desiderata discussed above. Utilizing on-policy RL in our method is\na natural way to satisfy desideratum [D1]. Our approach, SCoRe will extend standard single-turn RL\n(Equation 2) to the multi-turn setting under the hierarchical framework from Zhou et al. (2024).\nKey challenges. While multi-turn RL that optimizes Equation 1 addresses the issue with distribution\nshift, it is unclear whether it also satisfies desideratum [D2]. Base model initializations for fine-tuning\npresent a highly-skewed distribution over edit distances (Figure 3a), which makes them susceptible to\nmode collapse, a well-known issue in deep RL (Mei et al., 2020; Schaul et al., 2019). Even if the base\nmodel could produce a less-skewed distribution over edit distance ratios during self-correction, we still\nneed the RL training procedure to learn a self-correction strategy from the training data that generalizes\nto test prompts.\nTo see whether RL training can learn a self-correction strategy by purely optimizing the final attempt’s\nreward, we ran a naïve multi-turn RL baseline to optimize Equation 1. We find empirically in Figure 5 that\nwhile the performance of each attempt improves with training via naïve multi-turn RL, the performance\nof the second attempt is tightly coupled with the first attempt. As training progresses, standard multi-turn\nconverges to be overly biased towards not changing its response, resulting in no self-correction ability.\nWhy does this happen? There are at least two equally good solutions when optimizing a policy with\nRL on the training data: (i) learning to improve from the first to the second attempt, or (ii) learning to\nproduce the best first-attempt response followed by no correction in the second attempt. Of course only\nthe former strategy generalizes to new problems, but an overparameterized LLM may not necessarily\nlearn strategy (i) instead of (ii), since both of these strategies appear equally optimal on the training set.\nAbstractly, learning the “meta strategy” of self-correction during training is difficult unless the “direct”\nstrategy that optimizes reward appears less viable. Conceptually, this is similar to the memorization\nchallenge in meta-learning (Yin et al., 2019), which suggests that when provided with mutually exclusive\ntasks, few-shot meta-learning is likely to recover the supervised learning solution (without relying on\nadditional context from the few shots) that directly predicts the output for an input. In our case, this is\nanalogous to not self-correcting past attempts, but rather directly attempting produce a good response.\nMethod overview. Our approach SCoRe is designed to address the key challenges identified above. SCoRe\noperates in two stages. In the first stage (Stage I), SCoRe trains a model initialization that is less prone to\ncollapse in subsequent RL by explicitly teaching the model to correct its second-attempt responses under\na relatively static first-attempt distribution. This initialization amplifies the coverage of second-attempt\nresponses given the model’s own first attempt distribution, with a bias towards high-reward responses.\nWe then use this model initialization to seed the actual multi-turn RL run (Stage II). To bias learning\ntowards a solution that learns to self-correct, we shape the reward at the second attempt to provide\na large positive reward bonus in favor of self-correction. Both stages bias the model towards learning\n9\nTraining Language Models to Self-Correct via Reinforcement Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining fraction\n0.50\n0.55\n0.60\n0.65\n0.70\nTrain Accuracy\nStage I\nStage II\nMulti-turn baseline\nTurn 1\nTurn 2\n(a) Evolution of training reward with more training. When\ntraining with naïve multi-turn RL, the responses at both the\nattempts become tightly coupled together, leading to poor cov-\nerage for subsequent iterations and worse learning progress.\nStage I in SCoRe is explicitly designed to alleviate this, leading\nto increased exploration and better final performance.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining fraction (over stage 1)\n0.20\n0.25\n0.30\n0.35\nDifferent answer freq.\nMulti-turn RL baseline\nStage 1\n(b) Frequency in which the learner proposes a different\nanswer in the second turn. Without explicitly modifying\nthe policy initialization as in SCoRe, the policy quickly learns\nto not change its answer, leading to poor exploration. This\nis evident by a decrease in the number of prompts for which\nthe second attempt produces a different answer.\nFigure 5 ∣Failure modes of naïve multi-turn RL training for inducing self-correction capabilities. These results indicate that\nsome explicit approach to alter the policy initialization is required for learning. Stage I in SCoRe exactly tackles this.\nself-correction by initializing the model appropriately and controlling subsequent RL.\n5.1. Stage I: Training a Model Initialization to Prevent Collapse\nThe goal of Stage I of SCoRe is to obtain a good model initialization by improving the base model’s\ncoverage over second-attempt responses so that subsequent training for self-correction is less susceptible\nto collapse we observed with STaR/SFT. While this would typically be done via SFT in LLM fine-tuning\npipelines, our experiments in Section 4 show that SFT trains the model to latch onto only one mode\nof correction behavior. As a result, an SFT initialization is not expected to generate informative and\nexploratory traces for learning. Therefore, we do not initialize our RL training with SFT and instead\ndevelop Stage I to produce a separate initialization that is less prone to collapse.\nTo do so, we explicitly fine-tune the base model to produce high-reward revisions at the second attempt,\nwhile forcing the model to not change its first-attempt response, by constraining the first-attempt response\ndistribution as close as possible to that of the base model using a KL-divergence. While does this appear\nsub-optimal – a first-attempt response with fewer mistakes could be corrected to a better second-attempt\nresponse – but as we will show, this stage is critical in reducing the base model’s bias towards simply\ncoupling the first and second-attempt distributions, and thus becoming trapped in a local optima when\nactual multi-turn RL is run. Formally, the objective we optimize is:\nmax\n𝜃\nE𝒙1,𝒚1∼𝜋𝜃(⋅∣𝒙),𝒚2∼𝜋𝜃(⋅∣[𝒙1,𝑝1])[̂𝑟(𝒚2, 𝒚∗) −𝛽2𝐷𝐾𝐿(𝜋𝜃(⋅∣∣𝒙1)∣∣𝜋ref(⋅∣𝒙1))],\n(3)\nwhere 𝛽2 is a hyper parameter designed to enforce a strict KL penalty only on the first attempts to\navoid shift of the first-turn responses (denoted by the term in blue). Note that we still utilize the default\nKL-divergence penalty from Equation 2, but that is applied with a much smaller weight and is omitted\nfrom Equation 3 for brevity. Indeed, we show that unlike naïve multi-turn RL, Stage I is more effective at\ndecoupling the two responses (Figure 5b).\n10\nTraining Language Models to Self-Correct via Reinforcement Learning\noundation model\nprompt (x)\nresponse (y)\nStage I: Train an initialization for self-correction \nStage II: Run Multi-turn RL with reward shaping\nFoundation model trained with SCoRe\nfirst attempt (y1)\nsecond attempt (y2)\nprompt (x)\nMaximize \nreward\nConstrain to \nbase model\nMaximize shaped \nreward\nMaximize reward \nFoundation model trained with SCoRe\nfirst attempt (y1)\nsecond attempt (y2)\nprompt (x)\nOptionally interleave stages\nFigure 6 ∣An overview of our approach (SCoRe). SCoRe trains a model in two stages: Stage I: instead of running SFT\n(that produces pathological amplification of biases) to initialize RL training, we train a good initialization that can produce\nhigh-reward responses in the second-attempt while mimicking the base model’s initial response at the first attempt. Stage II:\njointly optimizing both attempts, where the latter uses a shaped reward to incentivize discovery of the self-correction strategy\ninstead of the simple strategy of product the best first response followed by making any minor edits to it in the second attempt.\n5.2. Stage II: Multi-Turn RL with Reward Shaping\nEquipped with a model initialization from Stage I that exhibits a substantially smaller bias to couple the\ntwo responses, the second stage of SCoRe now trains responses at both attempts towards optimizing\nreward in line with Equation 1. Of course, we also want to make sure to not degrade the first-attempt\nresponses in the process. Therefore, for two-turn self-correction problem, we train the policy 𝜋𝜃(⋅∣⋅)\nagainst the following objective:\nmax\n𝜃\nE𝒙1,𝒚1∼𝜋𝜃(⋅∣𝒙),𝒚2∼𝜋𝜃(⋅∣[𝒙1,𝑝1]) [\n2\n∑\n𝑖=1\n̂𝑟(𝒚𝑖, 𝒚∗) −𝛽1𝐷𝐾𝐿(𝜋𝜃(⋅∣𝒙𝑖)∣∣𝜋ref(⋅∣𝒙𝑖))] ,\n(4)\nwhere 𝒙𝑖, 𝑖∈{1, 2} corresponds to the set of input tokens passed as context to the model. SCoRe optimizes\nEquation 4 with an on-policy policy gradient approach.\nReward shaping to incentivize self-correction. As discussed earlier, it is unclear if running RL for\noptimizing Equation 4 prefers a strategy that incentivizes self-correction over finding the best first-attempt\nresponse and keeping it unchanged, since both of these strategies appear equally good on the small\ntraining dataset. To mitigate this issue, we bias the learning problem towards the self-correction strategy\nvia reward shaping: by providing a higher emphasis to traces that flip correctness from the first attempt\nto the second, we can bias the model to learn a self-correction solution. Concretely, given an two-turn\non-policy rollout 𝜏= {𝒙1, ˆ𝒚1, ̂𝑟(𝒚1, 𝒚∗), 𝒙2, ˆ𝒚2, ̂𝑟(𝒚2, 𝒚∗)} (where 𝒙2 denotes all the tokens from the first\nturn concatenated with each other), we propose to modify the reward ̂𝑟(𝒚2, 𝒚∗) used for training in\nEquation 4, at the second attempt with an additional bonus ̂𝑏(𝒚2∣𝒚1, 𝒚∗) given by:\n̂𝑏(𝒚2∣𝒚1, 𝒚∗) = 𝛼⋅(̂𝑟(𝒚2, 𝒚∗) −̂𝑟(𝒚1, 𝒚∗)) ,\n(5)\nwhere 𝛼is a positive constant multiplier, ideally a real number significantly larger than 1.0. Adding this\nbonus to the second attempt only emphasizes traces that flip the correctness of the response and assigns\na heavy negative penalty to transitions that change a correct response to incorrect in the second attempt.\nIn contrast, transitions that do not flip correctness of the response and are likely to lead to collapse of not\n11\nTraining Language Models to Self-Correct via Reinforcement Learning\nmaking meaningful edits contribute much less to the overall loss. Thus, the addition of this bonus should\nregularize the training process from collapsing on to the “direct” solution that might look optimal on the\ntraining set but does not produce self-correction behavior on new examples.\n5.3. Putting it Together and Implementation Details\nOur approach is illustrated pictorially in Figure 6. SCoRe applies stages I and II in an interleaved fashion\nfor multiple iterations (e.g., Figure 5 shows two applications each of Stage I and II). We use a small 𝛽1 for\nall experiments (i.e., the coefficient on the KL divergence penalty against the base model in Equation 2),\nand found that setting 𝛽2 = 10𝛽1 to work sufficiently well in our experiments. In practice, one can also\nuse an adaptive 𝛽2 that attempts to balance the magnitudes of the first-attempt KL regularization and\nthe second-attempt policy loss. In some of our experiments, we also choose to amplify the coverage of\nstates used for on-policy RL by incorporating first-attempt solutions obtained by repeatedly sampling the\nbase model as offline prompts in RL. We find that incorporating this data, especially in Stage 2, where\nthe first-turn policy may have drifted further from that of the base model, can have substantial benefits\nespecially when attempting to learn from limited data.\nTakeaways and Implications\nThe core insight behind our method is that we must make it more attractive to learn the more\nnuanced algorithmic strategy instead of collapsing to an ungeneralizable behavior mode. Further-\nmore, to avoid the challenge of distribution shift, this must be done on self-generated online data.\nSCoRe instantiates this principle when learning the model initialization for multi-turn RL (Stage\nI) and when using a reward bonus to prevent training from producing non-correcting strategies\n(Stage II).\n6. Experimental Evaluation\nThe goal of our experiments is to demonstrate the efficacy of SCoRe in teaching LLMs how to correct\ntheir own mistakes by training on their own data. In addition, we also aim to understand the impact of\neach of the components of SCoRe in contributing to this ability. To this end, we perform a comparative\nevaluation of SCoRe against prior methods that also use self-generated data to train for self-correction,\nand run several ablation studies on two representative reasoning tasks where error correction is crucial.\nTasks. We mainly focus on math and coding tasks: (a) math problem solving on MATH (Hendrycks\net al., 2021), and (b) code generation on MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021)\nfor evaluating the efficacy of our approach. Concretely, we use the following train-test splits in our\nexperiments: (1) MATH: following Lightman et al. (2023), we augment the MATH training set with 4500\nproblems from the test set, and report results on the remaining 500 problems; and (2) Code generation:\nwe train on MBPP and report results on HumanEval, which does not expose test cases to the model.\nEvaluation protocol and metrics. We report the self-correction accuracy on a number of tasks with\ntwo sequential attempts at the problem, i.e., one round of self-correction. For MBPP, following the\nevaluation protocol of Ni et al. (2024), we also report results on MBPP-R, an offline repair task that\nrequires correcting incorrect first-attempt programs generated from PaLM 2.\nModels. For all of our experiments on MBPP, we fine-tune Gemini 1.0 Pro and for MATH, we fine-\ntune Gemini 1.5 Flash. For all evaluations, we use greedy decoding (i.e. temperature 0), except for\ninference-compute scaling in Section 6.2 where we set temperature to be 0.7. For all training methods, we\n12\nTraining Language Models to Self-Correct via Reinforcement Learning\nTable 3 ∣Performance of SCoRe on MATH. Observe that SCoRe not only attains a higher accuracy at both attempts, but also\nprovides the most positive self-correction performance Δ(t1, t2), and improves upon the number of problems that move from\nincorrect to correct, while substantially reducing the number of problems that become incorrect in the second attempt.\nApproach\nAccuracy@t1\nAccuracy@t2\nΔ(t1, t2)\nΔi→c(t1, t2)\nΔc→i(t1, t2)\nBase model\n52.6%\n41.4%\n-11.2%\n4.6%\n15.8%\nSelf-Refine\n52.8%\n51.8%\n-1.0%\n3.2%\n4.2%\nSTaR w/ 𝒟+\nStaR\n53.6%\n54.0%\n0.4%\n2.6%\n2.2%\nPair-SFT w/ 𝒟SFT\n52.4%\n54.2%\n1.8%\n5.4%\n3.6%\nSCoRe (Ours)\n60.0%\n64.4%\n4.4%\n5.8%\n1.4%\nattempted to use a fixed budget of model samples and gradient updates, and do not vary hyperparameters\nsuch as learning rate and batch size between runs. For all RL runs, we selected checkpoints with the\nhighest training reward, although a small held-out validation set of problems can also be used. Additional\ndetails about the experimental setup can be found in the Appendix.\nEvaluation prompts. We use a zero-shot CoT prompting for evaluation on MATH, zero-shot prompting\nfor evaluation on HumanEval, and the canonical three-shot prompt for first-attempt training samples\non MBPP. At the second attempt, we utilize an instruction that does not reveal the correctness of the\nprevious answer, but asks the model to attempt to deduce whether a mistake exists in its first attempt\nresponse, and, if so, potentially rewrite its response. Full prompts and self-correction instructions can be\nfound in Appendix A.\nPrior approaches and comparisons. We compare SCoRe to prior approaches and baselines. We compare\nto Self-Refine (Madaan et al., 2023), a representative prompting-based approach to elicit self-correction\nbehaviors from a model, akin to Reflexion (Shinn et al., 2023). Of the fine-tuning based approaches, we\ncompare to Pair-SFT based on the approach from Welleck et al. (2023), and multi-turn STaR (Singh\net al., 2023; Zelikman et al., 2022) that fine-tune the model by minimizing negative log-likelihood\non synthetically paired repair traces and successful repair traces respectively. Due to a difference in\nassumptions and base models, we cannot compare SCoRe directly with results in prior papers that utilize\noracle information (e.g., RISE (Qu et al., 2024)) or run multiple models (e.g., GLoRE (Havrilla et al.,\n2024b), the full version of Self-Correct (Welleck et al., 2023) with a refinement model), largely because\nthese comparisons will be apples-to-oranges with distinct setups, different refinement or oracle models.\n6.1. Benchmark Results\nMATH. Our results are in Table 3, as well as in Figure 1. SCoRe exhibits substantially stronger performance\non both direct and self-correction accuracies. Notably, the intrinsic self-correction gain Δ(t1, t2) of 4.4%\nis the first significantly positive delta, despite having fewer incorrect problems to correct by virtue of its\nhigher Accuracy@t1. Relative to the base 1.5 Flash model, SCoRe improves Δ(t1, t2) by 15.6%, and\nAccuracy@t2 by 23.0%, and over the nearest baseline, Pair-SFT, by 10.2% and 2.6% respectively.\nBy observing the frequency of problems that change from incorrect at from the first attempt to correct\nin the second attempt and vice versa, we see that SCoRe improves the rate at which it fixes incorrect\nanswers (14.5%, compared to 9.5% for base) and reduces the proportion of correct answers it changes.\n13\nTraining Language Models to Self-Correct via Reinforcement Learning\nTable 4 ∣Performance of SCoRe on HumanEval. Observe that ScoRe attains the highest accuracy at the second attempt\n(Accuracy@t2), and also substantially improves the number of problems that become correct with the use of additional\nsequential attempts. In addition, SCoRe also attains the highest correction rate on MBPP-R, an offline repair task.\nMethod\nMBPP-R\nAccuracy@t1\nAccuracy@t2\nΔ(t1, t2)\nΔi→c(t1, t2)\nΔc→i(t1, t2)\nBase model\n47.3%\n53.7%\n56.7%\n3.0%\n7.9%\n4.9%\nSelf-Refine\n30.7%\n53.7%\n52.5%\n-1.2%\n9.8%\n11.0%\nPair-SFT\n59.8%\n56.1%\n54.3%\n-1.8%\n4.3%\n6.1%\nSCoRe (Ours)\n60.6%\n52.4%\n64.6%\n12.2%\n15.2%\n3.0%\nTable 5 ∣Ablation studies to understand the impact of various components in SCoRe. Observe that while single-turn\ntraining is effective at optimizing the first-attempt accuracy of the model, it leads to degradation in the second attempt. Instead,\nSCoRe allows us to attain a higher second-attempt accuracy even though it attains a slightly worse first-attempt accuracy. The\nperformance improvements without Stage I or without reward shaping in SCoRe are small when measured by the difference in\naccuracy over the two attempts. Utilizing STaR generally leads to worse performance even when it is run from an effective\nStage I checkpoint. These results highlight the importance of various components in SCoRe.\nMethod\nAccuracy@t1\nAccuracy@t2\nΔ(t1, t2)\nSCoRe (Ours)\n60.0%\n64.4%\n4.4%\nw/o multi-turn training\n61.8%\n59.4%\n-2.4%\nw/o Stage I\n59.2%\n61.4%\n2.2%\nw/o reward shaping\n60.0%\n62.6%\n2.6%\nw/ STaR instead of REINFORCE Stage II\n56.2%\n58.4%\n2.2%\nCode generation. Our results for the code generation task are shown in Table 4. Generally, we find that\nSCoRe achieves both improved self-correction as well as strong offline repair performance. For MBPP-R,\nwe find that SCoRe improves the base model from 47.3% to 60.6%, which is comparable to the gap\nbetween GPT-3.5 and GPT-4 (42.9% and 63.2% respectively) (Ni et al., 2024). Despite only training\non MBPP, we find that SCoRe is especially effective at generalizing to HumanEval, achieving a 12.2%\nintrinsic self-correction delta, or 9% higher than the base model. By contrast, Pair-SFT works nearly\nas well on the static repair task MBPP-R, but actually degrades the base model when evaluated in the\nself-correction setting, thus underscoring the importances of on-policy sampling for self-correction.\n6.2. Inference-Compute Scaling with Self-Correction\nNext, we investigate if SCoRe can be used in conjunction with inference-time compute scaling strategies.\nTo do so, we evaluate self-consistency decoding (Wang et al., 2022), also known as majority voting, where\nwe sample a diverse set of solutions, and then select the most consistent answer among these solutions.\nTypically, the default strategy is to sample all solutions in parallel to perform majority voting. However,\nwe show in Figure 1 (right) that instead of sampling 2𝐾solutions in parallel, it is more compute-efficient\nto sample 𝐾solutions in parallel, then perform one round of self-correction on each solution. With 32\nsolution budget per problem, parallel sampling shows a 7.4% accuracy gain, while combining it with\nsequential sampling using self-correction yields a 10.5% improvement.\n14\nTraining Language Models to Self-Correct via Reinforcement Learning\n6.3. Ablation Studies: Understanding the Impact of SCoRe Components\nFinally, we also present a number of ablation studies to understand the importance of various compo-\nnents in SCoRe. We perform these ablations on the MATH dataset. Concretely, we aim to answer the\nfollowing questions: (1) the importance of multi-turn training: Can RL trained to maximize single-turn\nperformance achieve better accuracy@t1 or accuracy@t2?; (2) the importance of multi-stage training:\nHow essential is Stage I to SCoRe? In other words, why not run Stage II directly?; (3) the impact of\nreward shaping. How would removing the reward shaping terms affect performance of SCoRe in Stage\nII, assuming Stage I was done identically?; (4) the importance of on-policy RL: What if we replaced\nREINFORCE in Stage II with STaR?.\nThe results of all of these ablation experiments are shown in Table 5. As expected, single-turn training\nimproves turn 1 performance, but has negative Δ(t1, t2). As shown in Figure 5, Stage I is critical to\nSCoRe; without it, the model achieves 2% lower Δ(t1, t2) and 3% lower accuracy@t2. Similarly, we find\nthat removing reward shaping also hurts performance, indicating that the RL objectives in both stages\nplay a significant role in teaching self-correction behavior. We also find that replacing REINFORCE with\nSTaR in Stage II results in significantly lower absolute performance with no visible improvements in\nself-improvement performance, which contrasts with the findings in Havrilla et al. (2024a) that STaR and\non-policy RL have similar convergence rates for single-turn RL. This suggests that leveraging on-policy\nsamples is especially critical in the self-correction setting, which presents a multi-turn problem that\nadmits potentially spurious solutions.\n6.4. Qualitative Analysis of SCoRe\nWe also perform a qualitative investigation into how SCoRe addresses the self-repair shortcomings of\nbase LLMs, and provide several examples in Appendix B. We find that SCoRe is able to refine its own\nresponses in a variety of manners - rewriting the entire solution when necessary, or reproducing the\ncorrect parts of the solution, while revising the incorrect ones. For the latter, we interestingly find that\nSCoRe is especially adept at revising its computational mistakes, and even demonstrates a bias towards\nshowing more steps in certain computations and manipulations in order to increase its probability of\nproducing a correct answer. We additionally observe that the model learns to occasionally self-correct\nwithin a turn, e.g. MATH example 4.\n7. Discussion, Limitations, and Conclusion\nIn this work, we investigated how to imbue LLMs with a self-correction strategy that enables them\nto correct their own responses on the fly, at test-time. Specifically, we proposed SCoRe, a multi-turn\nonline reinforcement learning (RL) approach for training language models to correct their own mistakes,\nand demonstrated through extensive evaluations that it is the first method that can attain significantly\npositive intrinsic self-correction performance. To motivate the design of SCoRe, we rigorously analyzed\nthe behavior of various fine-tuning baselines and identified failure modes in which the model learns a\nnon-correcting strategy (e.g. learning to make no edits) under these approaches. SCoRe is designed to\nelicit a self-correcting strategy by utilizing a two-stage structure and reward shaping, both of which help\nprevent model collapse into not learning effective self-improvement behavior.\nLimitations. There are various limitations of this work that also provide interesting avenues for future\nwork. We did not train SCoRe for more than one round of iterative self-correction in this paper, which\nmeans that subsequent rounds of self-correction may not be as effective as the first one. An interesting\navenue for future work is to train with more than two attempts via RL, which is already a common and\n15\nTraining Language Models to Self-Correct via Reinforcement Learning\neffective practice to obtain effective self-correction behavior over more than two rounds with SFT (Qu\net al., 2024; Snell et al., 2024). Unifying Stages I and II of SCoRe is also an interesting avenue for\nresearch, since that would alleviate the limitation of running multiple steps and help in designing a more\nrobust method.\nBroader perspectives. Our work has several implications. First, it suggests that learning meta-strategies\n(e.g., self-correction in this paper) might require going beyond the standard paradigm of supervised\nfine-tuning followed by single-turn RL (as shown in Section 4). It demonstrates that multi-turn RL can\nprovide for one such approach. Second, our results also hint that perhaps using more detailed or granular\nsupervision when generating on-policy rollouts in multi-turn RL might further improve the model’s\ncapabilities at implementing nuanced strategies: even though SCoRe did not use dense or fine-grained\nfeedback, it was already able to improve performance of existing models substantially. Utilizing dense\nfeedback is likely to complement our method well. Finally, the importance of our two-stage recipe (based\non careful initialization and reward shaping) in obtaining positive self-correction perhaps more generally\nhints that some kind of regularization is required to ensure that LLMs learn nuanced strategies that can\ngeneralize well to novel, unseen queries at test-time.\nAcknowledgements\nThe authors would like to thank Satinder Baveja, Kalesha Bullard, Gheorghe Comanici, Claire Cui,\nValentin Dalibard, Angelos Filos, Yang Gao, Zoubin Ghahramani, Izzeddin Gur, Raia Hadsell, Clara Huiyi\nHu, Melvin Johnson, Mina Khan, Balaji Lakshminarayanan, Yiran Mao, Hussain Masoom, Junhyuk Oh,\nJordi Orbay, David Silver, and Yury Sulsky for helpful discussions, feedback, and sponsorship. We thank\nAmrith Setlur, Yuxiao Qu, Charlie Snell, Tianhe Yu, and Xinyang (Young) Geng for helpful discussions\nand feedback on an earlier version of the paper.\nAuthor Contributions\nAK and VZ led the paper, with substantial technical contributions from RA and YS. VZ led the experimen-\ntation in the final paper with AK, with support from RA and YS. AK and RA conceived the initial idea\nwith advice and discussions from DS, FB, AF, JDC, AS, and GT. JDC, YS, AS, RA, and AK iterated on the\nmethodology. The development of the final method was done by AK and VZ, with inputs from RA and\nFB. VZ led the infrastructure development, while RA, YS, CP, SI, KB, DS, and LMZ contributed to the\ninfrastructure. AK, RA, FB, AF, DP, GT advised on the overall direction. AK and VZ wrote the manuscript,\nwith input from all co-authors. KM provided program management. FB, and AF co-supervised the project.\nReferences\nA. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer, A. Üstün, and S. Hooker. Back to basics:\nRevisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint\narXiv:2402.14740, 2024.\nA. F. Akyürek, E. Akyürek, A. Madaan, A. Kalyan, P. Clark, D. Wijaya, and N. Tandon. Rl4f: Generating\nnatural language feedback with reinforcement learning for repairing model outputs. arXiv preprint\narXiv:2305.08844, 2023.\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al.\nProgram synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n16\nTraining Language Models to Self-Correct via Reinforcement Learning\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,\nG. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374,\n2021.\nX. Chen, M. Lin, N. Schärli, and D. Zhou. Teaching large language models to self-debug. arXiv preprint\narXiv:2304.05128, 2023.\nZ. Chen, Y. Deng, H. Yuan, K. Ji, and Q. Gu. Self-play fine-tuning converts weak language models to\nstrong language models. arXiv preprint arXiv:2401.01335, 2024.\nJ. Farebrother, J. Orbay, Q. Vuong, A. A. Taïga, Y. Chebotar, T. Xiao, A. Irpan, S. Levine, P. S. Castro,\nA. Faust, et al. Stop regressing: Training value functions via classification for scalable deep rl. arXiv\npreprint arXiv:2403.03950, 2024.\nA. Havrilla, Y. Du, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, S. Sukhbaatar,\nand R. Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint\narXiv:2403.04642, 2024a.\nA. Havrilla, S. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, and R. Railneau.\nGlore: When, where, and how to improve llm reasoning via global and local refinements. arXiv preprint\narXiv:2402.10963, 2024b.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring\nmathematical problem solving with the math dataset. NeurIPS, 2021.\nJ. Hong, N. Lee, and J. Thorne. Reference-free monolithic preference optimization with odds ratio. arXiv\npreprint arXiv:2403.07691, 2024.\nJ. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large language models\ncannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023.\nN. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.\nLivecodebench: Holistic and contamination free evaluation of large language models for code. arXiv\npreprint arXiv:2403.07974, 2024.\nR. Kamoi, Y. Zhang, N. Zhang, J. Han, and R. Zhang. When can llms actually correct their own mistakes?\na critical survey of self-correction of llms. arXiv preprint arXiv:2406.01297, 2024.\nG. Kim, P. Baldi, and S. McAleer.\nLanguage models can solve computer tasks.\narXiv preprint\narXiv:2303.17491, 2023.\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and\nK. Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\nX. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, et al. Agentbench:\nEvaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023.\nA. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi, A. Tang, D. Pykhtar, J. Liu, Y. Wei, et al.\nStarcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.\n17\nTraining Language Models to Self-Correct via Reinforcement Learning\nA. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,\nY. Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651,\n2023.\nJ. Mei, C. Xiao, C. Szepesvari, and D. Schuurmans. On the global convergence rates of softmax policy\ngradient methods. In International Conference on Machine Learning, pages 6820–6829. PMLR, 2020.\nA. Ni, M. Allamanis, A. Cohan, Y. Deng, K. Shi, C. Sutton, and P. Yin. Next: Teaching large language\nmodels to reason about code execution. arXiv preprint arXiv:2404.14662, 2024.\nT. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama. Is self-repair a silver bullet for code\ngeneration? In The Twelfth International Conference on Learning Representations, 2023.\nL. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, and W. Y. Wang. Automatically correcting large language\nmodels: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188,\n2023.\nD. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings. Refiner: Reasoning\nfeedback on intermediate representations. arXiv preprint arXiv:2304.01904, 2023.\nY. Qu, T. Zhang, N. Garg, and A. Kumar. Recursive introspection: Teaching foundation models how to\nself-improve. 2024.\nM. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat,\nJ. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of\ncontext. arXiv preprint arXiv:2403.05530, 2024.\nW. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike. Self-critiquing models for assisting\nhuman evaluators. arXiv preprint arXiv:2206.05802, 2022.\nT. Schaul, D. Borsa, J. Modayil, and R. Pascanu. Ray interference: a source of plateaus in deep reinforce-\nment learning. CoRR, abs/1904.11455, 2019. URL http://arxiv.org/abs/1904.11455.\nL. Shani, A. Rosenberg, A. Cassel, O. Lang, D. Calandriello, A. Zipori, H. Noga, O. Keller, B. Piot,\nI. Szpektor, et al. Multi-turn reinforcement learning from preference human feedback. arXiv preprint\narXiv:2405.14655, 2024.\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the\nlimits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\nN. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and\nself-reflection. arXiv preprint arXiv:2303.11366, 2023.\nA. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil, P. J. Liu, J. Harrison, J. Lee, K. Xu, A. Parisi, et al.\nBeyond human data: Scaling self-training for problem-solving with language models. arXiv preprint\narXiv:2312.06585, 2023.\nC. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine. Offline rl for natural language generation with\nimplicit language q learning. arXiv preprint arXiv:2206.11871, 2022.\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than\nscaling model parameters. arXiv preprint arXiv:2408.03314, 2024.\n18\nTraining Language Models to Self-Correct via Reinforcement Learning\nC. Team. Codegemma: Open code models based on gemma. arXiv preprint arXiv:2406.11409, 2024.\nG. Tyen, H. Mansoor, V. Cărbune, Y. P. Chen, and T. Mak. Llms cannot find reasoning errors, but can\ncorrect them given the error location. In Findings of the Association for Computational Linguistics ACL\n2024, pages 13894–13908, 2024.\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency\nimproves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\nS. Welleck, X. Lu, P. West, F. Brahman, T. Shen, D. Khashabi, and Y. Choi. Generating sequences by\nlearning to self-correct. In The Eleventh International Conference on Learning Representations, 2023.\nURL https://openreview.net/forum?id=hH36JeQZDaO.\nW. Xiong, C. Shi, J. Shen, A. Rosenberg, Z. Qin, D. Calandriello, M. Khalman, R. Joshi, B. Piot,\nM. Saleh, et al. Building math agents with multi-turn iterative preference learning. arXiv preprint\narXiv:2409.02392, 2024.\nS. Ye, Y. Jo, D. Kim, S. Kim, H. Hwang, and M. Seo. Selfee: Iterative self-revising llm empowered by\nself-feedback generation. Blog post, 2023.\nM. Yin, G. Tucker, M. Zhou, S. Levine, and C. Finn. Meta-learning without memorization. arXiv preprint\narXiv:1912.03820, 2019.\nL. Yuan, G. Cui, H. Wang, N. Ding, X. Wang, J. Deng, B. Shan, H. Chen, R. Xie, Y. Lin, et al. Advancing\nllm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078, 2024.\nE. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances in\nNeural Information Processing Systems, 35:15476–15488, 2022.\nY. Zhang, M. Khalifa, L. Logeswaran, J. Kim, M. Lee, H. Lee, and L. Wang. Small language models need\nstrong verifiers to self-correct reasoning. arXiv preprint arXiv:2404.17140, 2024.\nH. S. Zheng, S. Mishra, H. Zhang, X. Chen, M. Chen, A. Nova, L. Hou, H.-T. Cheng, Q. V. Le, E. H. Chi,\net al. Natural plan: Benchmarking llms on natural language planning. arXiv preprint arXiv:2406.04520,\n2024.\nY. Zhou, A. Zanette, J. Pan, S. Levine, and A. Kumar. Archer: Training language model agents via\nhierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.\n19\nTraining Language Models to Self-Correct via Reinforcement Learning\nAppendices\nA. Prompts\nMATH Zero-shot Prompt\nYou are a math expert. When you respond, respond only with the Solution of the final Problem, thinking step by\nstep. At the end of the Solution, when you give your final answer, write it in the form \"Final Answer: The final\nanswer is $answer$. I hope it is correct.\"\nMATH Self-Correction Instruction\nThere might be an error in the solution above because of lack of understanding of the question. Please correct\nthe error, if any, and rewrite the solution. Only output the final solution! At the end of the Solution, when you\ngive your final answer, write it in the form \"Final Answer: The final answer is $answer$. I hope it is correct.\"\nMBPP 3-shot Prompt\nYou are an expert Python programmer, and here is your task: Write a function to find the similar elements from\nthe given two tuple lists. Your code should pass these tests:\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)\n[BEGIN]\ndef similar_elements(test_tup1, test_tup2):\nres = tuple(set(test_tup1) & set(test_tup2))\nreturn (res)\n[DONE]\nYou are an expert Python programmer, and here is your task: Write a python function to identify non−prime\nnumbers. Your code should pass these tests:\nassert is_not_prime(2) == False\nassert is_not_prime(10) == True\nassert is_not_prime(35) == True\n[BEGIN]\nimport math\ndef is_not_prime(n):\nresult = False\nfor i in range(2,int(math.sqrt(n)) + 1):\nif n %\nresult = True\nreturn result\n20\nTraining Language Models to Self-Correct via Reinforcement Learning\n[DONE]\nYou are an expert Python programmer, and here is your task: Write a function to find the largest integers from a\ngiven list of numbers using heap queue algorithm. Your code should pass these tests:\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65]\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75]\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]\n[BEGIN]\nimport heapq as hq\ndef heap_queue_largest(nums,n):\nlargest_nums = hq.nlargest(n, nums)\nreturn largest_nums\n[DONE]\nMBPP/HumanEval Self-Correction Instruction\n# There might be an error in the code above because of lack of understanding of the question. Please correct the\nerror, if any, and rewrite the solution. Only output the final correct Python program!\nB. Examples of SCoRe Self-Correction\nHumanEval Example 1\nProblem:\ndef cycpattern_check(a , b):\n\"\"\"You are given 2 words. You need to return True if the second word or any of its rotations is a substring in\nthe first word\ncycpattern_check(\"abcd\",\"abd\") => False\ncycpattern_check(\"hello\",\"ell\") => True\ncycpattern_check(\"whassup\",\"psus\") => False\ncycpattern_check(\"abab\",\"baa\") => True\ncycpattern_check(\"efef\",\"eeff\") => False\ncycpattern_check(\"himenss\",\"simen\") => True\n\"\"\"\nTurn 1 solution (incorrect):\ndef cycpattern_check(a , b):\n\"\"\"You are given 2 words. You need to return True if the second word or any of its rotations is a substring in\nthe first word\ncycpattern_check(\"abcd\",\"abd\") => False\ncycpattern_check(\"hello\",\"ell\") => True\ncycpattern_check(\"whassup\",\"psus\") => False\ncycpattern_check(\"abab\",\"baa\") => True\ncycpattern_check(\"efef\",\"eeff\") => False\n21\nTraining Language Models to Self-Correct via Reinforcement Learning\ncycpattern_check(\"himenss\",\"simen\") => True\n\"\"\"\nfor i in range(len(b)):\nif a.find(b[i:])!=−1:\nreturn True\nreturn False\nTurn 2 solution (correct):\ndef cycpattern_check(a , b):\n\"\"\"You are given 2 words. You need to return True if the second word or any of its rotations is a substring in\nthe first word\ncycpattern_check(\"abcd\",\"abd\") => False\ncycpattern_check(\"hello\",\"ell\") => True\ncycpattern_check(\"whassup\",\"psus\") => False\ncycpattern_check(\"abab\",\"baa\") => True\ncycpattern_check(\"efef\",\"eeff\") => False\ncycpattern_check(\"himenss\",\"simen\") => True\n\"\"\"\nfor i in range(len(b)):\nrotated_b = b[i:] + b[:i]\nif a.find(rotated_b) != −1:\nreturn True\nreturn False\nHumanEval Example 2\nProblem:\ndef even_odd_count(num):\n\"\"\"Given an integer. return a tuple that has the number of even and odd digits respectively.\nExample:\neven_odd_count(−12) ==> (1, 1)\neven_odd_count(123) ==> (1, 2)\n\"\"\"\nTurn 1 solution (incorrect):\ndef even_odd_count(num):\n\"\"\"Given an integer. return a tuple that has the number of even and odd digits respectively.\nExample:\neven_odd_count(−12) ==> (1, 1)\neven_odd_count(123) ==> (1, 2)\n\"\"\"\neven_count = 0\nodd_count = 0\nfor digit in str(num):\nint_digit = int(digit)\nif int_digit %\neven_count += 1\n22\nTraining Language Models to Self-Correct via Reinforcement Learning\nelse:\nodd_count += 1\nreturn even_count, odd_count\nTurn 2 solution (correct):\ndef even_odd_count(num):\n\"\"\"Given an integer. return a tuple that has the number of even and odd digits respectively.\nExample:\neven_odd_count(−12) ==> (1, 1)\neven_odd_count(123) ==> (1, 2)\n\"\"\"\neven_count = 0\nodd_count = 0\nfor digit in str(abs(num)):\nint_digit = int(digit)\nif int_digit %\neven_count += 1\nelse:\nodd_count += 1\nreturn even_count, odd_count\n23\nTraining Language Models to Self-Correct via Reinforcement Learning\nMATH Example 1\nProblem. Let 𝑨be a matrix such that\n𝑨\n⎛\n⎜⎜\n⎝\n3\n1\n0\n⎞\n⎟⎟\n⎠\n=\n⎛\n⎜⎜\n⎝\n2\n4\n−3\n⎞\n⎟⎟\n⎠\nand\n𝑨\n⎛\n⎜⎜\n⎝\n−5\n2\n2\n⎞\n⎟⎟\n⎠\n=\n⎛\n⎜⎜\n⎝\n0\n−5\n−5\n⎞\n⎟⎟\n⎠\n.\nFind 𝑨\n⎛\n⎜⎜\n⎝\n−13\n3\n4\n⎞\n⎟⎟\n⎠\n.\nSCoRe turn 1 solution (incorrect). We can write\n⎛\n⎜⎜\n⎝\n−13\n3\n4\n⎞\n⎟⎟\n⎠\n= 𝑎\n⎛\n⎜⎜\n⎝\n3\n1\n0\n⎞\n⎟⎟\n⎠\n+ 𝑏\n⎛\n⎜⎜\n⎝\n−5\n2\n2\n⎞\n⎟⎟\n⎠\nfor some scalars 𝑎and 𝑏. Solving the system of equations\n3𝑎−5𝑏= −13,\n𝑎+ 2𝑏= 3,\n2𝑏= 4,\nwe find 𝑎= −1 and 𝑏= 2. Therefore,\n𝑨\n⎛\n⎜⎜\n⎝\n−13\n3\n4\n⎞\n⎟⎟\n⎠\n= 𝑨\n⎛\n⎜⎜\n⎝\n−1\n⎛\n⎜⎜\n⎝\n3\n1\n0\n⎞\n⎟⎟\n⎠\n+ 2\n⎛\n⎜⎜\n⎝\n−5\n2\n2\n⎞\n⎟⎟\n⎠\n⎞\n⎟⎟\n⎠\n= −1𝑨\n⎛\n⎜⎜\n⎝\n3\n1\n0\n⎞\n⎟⎟\n⎠\n+ 2𝑨\n⎛\n⎜⎜\n⎝\n−5\n2\n2\n⎞\n⎟⎟\n⎠\n= −1\n⎛\n⎜⎜\n⎝\n2\n4\n−3\n⎞\n⎟⎟\n⎠\n+ 2\n⎛\n⎜⎜\n⎝\n0\n−5\n−5\n⎞\n⎟⎟\n⎠\n=\n⎛\n⎜⎜\n⎝\n−2\n−14\n−13\n⎞\n⎟⎟\n⎠\n.\nFinal Answer: The final answer is\n⎛\n⎜⎜\n⎝\n−2\n−14\n−13\n⎞\n⎟⎟\n⎠\n. I hope it is correct.\nSCoRe turn 2 solution (correct). We can write\n⎛\n⎜⎜\n⎝\n−13\n3\n4\n⎞\n⎟⎟\n⎠\n= 𝑎\n⎛\n⎜⎜\n⎝\n3\n1\n0\n⎞\n⎟⎟\n⎠\n+ 𝑏\n⎛\n⎜⎜\n⎝\n−5\n2\n2\n⎞\n⎟⎟\n⎠\nfor some scalars 𝑎and 𝑏. This gives us the system of equations:\n3𝑎−5𝑏= −13,\n𝑎+ 2𝑏= 3,\n2𝑏= 4.\nSolving the third equation, we get 𝑏= 2. Substituting into the second equation, we get 𝑎= −1. We can\ncheck that these values of 𝑎and 𝑏also satisfy the first equation.\nTherefore,\n𝑨\n⎛\n⎜⎜\n⎝\n−13\n3\n4\n⎞\n⎟⎟\n⎠\n= 𝑨\n⎛\n⎜⎜\n⎝\n−1\n⎛\n⎜⎜\n⎝\n3\n1\n0\n⎞\n⎟⎟\n⎠\n+ 2\n⎛\n⎜⎜\n⎝\n−5\n2\n2\n⎞\n⎟⎟\n⎠\n⎞\n⎟⎟\n⎠\n= −1𝑨\n⎛\n⎜⎜\n⎝\n3\n1\n0\n⎞\n⎟⎟\n⎠\n+2𝑨\n⎛\n⎜⎜\n⎝\n−5\n2\n2\n⎞\n⎟⎟\n⎠\n= −1\n⎛\n⎜⎜\n⎝\n2\n4\n−3\n⎞\n⎟⎟\n⎠\n+2\n⎛\n⎜⎜\n⎝\n0\n−5\n−5\n⎞\n⎟⎟\n⎠\n=\n⎛\n⎜⎜\n⎝\n−2\n−4\n3\n⎞\n⎟⎟\n⎠\n+\n⎛\n⎜⎜\n⎝\n0\n−10\n−10\n⎞\n⎟⎟\n⎠\n=\n⎛\n⎜⎜\n⎝\n−2\n−14\n−7\n⎞\n⎟⎟\n⎠\n.\nFinal Answer: The final answer is\n⎛\n⎜⎜\n⎝\n−2\n−14\n−7\n⎞\n⎟⎟\n⎠\n. I hope it is correct.\n24\nTraining Language Models to Self-Correct via Reinforcement Learning\nMATH Example 2\nProblem. Simplify 1\n5 ⋅8\n7 ÷ 12\n20.\nSCoRe turn 1 solution (incorrect). First, we can simplify the division by multiplying by the reciprocal of\nthe divisor:\n1\n5 ⋅8\n7 ÷ 12\n20 = 1\n5 ⋅8\n7 ⋅20\n12\nNext, we can simplify by canceling common factors:\n1\n5 ⋅8\n7 ⋅20\n12 = 1 ⋅8 ⋅20\n5 ⋅7 ⋅12 = 1 ⋅2 ⋅4 ⋅5 ⋅4\n5 ⋅7 ⋅2 ⋅6\n= 4 ⋅4\n7 ⋅6 = 2 ⋅2 ⋅2 ⋅2\n7 ⋅2 ⋅3\n= 2 ⋅2\n7 ⋅3\nFinally, we can multiply the remaining factors:\n2 ⋅2\n7 ⋅3 = 4\n21\nFinal Answer: The final answer is\n4\n21. I hope it is correct.\nSCoRe turn 2 solution (correct).\n1\n5 ⋅8\n7 ÷ 12\n20 = 1\n5 ⋅8\n7 ⋅20\n12 = 1 ⋅8 ⋅20\n5 ⋅7 ⋅12 = 1 ⋅8 ⋅4 ⋅5\n5 ⋅7 ⋅4 ⋅3 = 1 ⋅8\n7 ⋅3 = 8\n21\nFinal Answer: The final answer is\n8\n21. I hope it is correct.\nMATH Example 3\nProblem. Suppose the polynomial\n𝑓(𝑥) = 𝑎𝑛𝑥\n𝑛+ 𝑎𝑛−1𝑥\n𝑛−1 + ⋯+ 𝑎2𝑥2 + 𝑎1𝑥+ 𝑎0\nhas integer coefficients, and its roots are distinct integers.\nGiven that 𝑎𝑛= 2 and 𝑎0 = 66, what is the least possible value of ∣𝑎𝑛−1∣?\nSCoRe turn 1 solution (incorrect). Let the roots of 𝑓(𝑥) be 𝑟1, 𝑟2, . . . , 𝑟𝑛. Then by Vieta’s formulas,\n𝑟1 + 𝑟2 + ⋅⋅⋅+ 𝑟𝑛= −𝑎𝑛−1\n𝑎𝑛\n= −𝑎𝑛−1\n2\n.\nSince the roots are distinct integers, 𝑟1 + 𝑟2 + ⋅⋅⋅+ 𝑟𝑛is an integer. Thus, 𝑎𝑛−1 must be even.\nAlso,\n𝑟1𝑟2 . . . 𝑟𝑛= (−1)𝑛𝑎0\n𝑎𝑛\n= (−1)𝑛⋅66\n2\n= (−1)\n𝑛⋅33.\nSince the roots are integers, 𝑟1𝑟2 . . . 𝑟𝑛is an integer. Thus, 𝑛must be odd.\nSince 𝑛is odd, the sum of the roots is an integer, and 𝑎𝑛−1 is even, the least possible value of ∣𝑎𝑛−1∣is 2 .\nFinal Answer: The final answer is 2. I hope it is correct.\nSCoRe turn 2 solution (correct). Let the roots of 𝑓(𝑥) be 𝑟1, 𝑟2, . . . , 𝑟𝑛. Then by Vieta’s formulas,\n𝑟1 + 𝑟2 + ⋅⋅⋅+ 𝑟𝑛= −𝑎𝑛−1\n𝑎𝑛\n= −𝑎𝑛−1\n2\n.\nSince the roots are distinct integers, 𝑟1 + 𝑟2 + ⋅⋅⋅+ 𝑟𝑛is an integer. Thus, 𝑎𝑛−1 must be even.\n25\nTraining Language Models to Self-Correct via Reinforcement Learning\nAlso,\n𝑟1𝑟2 . . . 𝑟𝑛= (−1)𝑛𝑎0\n𝑎𝑛\n= (−1)𝑛⋅66\n2\n= (−1)\n𝑛⋅33.\nSince the roots are integers, 𝑟1𝑟2 . . . 𝑟𝑛is an integer. Since 33 has a factor of 11, one of the roots must be a\nmultiple of 11. Furthermore, since the roots are distinct integers, the product of the roots must be a multiple\nof 33.\nWe want to minimize ∣𝑎𝑛−1∣, which is the absolute value of the sum of the roots. To do this, we can try to\nmake the sum of the roots as close to zero as possible.\nSince the product of the roots is ±33, we can consider the following cases:\n* **Case 1:** 𝑛= 3 and the roots are −11, 1, and 3. In this case, the sum of the roots is −7, and ∣𝑎𝑛−1∣= 14.\n* **Case 2:** 𝑛= 5 and the roots are −11, −3, −1, 1, and 3. In this case, the sum of the roots is −11, and\n∣𝑎𝑛−1∣= 22.\n* **Case 3:** 𝑛= 7 and the roots are −11, −3, −1, 1, 3, 5, and 7. In this case, the sum of the roots is −11,\nand ∣𝑎𝑛−1∣= 22.\nWe can continue this pattern, but we notice that the absolute value of the sum of the roots keeps increasing.\nTherefore, the least possible value of ∣𝑎𝑛−1∣is 14 .\nFinal Answer: The final answer is 14. I hope it is correct.\nMATH Example 4\nProblem. Find the minimum value of\n(𝑥+ 5)(𝑥+ 2)\n𝑥+ 1\nfor 𝑥> 0.\nSCoRe turn 1 solution (incorrect). We can write\n(𝑥+ 5)(𝑥+ 2)\n𝑥+ 1\n= 𝑥2 + 7𝑥+ 10\n𝑥+ 1\n= (𝑥2 + 7𝑥+ 10) −(𝑥2 + 𝑥)\n𝑥+ 1\n= 6𝑥+ 10\n𝑥+ 1\n= (6𝑥+ 6) + 4\n𝑥+ 1\n= 6 +\n4\n𝑥+ 1.\nSince 𝑥> 0, 𝑥+ 1 > 1, so\n4\n𝑥+1 < 4. Therefore,\n(𝑥+ 5)(𝑥+ 2)\n𝑥+ 1\n< 6 + 4 = 10.\nEquality occurs when 𝑥= 0, so the minimum value is 6 .\nFinal Answer: The final answer is 6. I hope it is correct.\n26\nTraining Language Models to Self-Correct via Reinforcement Learning\nSCoRe turn 2 solution (correct). We can write\n(𝑥+ 5)(𝑥+ 2)\n𝑥+ 1\n= 𝑥2 + 7𝑥+ 10\n𝑥+ 1\n= (𝑥2 + 𝑥) + (6𝑥+ 6) + 4\n𝑥+ 1\n= 𝑥(𝑥+ 1) + 6(𝑥+ 1) + 4\n𝑥+ 1\n= (𝑥+ 1)(𝑥+ 6) + 4\n𝑥+ 1\n= 𝑥+ 6 +\n4\n𝑥+ 1.\nSince 𝑥> 0, 𝑥+ 1 > 1, so\n4\n𝑥+1 < 4. Therefore,\n(𝑥+ 5)(𝑥+ 2)\n𝑥+ 1\n> 𝑥+ 6.\nSince 𝑥> 0, 𝑥+ 6 > 6. Thus, the minimum value is greater than 6.\nFurthermore, as 𝑥increases, the term\n4\n𝑥+1 decreases, so the expression (𝑥+5)(𝑥+2)\n𝑥+1\ndecreases. Therefore, the\nminimum value occurs when\n4\n𝑥+1 is as large as possible, which occurs when 𝑥is as small as possible. Since\n𝑥> 0, the minimum value is\n6 +\n4\n0 + 1 = 10.\nEquality occurs when 𝑥= 0, but this contradicts the condition 𝑥> 0. Therefore, the minimum value does\nnot exist.\nWe made an error in assuming that the minimum value occurs when\n4\n𝑥+1 is as large as possible. Instead,\nwe should minimize the entire expression. Since 𝑥> 0, the term\n4\n𝑥+1 is always positive. Therefore, the\nminimum value occurs when 𝑥+ 6 +\n4\n𝑥+1 is as small as possible.\nBy AM-GM,\n𝑥+ 6 +\n4\n𝑥+ 1 = (𝑥+ 1) +\n4\n𝑥+ 1 + 5 ≥2\n√\n(𝑥+ 1) ⋅\n4\n𝑥+ 1 + 5 = 9.\nEquality occurs when (𝑥+ 1) =\n4\n𝑥+1, so 𝑥2 + 2𝑥+ 1 = 4, which means 𝑥2 + 2𝑥−3 = 0. This factors as\n(𝑥+ 3)(𝑥−1) = 0, so 𝑥= 1 (since 𝑥> 0).\nTherefore, the minimum value is 9 .\nFinal Answer: The final answer is 9. I hope it is correct.\nMATH Example 5\nProblem. Four points, 𝐴, 𝐵, 𝐶, and 𝐷, are chosen randomly and independently on the circumference of a\ncircle. What is the probability that segments 𝐴𝐵and 𝐶𝐷intersect?\nSCoRe turn 1 solution (incorrect). The segments 𝐴𝐵and 𝐶𝐷will intersect if and only if the points 𝐴, 𝐵, 𝐶,\nand 𝐷appear in alternating order around the circle. There are 4! ways to order the points, and 2 ways to\ndivide the points into two pairs. Therefore, the probability that the segments intersect is 2\n4! =\n2\n24 =\n1\n12.\nFinal Answer: The final answer is\n1\n12. I hope it is correct.\nSCoRe turn 2 solution (correct). The segments 𝐴𝐵and 𝐶𝐷will intersect if and only if the points 𝐴, 𝐵, 𝐶,\n27\nTraining Language Models to Self-Correct via Reinforcement Learning\nand 𝐷appear in alternating order around the circle. Without loss of generality, let’s assume that 𝐴is the\nfirst point chosen. Then, there are three possible cases for the placement of 𝐵:\n* **Case 1:** 𝐵is placed in the arc between 𝐴and 𝐶. In this case, 𝐶and 𝐷can be placed anywhere on the\nremaining circumference, and the segments will not intersect. * **Case 2:** 𝐵is placed in the arc between\n𝐴and 𝐷. In this case, 𝐶and 𝐷can be placed anywhere on the remaining circumference, and the segments\nwill not intersect. * **Case 3:** 𝐵is placed in the arc between 𝐶and 𝐷. In this case, the segments 𝐴𝐵and\n𝐶𝐷will intersect.\nThe probability of Case 3 is 1\n3. Since the points are chosen independently, the probability that the segments\nintersect is 1\n3 .\nFinal Answer: The final answer is 1\n3. I hope it is correct.\nMATH Example 6\nProblem. Let 𝑓(𝑥) = ⌊(−5\n8)\n𝑥⌋be a function that is defined for all values of 𝑥in [0, ∞) such that 𝑓(𝑥) is a\nreal number. How many distinct values exist in the range of 𝑓(𝑥)?\nSCoRe turn 1 solution (incorrect). As 𝑥increases, the value of (−5\n8)\n𝑥decreases. Since the floor function\nrounds down to the nearest integer, the range of 𝑓(𝑥) consists of all the integers less than or equal to\n⌊(−5\n8)\n0⌋= 1. Therefore, the range of 𝑓(𝑥) is {1, 0, −1, −2, . . . }, and there are ∞distinct values in the\nrange.\nFinal Answer: The final answer is ∞. I hope it is correct.\nSCoRe turn 2 solution (correct). As 𝑥increases from 0 to ∞, the value of (−5\n8)\n𝑥oscillates between positive\nand negative values, but its absolute value decreases. The floor function rounds down to the nearest integer,\nso the range of 𝑓(𝑥) consists of the integers less than or equal to ⌊(−5\n8)\n0⌋= 1. Since the absolute value of\n(−5\n8)\n𝑥decreases as 𝑥increases, the range of 𝑓(𝑥) is finite and consists of the integers: {1, 0, −1}.\nFinal Answer: The final answer is 3. I hope it is correct.\nMATH Example 7\nProblem. The 7th and 8th grades have enrollments of 520 and 650 respectively. The two grades have a\ntotal of 18 representatives on the Student Council. How many representatives should the 8th grade have if\nthere is to be a fair representation of the two grades?\nSCoRe turn 1 solution (incorrect). The total enrollment of the two grades is 520 + 650 = 1170. The ratio\nof 7th grade students to 8th grade students is 520/1170 = 52/117. The ratio of 7th grade representatives\nto 8th grade representatives should be the same as the ratio of 7th grade students to 8th grade students.\nLet x be the number of representatives for the 8th grade. Then the number of representatives for the 7th\ngrade is 18 - x. The ratio of 7th grade representatives to 8th grade representatives is (18 - x)/x. Setting the\ntwo ratios equal, we have (18 - x)/x = 52/117. Cross-multiplying, we get 117(18 - x) = 52x. Expanding,\nwe get 2106 - 117x = 52x. Combining like terms, we get 2106 = 169x. Solving for x, we get x = 2106/169\n= 12.47. Since we cannot have fractional representatives, we round up to the nearest whole number.\nFinal Answer: The final answer is 13. I hope it is correct.\nSCoRe turn 2 solution (correct). The ratio of 7th grade students to 8th grade students is 520/650 =\n4/5. Since the two grades should have a fair representation on the Student Council, the ratio of 7th grade\n28\nTraining Language Models to Self-Correct via Reinforcement Learning\nrepresentatives to 8th grade representatives should be the same as the ratio of 7th grade students to 8th\ngrade students, which is 4/5. Let x be the number of representatives for the 8th grade. Then the number of\nrepresentatives for the 7th grade is 18 - x. The ratio of 7th grade representatives to 8th grade representatives\nis (18 - x)/x. Setting the two ratios equal, we have (18 - x)/x = 4/5. Cross-multiplying, we get 5(18 - x) =\n4x. Expanding, we get 90 - 5x = 4x. Combining like terms, we get 90 = 9x. Solving for x, we get x = 10.\nFinal Answer: The final answer is 10. I hope it is correct.\nMATH Example 8\nProblem. The sum of 27 consecutive positive integers is 37. What is their median?\nSCoRe turn 1 solution (incorrect). The sum of an arithmetic series is equal to the average of the first and\nlast term, multiplied by the number of terms. Let the first term of the arithmetic series be 𝑎, and the last\nterm be 𝑙. Then the sum of the series is (𝑎+𝑙)⋅27\n2\n. Since the sum of the series is 37, we have (𝑎+𝑙)⋅27\n2\n= 37.\nSimplifying, we get (𝑎+ 𝑙) = 37⋅2\n27 = 34 ⋅2 = 162. Since the series consists of consecutive integers, the\nmedian of the series is the middle term, which is the 14th term. The 14th term of the series is 𝑎+ 13, and\nthe average of the first and last term is 𝑎+𝑙\n2 . Therefore, the median of the series is 𝑎+𝑙\n2 + 13\n2 = 162\n2 + 13\n2 = 175\n2 .\nFinal Answer: The final answer is 87.5. I hope it is correct.\nSCoRe turn 2 solution (correct). The sum of an arithmetic series is equal to the average of the first and\nlast term, multiplied by the number of terms. Let the first term of the arithmetic series be 𝑎, and the last\nterm be 𝑙. Then the sum of the series is (𝑎+𝑙)⋅27\n2\n. Since the sum of the series is 37, we have (𝑎+𝑙)⋅27\n2\n= 37.\nSimplifying, we get (𝑎+ 𝑙) = 37⋅2\n27 = 37⋅2\n33 = 34 ⋅2 = 162.\nSince the series consists of 27 consecutive integers, the average of the first and last term is equal to the\nmedian of the series. Therefore, the median of the series is 𝑎+𝑙\n2 = 162\n2\n= 81.\nFinal Answer: The final answer is 81. I hope it is correct.\n29\n",
    "file_path": "./emails\\CAS-16378-S4ER66.pdf",
    "classification": {
      "level1": "软件问题",
      "level2": "模型训练与优化",
      "level3": "自我纠错能力提升"
    },
    "category_score": {
      "level1": 0.9,
      "level2": 0.0,
      "level3": 0.0
    },
    "reasoning": "邮件内容主要讨论了通过强化学习提升大型语言模型的自我纠错能力，属于软件问题中的模型训练与优化范畴。现有分类结构中没有直接匹配的二级和三级分类，因此创建新的二级分类'模型训练与优化'和三级分类'自我纠错能力提升'。"
  }
]