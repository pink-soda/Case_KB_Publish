[
  {
    "content": "æ”¹é€  Dify å®ç°ç”Ÿäº§å¯ç”¨çš„\nAI Agent åº”ç”¨è½åœ°\né™ˆè¿ªè±ª | é¡ºä¸°ç§‘æŠ€\næ¼”è®²å˜‰å®¾\né™ˆè¿ªè±ª\né¡ºä¸°ç§‘æŠ€ AI æŠ€æœ¯å¹³å°é«˜çº§å·¥ç¨‹å¸ˆ\nç›®å‰æ‹…ä»»é¡ºä¸°ç§‘æŠ€ AI æŠ€æœ¯å¹³å°é«˜çº§å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£é¡ºä¸°é›†å›¢ AI å’Œå¤§æ¨¡å‹\nåŸºç¡€æ¶æ„åŠŸèƒ½ï¼Œæ›¾ä»»ç¬¬å››èŒƒå¼å¹³å°æ¶æ„å¸ˆå’Œ OpenMLDB é¡¹ç›® PMCï¼Œè¿‡\nå»åœ¨å°ç±³æ‹…ä»»äº‘æ·±åº¦å­¦ä¹ å¹³å°æ¶æ„å¸ˆä»¥åŠä¼˜æ€å¾·äº‘è®¡ç®—å…¬å¸å­˜å‚¨å’Œå®¹å™¨å›¢\né˜Ÿè´Ÿè´£äººã€‚æ´»è·ƒäºåˆ†å¸ƒå¼ç³»ç»Ÿã€æœºå™¨å­¦ä¹ ç›¸å…³çš„å¼€æºç¤¾åŒºï¼Œä¹Ÿæ˜¯HBaseã€\nOpenStackã€TensorFlowã€TVM ç­‰å¼€æºé¡¹ç›®è´¡çŒ®è€…ã€‚\nç›® å½•\nCONTENTS\n1.\nä»‹ç» Dify å¼€å‘å¹³å°\n2.\næ”¹é€  Dify å¼€å‘å¹³å°\n3.\nè½åœ°è½¯ä»¶æœºå™¨äºº Agent åœºæ™¯\n4.\nè½åœ°æ•°æ®å¹³å° Agent åœºæ™¯\n5.\nDify å®è·µæ€»ç»“\nä»‹ç» Dify å¼€å‘å¹³å°\nPART 01\nä»‹ç» Dify å¼€å‘å¹³å° - What\nâ€¢ ç®€ä»‹ï¼šDifyæ˜¯ä¸€ä¸ªä½ä»£ç AIå¹³å°ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·æ›´å¿«æ·åœ°æ„å»ºAIé©±åŠ¨çš„åº”ç”¨ç¨‹åºã€‚\nâ€¢ ç‰¹ç‚¹ï¼šæ¨¡å—åŒ–è®¾è®¡ã€æ˜“äºé›†æˆã€æä¾›å¤šç§AIæœåŠ¡æ¥å£ã€‚\nâ€¢ ä¼˜åŠ¿ï¼šä½ä»£ç å¼€å‘ã€é«˜æ•ˆéƒ¨ç½²ã€é¢å‘ä¸šåŠ¡éœ€æ±‚çš„AIæ”¯æŒã€‚\nä»‹ç» Dify å¼€å‘å¹³å° - Why\n1. ä½ä»£ç å¼€å‘ï¼Œç¼©çŸ­å¼€å‘å‘¨æœŸ\nâ€¢\nDifyæ”¯æŒä½ä»£ç å¼€å‘ï¼Œç”¨æˆ·åªéœ€å°‘é‡ç¼–ç å³å¯æ„å»ºå¤æ‚çš„AIåº”ç”¨ï¼Œæå¤§åœ°ç¼©çŸ­äº†å¼€å‘å‘¨æœŸã€‚\nâ€¢\nå¯è§†åŒ–çš„ç•Œé¢è®¾è®¡å’ŒAPIé›†æˆå·¥å…·ä½¿å¾—å¼€å‘æµç¨‹æ›´ä¸ºç®€æ´ï¼Œå³ä¾¿æ²¡æœ‰æ·±åšçš„ç¼–ç¨‹èƒŒæ™¯ä¹Ÿå¯ä»¥å¿«é€Ÿä¸Šæ‰‹ã€‚\n2. æ¨¡å—åŒ–æ¶æ„ï¼Œæ”¯æŒçµæ´»æ‰©å±•\nâ€¢\nDifyçš„æ¨¡å—åŒ–è®¾è®¡ä½¿å¹³å°æ›´å…·æ‰©å±•æ€§ï¼Œç”¨æˆ·å¯ä»¥æŒ‰éœ€é€‰æ‹©å’Œç»„åˆAIæ¨¡å—ï¼Œé€‚åº”å„ç§åº”ç”¨åœºæ™¯ã€‚\nâ€¢\nå¹³å°æ”¯æŒä¸åŒAIæ¨¡å‹çš„æ¥å…¥å’Œåˆ‡æ¢ï¼Œèƒ½æ»¡è¶³ä¸åŒç±»å‹çš„ä»»åŠ¡éœ€æ±‚ï¼Œå¦‚æ–‡æœ¬å¤„ç†ã€å›¾åƒåˆ†æã€æ•°æ®é¢„æµ‹ç­‰ã€‚\n3. é«˜æ•ˆéƒ¨ç½²ï¼Œå¿«é€Ÿä¸Šçº¿\nâ€¢\nDifyé›†æˆäº†ç®€åŒ–çš„éƒ¨ç½²å·¥å…·ï¼Œä½¿å¾—AIåº”ç”¨å¯ä»¥æ›´å¿«é€Ÿåœ°ä¸Šçº¿ã€‚\nâ€¢\næä¾›äº‘ç«¯éƒ¨ç½²å’Œæœ¬åœ°éƒ¨ç½²é€‰é¡¹ï¼Œæ”¯æŒä¼ä¸šæ ¹æ®éœ€æ±‚çµæ´»é€‰æ‹©éƒ¨ç½²ç¯å¢ƒï¼Œæœ‰æ•ˆåº”å¯¹æ•°æ®å®‰å…¨å’Œéšç§è¦æ±‚ã€‚\n4. å¤šæ¨¡å‹æ”¯æŒï¼Œæ»¡è¶³å¤šæ ·åŒ–éœ€æ±‚\nâ€¢\nDifyå…¼å®¹ä¸»æµçš„AIæ¨¡å‹ï¼Œå¹¶æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹çš„é›†æˆï¼Œç”¨æˆ·å¯ä»¥åœ¨å¹³å°å†…è°ƒå–ä¸åŒçš„æ¨¡å‹æœåŠ¡ï¼Œä»¥å®ç°ä»æ•°æ®åˆ†æåˆ°è‡ªç„¶è¯­è¨€å¤„ç†çš„å¤šæ ·åŒ–éœ€æ±‚ã€‚\nâ€¢\næ¨¡å‹çš„çµæ´»æ›¿æ¢å’Œå‡çº§ä½¿å¹³å°æ›´å…·é€‚åº”æ€§å’Œå‰ç»æ€§ã€‚\n5. èŠ‚çœæˆæœ¬ï¼Œæé«˜æ•ˆç‡\nâ€¢\nä½ä»£ç çš„å¼€å‘æ¨¡å¼å’Œä¾¿æ·çš„æ¨¡å‹ç®¡ç†å·¥å…·å¤§å¹…é™ä½äº†å¼€å‘å’Œç»´æŠ¤æˆæœ¬ã€‚\nâ€¢\né€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹å’ŒAIé©±åŠ¨çš„æ™ºèƒ½åº”ç”¨ï¼ŒDifyå¸®åŠ©ä¼ä¸šæé«˜äº†å·¥ä½œæ•ˆç‡ï¼Œå‡å°‘äº†äººå·¥å¹²é¢„ï¼ŒèŠ‚çœäº†è¿è¥æˆæœ¬ã€‚\nä»‹ç» Dify å¼€å‘å¹³å° - How\nä»‹ç» Dify å¼€å‘å¹³å° - Butï¼Ÿ\nâ€¢ å…ƒæ•°æ®å­˜å‚¨å®ç°æ–¹å¼å•ä¸€\nâ€¢ RAGèƒ½åŠ›æœ‰é™ï¼Œä¸æ”¯æŒæœ€æ–°æ£€ç´¢ç®—æ³•\nâ€¢ å‘é‡æ•°æ®åº“æ”¯æŒä¸å…¨\nâ€¢ åŸç”Ÿéƒ¨ç½²ä¸æ”¯æŒé«˜å¯ç”¨\nâ€¢ ä¸æ”¯æŒåˆ é™¤ message æ¥å£\nâ€¢ ç¤¾åŒºç‰ˆä¸æ”¯æŒæ¨¡å‹æœåŠ¡è´Ÿè½½å‡è¡¡\nâ€¢ â€¦â€¦\nä»‹ç» Dify å¼€å‘å¹³å° - é¡ºä¸°é›†æˆç‰ˆ\næ”¹é€  Dify å¼€å‘å¹³å°\nPART 02\næ”¹é€  Dify å¼€å‘å¹³å° - éƒ¨ç½²å¢å¼º\nDify\nPostgreSQL\nDify\nPostgreSQL\nMySQL\n(æ–°å¢ MySQL é…ç½®é¡¹ï¼Œéœ€æ”¹æ•°\næ®åº“å’Œè¡¨ç»“æ„ï¼ŒSQLè¯­å¥ä¿®æ”¹)\næ”¹é€  Dify å¼€å‘å¹³å° - éƒ¨ç½²å¢å¼º\nDify\nRedis\nDify\nRedis Sentinel\nRedis Sentinel\nRedis Sentinel\næ”¹é€  Dify å¼€å‘å¹³å° - æ£€ç´¢å¢å¼º\nâ€¢ æ–°å¢ ElasticSearch 8æ”¯æŒï¼Œæ”¯æŒæ··åˆæ£€ç´¢\nâ€¢ æ–°å¢ ES ç›¸å…³é…ç½®ï¼Œæ”¯æŒ ES Vector ç»„ä»¶\nâ€¢ æ”¯æŒ Contextual Retrieval (#8776)\nâ€¢ æ”¯æŒ GraphRAG / LightRAG (#6019)\næ”¹é€  Dify å¼€å‘å¹³å° - é›†æˆå†…éƒ¨æœåŠ¡\né›†æˆç§æœ‰åŒ–å¤§æ¨¡å‹\nâ€¢ å¼€æºå¤§æ¨¡å‹\nâ€¢ å¾®è°ƒé¢†åŸŸæ¨¡å‹\nâ€¢ å•†ä¸šæ¨¡å‹æœåŠ¡\nDify å†…ç½®å·¥å…·\nâ€¢ å‘é‡æ··åˆæ£€ç´¢å·¥å…·\nâ€¢ æ•°æ®å¹³å°è¡¨æ¥å£æŸ¥è¯¢æœåŠ¡\nDify æ¥å£æ”¹é€ \nâ€¢ ä¿®æ”¹ LLM èŠ‚ç‚¹è¾“å…¥è·å–æ–¹æ³•\nâ€¢ æ–°å¢åˆ é™¤ messages API\næ”¹é€  Dify å¼€å‘å¹³å° - Ongoing\nâ€¢ æ›´å¤š RAG ç®—æ³•é›†æˆï¼šGraphRAGã€LightRAG ç­‰\nâ€¢ å¤š Agent è°ƒåº¦ç³»ç»Ÿé›†æˆï¼šOpenAI Swarmã€MetaGPT ç­‰\nâ€¢ ç”Ÿäº§ç‰¹æ€§å¼€å‘ï¼šé«˜å¯ç”¨ã€è´Ÿè½½å‡è¡¡çš„æ¨¡å‹æœåŠ¡æ”¯æŒç­‰\nâ€¢ äº§å“åŒ–é›†æˆï¼šå¯¹æ ‡ HiAgent å’Œ Coze ç­‰å•†ä¸šäº§å“\nâ€¢ æ›´å¤šåº”ç”¨å’Œæµç¨‹é›†æˆï¼š\nâ€£  æ•°æ®å¢å¼ºå·¥ä½œæµ\nâ€£  å…¨å¹³å°è½¯ä»¶åŠ©æ‰‹\nâ€£  æ··åˆäº‘æ¨ç†æœåŠ¡\nâ€£  â€¦â€¦\nè½åœ°è½¯ä»¶æœºå™¨äºº Agent åœºæ™¯\nPART 03\nè½åœ°è½¯ä»¶æœºå™¨äºº Agent åœºæ™¯ - èƒŒæ™¯\nRPAï¼ˆRobotic Process Automationï¼Œæœºå™¨äººæµç¨‹è‡ªåŠ¨åŒ–ï¼‰æ˜¯ä¸€ç§\né€šè¿‡è½¯ä»¶æœºå™¨äººï¼ˆbotï¼‰æ¥æ¨¡æ‹Ÿäººç±»ç”¨æˆ·åœ¨åº”ç”¨ç¨‹åºä¸­æ‰§è¡Œè§„åˆ™\nåŒ–ä»»åŠ¡çš„æŠ€æœ¯ï¼Œæ—¨åœ¨ç®€åŒ–æµç¨‹ã€æé«˜æ•ˆç‡å¹¶å‡å°‘äººä¸ºé”™è¯¯ã€‚éšç€\nå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå°† RPA ä¸å¤§æ¨¡å‹é›†æˆï¼Œèƒ½å¤Ÿ\nè¿›ä¸€æ­¥æ‰©å±•å…¶åº”ç”¨åœºæ™¯ï¼Œå®ç°æ›´æ™ºèƒ½çš„è‡ªåŠ¨åŒ–ã€‚\nè½åœ°è½¯ä»¶æœºå™¨äºº Agent åœºæ™¯ - RPA vs Workflow\nç‰¹æ€§\nRPA (æœºå™¨â¼ˆæµç¨‹â¾ƒåŠ¨åŒ–)\nâ¼¯ä½œæµ (Workflow)\nå®šä¹‰\næ¨¡æ‹Ÿâ¼ˆç±»â½¤æˆ·æ“ä½œçš„â¾ƒåŠ¨åŒ–æŠ€æœ¯\nâ¼€ç³»åˆ—ä»»åŠ¡æˆ–æ´»åŠ¨çš„â¾ƒåŠ¨åŒ–å¤„ç†\nåº”â½¤åœºæ™¯\nâ¾¼åº¦é‡å¤ã€æ ‡å‡†åŒ–çš„ä»»åŠ¡ï¼Œå¦‚æ•°æ®è¾“â¼Šå’Œè¿ç§»\nå¤æ‚çš„ä¸šåŠ¡æµç¨‹ï¼Œå¦‚å®¡æ‰¹æµç¨‹ã€é¡¹â½¬ç®¡ç†\næŠ€æœ¯å®ç°\né€šè¿‡è½¯ä»¶æœºå™¨â¼ˆä¸â½¤æˆ·ç•Œâ¾¯äº¤äº’\né€šè¿‡è§„åˆ™å’Œé€»è¾‘æ„å»ºâ¾ƒåŠ¨åŒ–æµç¨‹\nçµæ´»æ€§ä¸å¯æ‰©å±•æ€§\nçµæ´»æ€§è¾ƒä½ï¼Œä¸»è¦é€‚â½¤äºæ ‡å‡†åŒ–ä»»åŠ¡\næ›´å…·çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œé€‚åº”åŠ¨æ€ä¸šåŠ¡éœ€æ±‚\nâ½¤æˆ·å‚ä¸\né€šå¸¸æ˜¯å®Œå…¨â¾ƒåŠ¨åŒ–ï¼Œâ½¤æˆ·â¼²é¢„è¾ƒå°‘\næ¶‰åŠå¤šâ½…å‚ä¸è€…ï¼Œâ½€æŒåä½œå’Œæ²Ÿé€š\nç›‘æ§ä¸åˆ†æ\nå…³æ³¨æœºå™¨â¼ˆçš„æ€§èƒ½å’Œæ‰§â¾çŠ¶æ€\næä¾›å…¨â¾¯çš„ç›‘æ§å’Œåˆ†æèƒ½â¼’ï¼Œå¯¹æ•´ä¸ªä¸šåŠ¡æµç¨‹è¿›â¾åˆ†æ\né›†æˆèƒ½â¼’\nèƒ½å¤Ÿé›†æˆç°æœ‰åº”â½¤ç¨‹åºå’Œç³»ç»Ÿï¼Œä¸éœ€è¦é‡æ„\né€šå¸¸éœ€è¦ä¸ä¸šåŠ¡ç³»ç»Ÿã€æ•°æ®åº“ã€API ç­‰é›†æˆ\nè½åœ°è½¯ä»¶æœºå™¨äºº Agent åœºæ™¯ - RPA with Dify\nâ€¢ ç›¸æ¯”äºä¼ ç»Ÿå·¥ä½œæµï¼ŒDify å¤§æ¨¡å‹èƒ½åŠ›å¯ä»¥å®ç°æ„å›¾è¯†åˆ«ã€ä»£ç ç”Ÿæˆç­‰åŠŸèƒ½\nâ€¢ é€šè¿‡çŸ¥è¯†åº“å¼•å…¥è®©å¤§æ¨¡å‹ç”Ÿæˆç»“æœæ›´åŠ å¯ç”¨\nâ€¢ é€šè¿‡å¾®è°ƒæ¨¡å‹è®©æ„å›¾è¯†åˆ«å‡†ç¡®ç‡å’Œæ€§èƒ½æ›´å¥½\nâ€¢ é€šè¿‡æµç¨‹æ‹†è§£ã€æ­¥éª¤æ‹†è§£ã€ä»£ç ç”Ÿæˆç­‰å¤šæ­¥å®ç°äººå·¥å‚ä¸è°ƒä¼˜\nâ€¢ æœ€ç»ˆå®ç°ç«¯åˆ°ç«¯è‡ªç„¶è¯­è¨€åˆ° RPA æµç¨‹çš„ AI åº”ç”¨è½åœ°\nè½åœ°è½¯ä»¶æœºå™¨äºº Agent åœºæ™¯ - RPA with Dify\nâ€¢ çŸ¥è¯†åº“æ­å»ºï¼Œå¼•å…¥ RPA é¢†åŸŸçŸ¥è¯†\nè½åœ°è½¯ä»¶æœºå™¨äºº Agent åœºæ™¯ - RPA with Dify\nâ€¢ å¤šåœºæ™¯å·¥ä½œæµï¼Œå®ç°å¤šæ­¥ä»»åŠ¡æ‹†è§£\nè½åœ°æ•°æ®å¹³å° Agent åœºæ™¯\nPART 04\nè½åœ°æ•°æ®å¹³å°Agentåœºæ™¯ - èƒŒæ™¯\n1. æ•°æ®å¤„ç†ä¸åˆ†æ\nâ€¢ æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†ï¼šåˆ©ç”¨ Dify çš„ RAGï¼ˆæ£€ç´¢å¢å¼º\nç”Ÿæˆï¼‰åŠŸèƒ½ï¼Œä»å„ç§æ–‡æ¡£æ ¼å¼ï¼ˆå¦‚ PDF å’Œ PPTï¼‰ä¸­\næå–æ–‡æœ¬ï¼Œè¿›è¡Œæ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†ã€‚ \nâ€¢ æ•°æ®åˆ†æä¸æŠ¥å‘Šç”Ÿæˆï¼šé€šè¿‡ Dify çš„æ¨¡å‹é›†æˆå’Œæç¤º\nè¯è®¾è®¡ï¼Œè‡ªåŠ¨åˆ†æå¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”Ÿæˆç»“æ„åŒ–çš„åˆ†\nææŠ¥å‘Šï¼Œè¾…åŠ©å†³ç­–ã€‚\n2. æ™ºèƒ½é—®ç­”ä¸çŸ¥è¯†åº“æ„å»º\nâ€¢ ä¼ä¸šçŸ¥è¯†åº“é—®ç­”ï¼šå°†ä¼ä¸šå†…éƒ¨çŸ¥è¯†åº“ä¸ Dify é›†æˆï¼Œ\næ„å»ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œæå‡å‘˜å·¥å’Œå®¢æˆ·çš„æŸ¥è¯¢æ•ˆç‡ã€‚ \nâ€¢ å®¢æˆ·æœåŠ¡è‡ªåŠ¨åŒ–ï¼šåˆ©ç”¨ Dify çš„ AI æ™ºèƒ½ä½“åŠŸèƒ½ï¼Œè‡ª\nåŠ¨å¤„ç†å®¢æˆ·å’¨è¯¢ï¼Œæä¾›å®æ—¶ã€å‡†ç¡®çš„æœåŠ¡ã€‚\n3. æ•°æ®é©±åŠ¨çš„ä¸šåŠ¡æµç¨‹è‡ªåŠ¨åŒ–\nâ€¢ è‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒï¼šç»“åˆ Dify çš„ AI æ™ºèƒ½ä½“å’Œå·¥ä½œæµ\nç¼–æ’åŠŸèƒ½ï¼Œå®ç°åŸºäºæ•°æ®çš„è‡ªåŠ¨åŒ–å†³ç­–æµç¨‹ï¼Œæå‡\nä¸šåŠ¡æ•ˆç‡ã€‚\nâ€¢ ä¸šåŠ¡æµç¨‹ä¼˜åŒ–ï¼šé€šè¿‡ Dify çš„å¯è§†åŒ–å·¥å…·ï¼Œåˆ†æå’Œ\nä¼˜åŒ–ä¸šåŠ¡æµç¨‹ï¼Œå‡å°‘äººå·¥å¹²é¢„ï¼Œé™ä½æˆæœ¬ã€‚\n4. å¤šæ¨¡æ€æ•°æ®å¤„ç†\nâ€¢ æ–‡æœ¬ä¸å›¾åƒæ•°æ®èåˆï¼šåˆ©ç”¨ Dify çš„æ¨¡å‹æ”¯æŒï¼Œå¤„\nç†æ–‡æœ¬å’Œå›¾åƒç­‰å¤šæ¨¡æ€æ•°æ®ï¼Œå®ç°ç»¼åˆåˆ†æã€‚\nâ€¢ å¤šè¯­è¨€æ•°æ®å¤„ç†ï¼šDify æ”¯æŒå¤šè¯­è¨€æ¨¡å‹ï¼Œé€‚ç”¨äºå…¨\nçƒåŒ–ä¸šåŠ¡çš„æ•°æ®å¤„ç†éœ€æ±‚ã€‚\nè½åœ°æ•°æ®å¹³å°Agentåœºæ™¯ - æ™ºèƒ½é”™è¯¯å½’å› \nâ€¢ æ™ºèƒ½é”™è¯¯å½’å› åˆ†æ\nè½åœ°æ•°æ®å¹³å°Agentåœºæ™¯ - æ™ºèƒ½é”™è¯¯å½’å› \nè½åœ°æ•°æ®å¹³å°Agentåœºæ™¯ - SQL ç”Ÿæˆå’Œè‡ªåŠ¨æŸ¥è¡¨\nSQL ç”Ÿæˆ\nâ€¢ è‡ªç„¶è¯­è¨€è½¬ SQLï¼šç”¨æˆ·åªéœ€ä»¥è‡ªç„¶è¯­è¨€æè¿°æ‰€éœ€çš„æ•°æ®æŸ¥è¯¢ï¼Œå¤§æ¨¡å‹ä¼šè§£æè¯¥æè¿°å¹¶ç”Ÿæˆç›¸åº”çš„ SQL æŸ¥è¯¢ã€‚\nâ€¢ å¤æ‚æŸ¥è¯¢æ”¯æŒï¼šå¤§æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤æ‚çš„æŸ¥è¯¢é€»è¾‘ï¼ŒåŒ…æ‹¬å¤šè¡¨è”æ¥ã€èšåˆå‡½æ•°ã€æ¡ä»¶è¿‡æ»¤ç­‰ï¼Œç”¨æˆ·åªéœ€æä¾›ä¸š\nåŠ¡é€»è¾‘å³å¯ã€‚\nâ€¢ åŠ¨æ€æŸ¥è¯¢ç”Ÿæˆï¼šæ ¹æ®è¾“å…¥çš„å‚æ•°åŠ¨æ€ç”Ÿæˆ SQL æŸ¥è¯¢ï¼Œæ”¯æŒçµæ´»çš„æŸ¥è¯¢éœ€æ±‚ï¼Œå¦‚æ’åºã€åˆ†ç»„å’Œé™åˆ¶ç»“æœé›†ã€‚\nè‡ªåŠ¨æŸ¥è¡¨åŠŸèƒ½\nâ€¢ è¡¨ç»“æ„ç†è§£ï¼šå¤§æ¨¡å‹å¯ä»¥è§£ææ•°æ®åº“çš„è¡¨ç»“æ„ä¿¡æ¯ï¼ˆå¦‚è¡¨åã€å­—æ®µååŠå…¶æ•°æ®ç±»å‹ï¼‰ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£\næ•°æ®åº“ schemaã€‚\nâ€¢ æµ·é‡è¡¨æŸ¥è¯¢ï¼šæ ¹æ®æ•°æ®åº“çš„è¡¨å±æ€§ï¼Œä»æ•°åä¸‡çš„è¡¨ä¸­æŸ¥æ‰¾ä¸šåŠ¡å…³è”çš„è¡¨å†…å®¹ï¼Œé€šè¿‡ç²—æ’å’Œç²¾æ’å®ç°æŸ¥è¡¨å‡†ç¡®\nç‡å’Œå¬å›ç‡ã€‚\nâ€¢ æ™ºèƒ½å»ºè®®ï¼šåœ¨ç”¨æˆ·è¾“å…¥æŸ¥è¯¢æ—¶ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®å½“å‰æ•°æ®åº“ç»“æ„æä¾›ç›¸å…³è¡¨å’Œå­—æ®µçš„æ™ºèƒ½å»ºè®®ï¼Œæé«˜æŸ¥è¯¢çš„å‡†\nç¡®æ€§å’Œæ•ˆç‡ã€‚\nè½åœ°æ•°æ®å¹³å°Agentåœºæ™¯ - SQL ç”Ÿæˆå’Œè‡ªåŠ¨æŸ¥è¡¨\nDify å®è·µæ€»ç»“\nPART 05\nDify å®è·µæ€»ç»“\nâ€¢ Dify åœ¨é¡ºä¸°å†…éƒ¨æœ‰æµ·é‡çš„åº”ç”¨åœºæ™¯ï¼ˆæ•°ç™¾ä¸ªåœ¨çº¿åº”ç”¨ï¼Œå‡ åä¸‡å¯¹è¯æ•°ï¼‰\nå¼€å‘æµç¨‹\næœªä½¿â½¤ Dify å¹³å°\nä½¿â½¤ Dify å¹³å°\næ€§èƒ½æå‡\nå¼€å‘åº”â½¤å‰&åç«¯\né›†æˆå’Œå°è£… LLM èƒ½â¼’ï¼ŒèŠ±è´¹è¾ƒå¤šæ—¶é—´å¼€å‘å‰ç«¯åº”â½¤\nç›´æ¥ä½¿â½¤ Dify çš„åç«¯æœåŠ¡ï¼Œå¯åŸºäº WebApp è„šâ¼¿æ¶å¼€å‘\n80%\nPrompt Engineering\nä»…èƒ½é€šè¿‡è°ƒâ½¤ API æˆ– Playground è¿›â¾\nç»“åˆâ½¤æˆ·è¾“â¼Šæ•°æ®æ‰€â»…å³æ‰€å¾—å®Œæˆè°ƒè¯•\n25%\næ•°æ®å‡†å¤‡ä¸åµŒâ¼Š\nç¼–å†™ä»£ç å®ç°â»“â½‚æœ¬æ•°æ®å¤„ç†ã€åµŒâ¼Š\nåœ¨å¹³å°ä¸Šä¼ â½‚æœ¬æˆ–ç»‘å®šæ•°æ®æºå³å¯\n80%\nåº”â½¤â½‡å¿—ä¸åˆ†æ\nç¼–å†™ä»£ç è®°å½•â½‡å¿—ï¼Œè®¿é—®æ•°æ®åº“æŸ¥çœ‹\nå¹³å°æä¾›å®æ—¶â½‡å¿—ä¸åˆ†æ\n70%\næ•°æ®åˆ†æä¸å¾®è°ƒ\næŠ€æœ¯â¼ˆå‘˜è¿›â¾æ•°æ®ç®¡ç†å’Œåˆ›å»ºå¾®è°ƒé˜Ÿåˆ—\nâ¾®æŠ€æœ¯â¼ˆå‘˜å¯ååŒï¼Œå¯è§†åŒ–æ¨¡å‹è°ƒæ•´\n60%\nAI æ’ä»¶å¼€å‘ä¸é›†æˆ\nç¼–å†™ä»£ç åˆ›å»ºã€é›†æˆ AI æ’ä»¶\nå¹³å°æä¾›å¯è§†åŒ–â¼¯å…·åˆ›å»ºã€é›†æˆæ’ä»¶èƒ½â¼’\n50%\nDify å®è·µæ€»ç»“\nâ€¢ æœªæ¥å°†é›†æˆé¡ºä¸°ç»Ÿä¸€çš„æ¨¡å‹å¹¿åœº\nâ€¢ é€šè¿‡æ’ä»¶å¢åŠ é¡ºä¸°å†…éƒ¨æ‰€æœ‰ API å¸‚åœº\nâ€¢ æ‰“ç ´å…¬æœ‰äº‘å’Œç§æœ‰äº‘é™åˆ¶ï¼Œå……åˆ†åˆ©ç”¨ vGPU æ± åŒ–æŠ€æœ¯\nâ€¢ å¤š Agent è°ƒåº¦æ”¯æŒï¼Œå°¤å…¶æ˜¯ OpenAI Swarm æ·±åº¦é›†æˆ\nâ€¢ â€¦â€¦\nåˆ©ç”¨AIæŠ€æœ¯æ·±åŒ–è®¡ç®—æœºå¯¹ç°å®ä¸–ç•Œçš„ç†è§£\næ¨åŠ¨ç ”å‘è¿›å…¥æ™ºèƒ½åŒ–æ—¶ä»£\n",
    "file_path": "./emails\\CAS-11358-S054FG.pdf",
    "classification": {
      "level1": "è½¯ä»¶é—®é¢˜",
      "level2": "å¹³å°æ”¹é€ ",
      "level3": "æ•°æ®åº“é›†æˆå’Œæ£€ç´¢å¢å¼º"
    },
    "category_score": {
      "level1": 0.0,
      "level2": 0.0,
      "level3": 0.0
    },
    "reasoning": "è¯¥æ¡ˆä¾‹æ¶‰åŠå¯¹Difyå¼€å‘å¹³å°çš„æ”¹é€ ï¼ŒåŒ…æ‹¬æ•°æ®åº“é›†æˆã€æ£€ç´¢å¢å¼ºã€å†…éƒ¨æœåŠ¡é›†æˆç­‰å¤šä¸ªæ–¹é¢ï¼Œå±äºè½¯ä»¶é—®é¢˜ä¸­çš„å¹³å°æ”¹é€ èŒƒç•´ã€‚å…·ä½“çš„æŠ€æœ¯é—®é¢˜åŒ…æ‹¬æ•°æ®åº“é›†æˆï¼ˆå¦‚æ–°å¢MySQLé…ç½®ï¼‰ã€æ£€ç´¢å¢å¼ºï¼ˆå¦‚æ–°å¢ElasticSearchæ”¯æŒï¼‰ç­‰ï¼Œå› æ­¤å½’ç±»ä¸ºæ•°æ®åº“é›†æˆå’Œæ£€ç´¢å¢å¼ºã€‚"
  },
  {
    "content": "1.OZON è´¦æˆ·æ³¨å†Œ\næ‰“å¼€æµè§ˆå™¨ï¼Œåœ¨æµè§ˆå™¨è¾“å…¥https://seller.ozon.ru/ch\n2.è½¬åˆ°ç™»å½•é¡µé¢ï¼Œå¹¶å•å‡»æ³¨å†Œã€‚\n3.è¾“å…¥ç”µè¯å·ç \n4.è¾“å…¥é‚®ç®±ï¼Œç„¶åå•å‡»è·å–éªŒè¯ç ã€‚å°†æ”¶åˆ°å‘é€çš„éªŒè¯ç ã€‚å¤åˆ¶éªŒè¯ç å¹¶ç²˜è´´åˆ°ç›¸åº”çš„å­—\næ®µã€‚\n5.æ³¨å†Œå›½å®¶é€‰æ‹©ä¸­å›½\n6. å¡«å†™å®Œæ•´çš„ä¿¡æ¯( å¯ä»¥å»ä¼æŸ¥æŸ¥å¤åˆ¶è¥ä¸šæ‰§ç…§ä¿¡æ¯ï¼‰\n7.ä¸Šä¼ è¥ä¸šæ‰§ç…§æ‰«æä»¶\n8.ç»‘å®šå®åé€šè¿‡çš„è¿è¿\nèµ„æ–™æ²¡æœ‰é—®é¢˜è”ç³»è¿è¿ç»ç†åé¦ˆåŠ æ€¥å½“å¤©ä¸‹åº—\n",
    "file_path": "./emails\\CAS-12551-Y9S4X3.pdf",
    "classification": {
      "level1": "è½¯ä»¶é—®é¢˜",
      "level2": "è´¦æˆ·æ³¨å†Œé—®é¢˜",
      "level3": "OZONå¹³å°è´¦æˆ·æ³¨å†Œ"
    },
    "category_score": {
      "level1": 0.95,
      "level2": 0.0,
      "level3": 0.0
    },
    "reasoning": "è¯¥æ¡ˆä¾‹æè¿°äº†åœ¨OZONå¹³å°ä¸Šè¿›è¡Œè´¦æˆ·æ³¨å†Œçš„å…·ä½“æ­¥éª¤ï¼Œæ¶‰åŠåˆ°æµè§ˆå™¨æ“ä½œã€ç”µè¯å·ç å’Œé‚®ç®±çš„è¾“å…¥ã€éªŒè¯ç çš„è·å–å’Œè¾“å…¥ã€å›½å®¶é€‰æ‹©ã€ä¿¡æ¯å¡«å†™å’Œè¥ä¸šæ‰§ç…§ä¸Šä¼ ç­‰å†…å®¹ã€‚è¿™äº›æ“ä½œéƒ½å±äºè½¯ä»¶ä½¿ç”¨è¿‡ç¨‹ä¸­çš„é—®é¢˜ï¼Œå› æ­¤å½’ç±»ä¸ºè½¯ä»¶é—®é¢˜ã€‚ç°æœ‰åˆ†ç±»ç»“æ„ä¸­æ²¡æœ‰é€‚åˆçš„äºŒçº§å’Œä¸‰çº§åˆ†ç±»ï¼Œå› æ­¤åˆ›å»ºæ–°çš„äºŒçº§åˆ†ç±»'è´¦æˆ·æ³¨å†Œé—®é¢˜'å’Œä¸‰çº§åˆ†ç±»'OZONå¹³å°è´¦æˆ·æ³¨å†Œ'ã€‚"
  },
  {
    "content": "Building Knowledge \nGraph RAG with \nLlamaIndex\nä½¿ç”¨ LlamaIndex æ„å»º\nçŸ¥è¯†å›¾è°± RAG\nBasic Steps for Building the \nKnowledge Graph\nåœ¨ LlamaIndex ä¸­æ„å»ºçŸ¥è¯†å›¾è°± RAG \nçš„åŸºæœ¬æ­¥éª¤\nImport Packages(å¯¼å…¥è½¯ä»¶åŒ…)\nLoad documents(åŠ è½½æ–‡ä»¶)\nConfigure service_context and \nstorage_context(é…ç½®æœåŠ¡ä¸Šä¸‹æ–‡å’Œå­˜å‚¨ä¸Šä¸‹æ–‡)\nservice_context\nstorage_context\nSimpleGraphStore\nNeo4jGraphStore\nNebulaGraphStore\næœ¬æœŸç„¦ç‚¹\nCreate Index(åˆ›å»ºç´¢å¼•)\nCreate query_engine(åˆ›å»ºæŸ¥è¯¢å¼•æ“)\nQuery with query_engine(ä½¿ç”¨æŸ¥è¯¢å¼•æ“è¿›è¡ŒæŸ¥\nè¯¢)\nExtending the Knowledge Graph \nRAG based on graph-store\nåŸºäºgraph-storeæ‰©å±•çŸ¥è¯†å›¾è°±RAG\nSimpleGraphStore\ngraph_store = SimpleGraphStore() \nCode + Doc\nî¤€\nNeo4jGraphStore\nCreate Neo4jGraph Database\nî¤€\nReference Notebook\nî¤€\nDoc\nî¤€\nNebulaGraphStore\nCreate NebulaGraph Database\nî¤€\nReference Notebook\nî¤€\nDoc\nî¤€\nä»£ç å®ç°\nOfficial Githubå®˜æ–¹\nî¤€\nå¯ä¹i_Klayç‰ˆæœ¬\nî¤€\n",
    "file_path": "./emails\\CAS-12579-S0T2G6.pdf",
    "classification": {
      "level1": "è½¯ä»¶é—®é¢˜",
      "level2": "å¹³å°æ”¹é€ ",
      "level3": "çŸ¥è¯†å›¾è°±æ„å»ºä¸æŸ¥è¯¢"
    },
    "category_score": {
      "level1": 1.0,
      "level2": 0.9,
      "level3": 0.0
    },
    "reasoning": "é‚®ä»¶å†…å®¹ä¸»è¦æ¶‰åŠä½¿ç”¨ LlamaIndex æ„å»ºçŸ¥è¯†å›¾è°± RAG çš„æ­¥éª¤å’Œç›¸å…³é…ç½®ï¼Œå±äºè½¯ä»¶é—®é¢˜ä¸­çš„å¹³å°æ”¹é€ ç±»é—®é¢˜ã€‚ç°æœ‰åˆ†ç±»ä¸­æ²¡æœ‰å…·ä½“çš„â€˜çŸ¥è¯†å›¾è°±æ„å»ºä¸æŸ¥è¯¢â€™åˆ†ç±»ï¼Œå› æ­¤éœ€è¦åœ¨ä¸‰çº§åˆ†ç±»ä¸­æ–°å¢è¯¥åˆ†ç±»ã€‚"
  },
  {
    "content": "88-NETLOGON\n1.\nNETLOGON DNS NETLOGONNETLOGON\n2.\nNETLOGON\nPass-Through\nNETLOGON\nPass-Through\n,NETLOGONDC\nDC\nNETLOGON\nNETLOGON\nNETLOGONDC\nNETLOGONNETLOGONADADNETLOGONWindows 10 30NETLOGONNETLOGON3030NETLOGONAD\nCMGE V2020-LNETLOGON\nCMGE V2020-LCMGE V2020-L\nNetlogon\nCMGE V2020-L\nCMGE V2020-L30NETLOGON\nNETLOGON\nNetlogon\n â€œNetlogon Remote Protocolâ€ â€œThe Netlogon Remote Protocol is a remote procedure call (RPC) interface that is used for user and \nmachine authentication on domain-based networks.â€ Netlogon/\n3.\nNetlogon Remote Protocol: https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-nrpc/ff8f970f-3e37-40f7-bd4b-\naf7336e4792f\nMachine Account Password Process: \n \nhttps://docs.microsoft.com/zh-cn/archive/blogs/askds/machine-account-password-process-2\n",
    "file_path": "./emails\\CAS-12653-SSDFB6.pdf",
    "classification": {
      "level1": "è½¯ä»¶é—®é¢˜",
      "level2": "ç½‘ç»œé—®é¢˜",
      "level3": "NetlogonæœåŠ¡é—®é¢˜"
    },
    "category_score": {
      "level1": 1.0,
      "level2": 0.0,
      "level3": 0.0
    },
    "reasoning": "é‚®ä»¶å†…å®¹ä¸­åå¤æåˆ°NetlogonæœåŠ¡ç›¸å…³é—®é¢˜ï¼Œå±äºè½¯ä»¶é—®é¢˜ä¸­çš„ç½‘ç»œé—®é¢˜èŒƒç•´ã€‚ç°æœ‰åˆ†ç±»ä¸­æ²¡æœ‰åŒ¹é…çš„äºŒçº§å’Œä¸‰çº§åˆ†ç±»ï¼Œå› æ­¤åˆ›å»ºæ–°çš„äºŒçº§å’Œä¸‰çº§åˆ†ç±»ã€‚"
  },
  {
    "content": "2024-9-20\nTraining Language Models to Self-Correct via\nReinforcement Learning\nAviral Kumar*+,1, Vincent Zhuang*+,1, Rishabh Agarwal*,1, Yi Su*,1, JD Co-Reyes1, Avi Singh1, Kate Baumli1,\nShariq Iqbal1, Colton Bishop1, Rebecca Roelofs1, Lei M Zhang1, Kay McKinney1, Disha Shrivastava1, Cosmin\nPaduraru1, George Tucker1, Doina Precup1, Feryal Behbahaniâ€ ,1 and Aleksandra Faustâ€ ,1\n1Google DeepMind, *Equal Contribution, +Randomly ordered via coin flip, â€ Jointly supervised.\nSelf-correction is a highly desirable capability of large language models (LLMs), yet it has consistently\nbeen found to be largely ineffective in modern LLMs. Existing approaches for training self-correction either\nrequire multiple models or rely on a more capable model or other forms of supervision. To this end, we\ndevelop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an\nLLMâ€™s self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants\nof supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling\nself-correction behavior. In particular, we observe that training via SFT either suffers from a distribution\nmismatch between the training data and the modelâ€™s own responses or implicitly prefers only a certain mode\nof correction behavior that is often not effective at test time. SCoRe addresses these challenges by training\nunder the modelâ€™s own distribution of self-generated correction traces and using appropriate regularization\nto steer the learning process into learning a self-correction strategy that is effective at test time as opposed\nto simply fitting high-reward responses for a given prompt. This regularization prescribes running a first\nphase of RL on a base model to generate a policy initialization that is less susceptible to collapse and then\nusing a reward bonus to amplify self-correction during training. When applied to Gemini 1.0 Pro and 1.5\nFlash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base\nmodelsâ€™ self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks.\n1. Introduction\nLarge language models (LLMs) have proven to be a useful tool in reasoning and scientific domains such\nas mathematical problem-solving and coding (Lozhkov et al., 2024; Shao et al., 2024; Team, 2024). An\naspirational property of LLMs in such settings is to able to implement algorithms: strategies that help\nthe LLM to use computation and interaction to improve its response on the test-time query. Modern\nLLMs largely do not implement algorithms reliably: for instance, consider a problem setting that requires\nmodels to detect and revise (or â€œself-correctâ€) their own responses to a given test-time query, so as to\nbe able to eventually arrive at the best-possible final response. This sort of self-correction capability has\nbeen shown by several recent works to be severely lacking in current LLMs, especially in the absence of\nexternal input (also referred to as intrinsic self-correction) (Huang et al., 2023; Kamoi et al., 2024).\nTo make progress towards the eventual goal of teaching LLMs to implement algorithms to handle\nchallenging inputs, we study a special instance of training LLMs to implement self-correction strategies\nto fix their mistakes â€œon-the-flyâ€. This should be possible: on many queries where current LLMs fail,\nthey still contain the underlying â€œknowledgeâ€ needed to arrive at the correct response but are unable\nto correctly elicit and draw inferences about their own knowledge when needed (Snell et al., 2024).\nFor example, strong LLMs can often successfully complete a sub-part of a math proof when prompted\nwith the remainder, but may not be able to complete it from scratch. In a similar vein, leveraging their\nprevious responses should, in principle, enable LLMs to improve their subsequent ones. Nevertheless,\nCorresponding author(s): [vincentzhuang, aviralkumar, rishabhagarwal, yisumtv]@google.com\narXiv:2409.12917v1  [cs.LG]  19 Sep 2024\nTraining Language Models to Self-Correct via Reinforcement Learning\nBase Model\nSTaR\nSFT\nSCoRe\n40\n45\n50\n55\n60\n65\n70\nTest Accuracy (%)\n-11.2%\n+0.4%\n+1.8%\n+4.4%\nGemini 1.5 Flash: MATH\nDirect Generation\nSelf-Correction (SC)\n (SC - Direct)\n21\n22\n23\n24\n25\nNumber of samples (K)\n60%\n62%\n64%\n66%\n68%\n70%\nSelf-Consistency@K\nScaling Inference Compute: MATH\nParallel Samples\nSequential (Self-Correct)\nFigure 1 âˆ£Left: SCoRe achieves state-of-the-art self-correction performance on MATH; Right: SCoRe inference-time scaling:\nspending samples on sequential self-correction becomes more effective than only on parallel direct samples (Section 6.2).\nself-correction has remained elusive, highlighting the need for going beyond existing training paradigms.\nHow can we instill LLMs with self-correction abilities? Prior attempts toward self-correcting LLMs either\nrely on prompt-engineering (Kim et al., 2023; Madaan et al., 2023) or fine-tuning models specifically for\nself-correction (Havrilla et al., 2024b; Qu et al., 2024; Welleck et al., 2023; Yuan et al., 2024). While the\nformer class of approaches often fail to effectively perform meaningful intrinsic self-correction, existing\nfine-tuning based approaches require running multiple models upon inference, e.g., a separate verifier or\nrefinement model (Havrilla et al., 2024b; Welleck et al., 2023), or require oracle â€œteacherâ€ supervision to\nguide the process of self-correction (Qu et al., 2024), without which self-correction does not necessarily\noutperform independent uncorrelated attempts at the problem. We develop an approach that is effective\nat self-correction without any of the aforementioned requirements. Our approach, Self-Correction via\nReinforcement Learning (SCoRe), trains only a single model that can both produce a response to a\nreasoning problem and also correct errors despite not receiving any oracle feedback. More importantly,\nSCoRe teaches this ability to models entirely by training on self-generated data, without any oracle.\nWe begin by studying the failure modes of existing fine-tuning based strategies in this setting. We observe\nthat running supervised fine-tuning on multi-turn self-correction traces coupled with rejection sampling\n(i.e., a â€œmulti-turnâ€ variant of STaR (Zelikman et al., 2022)) often amplifies the modelâ€™s bias to not\nmake any error corrections. A minimal edit strategy appears somewhat optimal as it inhibits the model\nfrom learning to make correct responses worse in the second attempt, even though it does not instill\nself-correction abilities to the model. If the training dataset for SFT is altered to explicitly down-weight\ncertain correction traces that only make minor edits, then the resulting training is able to avoid collapse.\nHowever, it suffers from the curse of distributional shift: a correction strategy learned by training on\noff-policy data does not necessarily enable the model to be succeed at correcting its own mistakes.\nHow does SCoRe work? SCoRe addresses the aforementioned challenges with SFT by utilizing online\nmulti-turn reinforcement learning (RL). Concretely, SCoRe runs multi-turn RL on self-generated data to\navoid challenges with distribution mismatch between training and inference. To avoid the failure mode\nof learning a minimal edit strategy when training on on-policy data, we train SCoRe in two stages, with\neach stage regularizing the learning process to not collapse its behavior. The first stage replaces SFT\nin conventional LLM fine-tuning workflows by training a model initialization that optimizes correction\n2\nTraining Language Models to Self-Correct via Reinforcement Learning\nperformance while constraining the first attempt to be close to the base model. The second stage runs\nmulti-turn RL to optimize reward at both attempts, while using a reward bonus term that encourages\nimproving responses from the first attempt to the second. Both the initialization and the reward bonus\nensure that the model cannot simply learn to produce the best first-attempt response and only minorly\nedit it. Overall, SCoRe is able to elicit knowledge from the base model to enable positive self-correction.\nOur main contribution is SCoRe, a multi-turn RL approach for teaching LLMs how to correct their own\nmistakes. To the best of our knowledge, SCoRe is the first approach to attain significantly positive\nintrinsic self-correction: relative to base Gemini models, our method attains an absolute 15.6% gain\non self-correction for reasoning problems from MATH (Hendrycks et al., 2021) and an absolute 9.1%\ngain on coding problems from HumanEval (Chen et al., 2021). We additionally motivate the design of\nSCoRe by extensively studying the failure modes of baseline approaches, which broadly indicate that\nreinforcement learning may play an essential role in self-learned self-correction.\n2. Related Work\nPrior works study self-correction for LLMs under a variety of assumptions and problem settings. The\nmost prominent problem settings include problems where external input tokens from an environment is\navailable, for e.g., agentic tasks (Liu et al., 2023), code repair (Jain et al., 2024), and tool use (Chen\net al., 2023). While self-correction with external feedback is possible with strong proprietary models (Pan\net al., 2023), even the strongest models struggle in the substantially more challenging setting when no\nexternal input is available (Kamoi et al., 2024). This setting is called intrinsic self-correction. Prior work\nthat attempts to amplify intrinsic correction abilities are largely based on prompting and fine-tuning.\nPrompting for intrinsic self-correction. Recent work demonstrates that LLMs struggle to self-correct\ntheir reasoning errors without external feedback and naÃ¯vely running self-correction can degrade perfor-\nmance (Huang et al., 2023; Qu et al., 2024; Tyen et al., 2024; Zheng et al., 2024). These experimental\nstudies are at odds with prior work (Kim et al., 2023; Madaan et al., 2023; Shinn et al., 2023) and largely\nstem from mismatched assumptions on the setting (Kamoi et al., 2024). For example, Kim et al. (2023);\nShinn et al. (2023) use oracle ground-truth answers during self-correction that may not be available\ngenerally. Madaan et al. (2023) use weak prompts for initial responses, thereby perhaps overestimate the\nimprovement possible by self-correction. This indicates that there is no major work showing successful\nintrinsic self-correction via prompting alone. In the context of code self-repair, Olausson et al. (2023)\nshow that even when strong models are prompted with some form of partial feedback, e.g., showing\ntest-cases but not the desired outcomes on those test-cases, they are often unable to correct their mistakes.\nSampling multiple responses in parallel attains much better results in Olausson et al. (2023).\nFine-tuning for intrinsic self-correction. To address the issues with prompting off-the-shelf models\nalone, several works run supervised fine-tuning (SFT) or weighted SFT on the LLM to generate a revision\ngiven an initial response. Nonetheless typical works in this literature rely on oracle feedback: e.g.,\nobtaining revisions directly from human annotators (Saunders et al., 2022) or stronger models (Qu\net al., 2024; Ye et al., 2023). Our work aims to train for self-correction entirely without the use of bigger\nmodels or humans, when the learner itself is asked to generate its own training data. Similar to these\nprior works, we assume access to a reward function for evaluating model-generated outputs, similar\nto AkyÃ¼rek et al. (2023); Welleck et al. (2023); Zhang et al. (2024). Other approaches build pipelines\nwith multiple models for self-correction (e.g., GLoRE (Havrilla et al., 2024b), Self-correct (Welleck et al.,\n2023), or AkyÃ¼rek et al. (2023); Paul et al. (2023)). While this can lead to good results, these pipelines\ndo not quite tackle self-correction and require system design for serving multiple models at deployment.\n3\nTraining Language Models to Self-Correct via Reinforcement Learning\nProblem. If ğ‘›â‰¡2 (mod 7), then find the remain-\nder when (ğ‘›+ 2)(ğ‘›+ 4)(ğ‘›+ 6) is divided by 7.\nSCoRe turn 1 solution (incorrect). Since ğ‘›â‰¡2\n(mod 7), we can write ğ‘›= 7ğ‘˜+2 for some integer\nğ‘˜. Substituting, we get\n(ğ‘›+ 2)(ğ‘›+ 4)(ğ‘›+ 6) = (7ğ‘˜+ 4)(7ğ‘˜+ 6)(7ğ‘˜+ 8)\nâ‰¡4 â‹…6 â‹…8\n(mod 7)\nâ‰¡192\n(mod 7)\nâ‰¡1\n(mod 7).\nFinal Answer: The final answer is 1. I hope it is\ncorrect.\nSelf-correction instruction. There might be an\nerror in the solution above because of lack of un-\nderstanding of the question. Please correct the\nerror, if any, and rewrite the solution.\nSCoRe turn 2 solution (correct). Since ğ‘›â‰¡2\n(mod 7), we can write ğ‘›= 7ğ‘˜+2 for some integer\nğ‘˜. Substituting, we get\n(ğ‘›+ 2)(ğ‘›+ 4)(ğ‘›+ 6) = (7ğ‘˜+ 4)(7ğ‘˜+ 6)(7ğ‘˜+ 8)\nâ‰¡4 â‹…6 â‹…8\n(mod 7)\nâ‰¡4 â‹…6 â‹…1\n(mod 7)\nâ‰¡24\n(mod 7)\nâ‰¡3\n(mod 7).\nFinal Answer: The final answer is 3. I hope it is\ncorrect.\nFoundation model\nFoundation model trained with ScoRe\nprompt (x)\nresponse (y)\nfirst attempt (y1)\nImproved \nresponse (y2)\nprompt (x)\nOptional external \ninput (p) \nStandard training\nTraining via SCoRe (Ours)\nLoss\nLoss\nThe problem setting of self-correction. SCoRe trains a\nmodel to not just produce the best possible response, but\ninstead aims to train the model to produce the best final\nresponse in the final attempt. In the second turn, extra\ninput in the form of an instruction asking the model to\ncorrect itself or model-generated may be provided.\nFigure 2 âˆ£An example trace and the problem setting of self-correction.\nMulti-turn RL for LLMs. Our approach utilizes a multi-turn policy gradient approach for training for\nself-correction, which extends the single-turn approach of Ahmadian et al. (2024) and can be viewed\nas an instantiation of the hierarchical RL framework from Zhou et al. (2024). Generally, prior work at\nthe intersection of LLMs and multi-turn RL builds value-based (Farebrother et al., 2024; Shani et al.,\n2024; Snell et al., 2022; Zhou et al., 2024), policy-based (Shao et al., 2024; Xiong et al., 2024), and\nmodel-based (Hong et al., 2024) approaches. While this line of work builds machinery to do RL (i.e.,\noptimize rewards) in a multi-turn Markov decision process (MDP), our primary contribution in this paper\nis to devise a formalization, for learning self-correction behavior instead of the RL machinery itself.\nSelf-correction with external feedback. Many works study self-correction with additional feedback from\nthe environment, most commonly in the setting of code generation, where unit test results or compiler\nexecution feedback are available (Chen et al., 2024; Jain et al., 2024; Olausson et al., 2023). Largely\nthese works prompt models to reason about code execution; Ni et al. (2024) propose a self-training\nmethod that leverages execution traces, though only evaluate it on correcting a fixed dataset of errors.\n4\nTraining Language Models to Self-Correct via Reinforcement Learning\n3. Preliminaries and Problem Setup\nOur goal is to develop an approach for training LLMs to improve their own predictions by entirely\ntraining on self-generated data. As discussed so far, we situate ourselves in the intrinsic self-correction\nsetting (Huang et al., 2023), where models attempt to correct their initial responses without any external\nfeedback. Concretely, given a dataset ğ’Ÿ= {(ğ’™ğ‘–, ğ’šâˆ—\nğ‘–)}ğ‘\nğ‘–=1 of problems ğ’™ğ‘–and oracle responses ğ’šâˆ—\nğ‘–, we will\ntrain an LLM policy ğœ‹ğœƒ(â‹…âˆ£[ğ’™, Ë†ğ’š1âˆ¶ğ‘™, ğ‘1âˆ¶ğ‘™]) that, given the problem ğ’™, previous ğ‘™model attempts Ë†ğ’š1âˆ¶ğ‘™at the\nproblem, and auxiliary instructions ğ‘1âˆ¶ğ‘™(e.g., instruction to find a mistake and improve the response),\nsolves the problem ğ’™as correctly as possible. This formalism is akin to the multi-turn MDP in Qu et al.\n(2024). Moreover, we assume access to a reward function / verifier Ì‚ğ‘Ÿ(ğ’š, ğ’šâˆ—), such as a string-matching\nbased answer checking function) that evaluates correctness of response ğ’šby comparing with the oracle\nresponse ğ’šâˆ—. Critically, we do not assume access to such a function at test-time and the model itself learns\nto deduce whether there was a mistake and corrects it, as is often the case in e.g. mathematical reasoning\nproblems. An example and overview of our problem setting is given in Figure 2.\nWe aim to find a model ğœ‹(â–¡âˆ£â—¦) (which we will also refer to as a policy) mapping a sequence of input\ntokens â—¦to a sequence of output tokens â–¡that maximizes the correctness reward obtained from the\nverifier at the end of ğ‘™+ 1 turns. Formally, this can be written as the following multi-step RL objective:\nmax\nğœ‹ğœƒ\nâ„°ğ’™,ğ’šâˆ—âˆ¼ğ’Ÿ,Ë†ğ’šğ‘™+1âˆ¼ğœ‹ğœƒ(â‹…âˆ£[ğ’™,Ë†ğ’š0âˆ¶ğ‘™,ğ‘1âˆ¶ğ‘™]) [Ì‚ğ‘Ÿ(Ë†ğ’šğ‘™+1, ğ’šâˆ—)] .\n(1)\nCrucially, note that unlike standard SFT or prevalent RL fine-tuning workflows that train the policy ğœ‹to\ndirectly produce an optimal response Ë†ğ’šfor an input ğ’™, Equation 1 trains ğœ‹over multiple turns / attempts\nsimultaneously, where intermediate turn responses Ë†ğ’š1âˆ¶ğ‘™are supervised indirectly with the final rewards.\nA base RL approach for fine-tuning LLMs. Our RL toolkit is based on on-policy policy gradient. These\nmethods, such as REINFORCE with a KL-divergence penalty against a fixed model (Ahmadian et al.,\n2024), are widely used in RL fine-tuning of LLMs, primarily in setting of single-turn RL from human\nfeedback. Formally, such policy gradient approaches train a policy ğœ‹ğœƒ(â‹…âˆ£ğ’™) to optimize:\nmax\nğœƒ\nEğ’™ğ‘¡,ğ’šğ‘¡âˆ¼ğœ‹ğœƒ(â‹…âˆ£ğ’™ğ‘¡) [Ì‚ğ‘Ÿ(ğ’šğ‘¡, ğ’šâˆ—) âˆ’ğ›½1ğ·ğ¾ğ¿(ğœ‹ğœƒ(â‹…âˆ£ğ’™ğ‘¡)âˆ£âˆ£ğœ‹ref(â‹…âˆ£ğ’™ğ‘¡))] ,\n(2)\nwhere ğœ‹ref is a reference anchor policy, typically chosen to be a pre-trained or SFT policy.\nMetrics. For measuring self-correction performance, we report and analyze the following metrics: (1)\nAccuracy@t1: the modelâ€™s accuracy at the first attempt; (2) Accuracy@t2: the modelâ€™s accuracy at the\nsecond attempt, (3) Î”(t1, t2): the net improvement in model accuracy between the first and second\nattempts, which measures the efficacy of self-correction, (4) Î”iâ†’c(t1, t2): the fraction of problems that\nare incorrect in the first attempt but become correct at the second attempt, which measures how many\nnew problems can self-correction solve; and (5) Î”câ†’i(t1, t2): the fraction of problems that are correct in\nthe first attempt but become incorrect at the second attempt, which measures how well the model is able\nto understand what makes a response correct.\n4. Supervised Fine-Tuning on Self-Generated Data is Insufficient for Self-Correction\nPerhaps a natural approach to train for self-correction is to utilize some form of supervised fine-tuning\non data collected from a base model. Variants of this recipe have been shown to scale well in single-\nturn reasoning problems (Havrilla et al., 2024a; Singh et al., 2023; Zelikman et al., 2022). Can such\nSFT-based approaches be effective for self-correction as well?\n5\nTraining Language Models to Self-Correct via Reinforcement Learning\nTable 1 âˆ£Self-correction performance after training on ğ’ŸSTaR and ğ’ŸSFT. For both approaches, we find that the gap between\nsecond-attempt and first-attempt performance (Î”(ğ‘¡1, ğ‘¡2)) is either overly negative or very small. In addition, both approaches\nerroneously modify a correct response to be incorrect, i.e., reflected in a high Î”câ†’i(ğ‘¡1, ğ‘¡2) and a low Î”iâ†’c(ğ‘¡1, ğ‘¡2).\nMethod\nAccuracy@t1\nAccuracy@t2\nÎ”(t1, t2)\nÎ”iâ†’c(t1, t2)\nÎ”câ†’i(t1, t2)\nBase model\n52.6%\n41.4%\n-11.2%\n4.6%\n15.8%\nSTaR ğ’ŸStaR\n55.4%\n41.2%\n-14.2%\n5.4%\n19.6%\nPair-SFT ğ’ŸSFT\n52.4%\n54.2%\n1.8%\n5.4%\n3.6%\nTable 2 âˆ£Self-correction performance after training on ğ’Ÿ+\nSTaR and ğ’Ÿ+\nSFT. Performance improves for STaR indicating that a\nhigher coverage dataset helps improve performance, but not for SFT where traning on traces where both responses are correct\nforces the model to simply not make any changes to its first-attempt response, no matter how correct or incorrect that is.\nMethod\nAccuracy@t1\nAccuracy@t2\nÎ”(t1, t2)\nÎ”iâ†’c(t1, t2)\nÎ”câ†’i(t1, t2)\nBase model\n52.6%\n41.4%\n-11.2%\n4.6%\n15.8%\nSTaR ğ’Ÿ+\nStaR\n53.6%\n54.0%\n0.4%\n2.6%\n2.2%\nPair-SFT ğ’Ÿ+\nSFT\n55.0%\n55.0%\n0%\n0%\n0%\nIn this section, we perform an empirical study to answer this question. We study two approaches:\nSTaR (Zelikman et al., 2022) and an approach akin to Welleck et al. (2023) that trains only one model.\nWe do not use learned process or outcome verifiers to guide correction traces, so our setup differs from\nSFT in Snell et al. (2024). We find that such methods improve substantially compared to the base modelâ€™s\nself-correction behavior, but still fail to attain a positive self-correction rate and produce a worse second\nattempt compared to their first attempt. By probing trained models, we find that these failures largely\nstem from supervised fine-tuning amplifying the initial bias of the base model resulting in only minor\nchanges to its first-attempt response. While these failures can be addressed if a different distribution over\ninitial responses is used for training, doing so fails to induce effective self-correction behavior under the\nmodelâ€™s own response distribution. Either way, learning is affected by distribution shift or amplification\nof the base modelâ€™s bias. These observations motivate the design of our method in Section 5.\n4.1. Analysis Setup: Methods and Dataset Construction\nMethods. We prompt off-the-shelf models to obtain a large number of two-turn self-correction traces.\nThe STaR approach, analogous to ReSTğ¸ğ‘€(Singh et al., 2023), filters these trajectories to only retain\nthose that successfully revise incorrect responses and runs SFT on the resulting dataset. In contrast,\nWelleck et al. (2023) use the base model data from above to construct sets of correct and incorrect\nresponses and then generates â€œsyntheticâ€ repair traces by pairing incorrect responses with correct ones.\nWe study a variant of their method we call Pair-SFT, which does not train a separate corrector model\nand does not augment this initial dataset with multi-turn traces.\nDataset construction. We perform our study on the MATH dataset, and generate self-correction traces\nby prompting the Gemini 1.5 Flash (Reid et al., 2024) using temperature 1.0. We construct datasets for\nSTaR and Pair-SFT as follows: (1) ğ’ŸSTaR âˆ¶= {(ğ’™ğ‘–, Ë†ğ’šâˆ’\nğ‘–, Ë†ğ’š+\nğ‘–)}ğ‘\nğ‘–=1, where Ë†ğ’šâˆ’\nğ‘–and Ë†ğ’š+\nğ‘–correspond to incorrect\n6\nTraining Language Models to Self-Correct via Reinforcement Learning\n(a) Histograms of edit distance ratios on MATH 500.\n(b) STaR edit distance ratios.\n(c) Pair-SFT edit distance ratios.\nFigure 3 âˆ£Edit distance between first-attempt and second-attempt responses obtained from fine-tuned models, our\napproach (SCoRe) and the base model. Observe that while training on self-generated error correction traces inherits the\nbi-modal distribution of edits as the base model, SFT tends to be quite conservative.\nand correct responses appearing within a single sequence of attempts from the current model, and (2)\nğ’ŸSFT âˆ¶= {(ğ’™ğ‘–, Ë†ğ’šâˆ’\nğ‘–, Ëœğ’š+\nğ‘–)}ğ‘\nğ‘–=1, where Ëœğ’š+\nğ‘–is a random correct response for problem ğ’™, randomly sampled\nfrom the set of all first-turn and second-turn responses produced by the model. We then ran supervised\nfine-tuning on both of these datasets: following Singh et al. (2023), we repeat 3 iterations of collecting\nand running SFT on ğ’ŸSTaR, but only 1 epoch on ğ’ŸSFT given the large dataset size.\n4.2. Empirical Findings\nWe plot the self-correction performance of the Gemini 1.5 Flash before and after running fine-tuning\non ğ’ŸSTaR (3 iterations) and ğ’ŸSFT in Table 1. We find that although Î”(t1, t2) is substantially higher for\nPair-SFT relative to the base model, there is still little benefit to doing self-correction (1.8% gain). By\nconsidering Î”iâ†’c and Î”câ†’i, we find that SFT mainly helps by reducing the number of correct problems\nthat are mistakenly changed to incorrect after revision, and does not significantly increase the fraction of\nincorrect first attempts that are correctly repaired. This result is consistent with prior studies on intrinsic\nself-correction that have found negligible or even negative Î”(t1, t2) (Huang et al., 2023; Qu et al., 2024).\nWe also find that unlike Pair-SFT, training on ğ’ŸSTaR does not reduce Î”câ†’i, indicating that the STaR policy\ndoes not have a clear understanding of when and when not to make modifications. We hypothesize\nthat this discrepancy is due to the data distributions of ğ’Ÿğ‘†ğ¹ğ‘‡and ğ’Ÿğ‘†ğ‘‡ğ‘ğ‘…: the former covers a much more\ndiverse space of revision trajectories due to the nature of random pairing. Observing this, we also\ntrained on an extended version of ğ’Ÿ+\nSTaR (and also ğ’Ÿ+\nSFT), which additionally presents more tuples with\nboth correct responses. We would expect the addition of such â€œcorrect-to-correctâ€ data to prevent the\nmodel from erroneously revising a correct response and, at the very least, restrict the modification of a\ncorrect response into only another correct response. As shown in Table 2, perhaps interestingly, we find\nthat including such data has opposite effects on STaR and SFT: for STaR, inclusion of this data helps\nsubstantially, though it still results in barely any meaningful self-correction performance. On the other\nhand, for SFT, inclusion of this data overly biases the model to not change its answer at all.\n7\nTraining Language Models to Self-Correct via Reinforcement Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEpoch fraction\n0.02\n0.04\n0.06\n0.08\n0.10\nValidation \ni\nc(t1, t2)}\n0.1\n0.2\n0.3\n0.4\nTrain \ni\nc(t1, t2)}\nFixed validation\nSelf-generated validation\nFixed train\nFigure 4 âˆ£Tracking self-correction performance on a different sets of\nfirst-attempt responses: (a) â€œfixed validationâ€: first response is distributed\nidentically as the training set, (b) â€œself-generatedâ€: first response is\ngenerated by the learner itself. Observe that over the course of training,\nwhile accuracy of correcting a fixed validation set of responses largely\nstays constant (and perhaps even slightly improves) and the accuracy on\nthe training data improves substantially (note the different axis for the\ntraining set accuracy on the right), the modelâ€™s correction abilities its own\nfirst-attempt response degrade substantially. This indicates that training\non a fixed offline dataset may not be effective at inducing self-correction\nabilities due to distribution shift.\nDiving deeper: analyzing self-correction\nbehavior.\nWe also visualized how the\nSTaR and SFT models edit their responses.\nIn particular, we measured edit distance\nratio, defined as the edit distance be-\ntween the responses normalized by the\ntotal length of both the responses, to sum-\nmarize the extent to which models mod-\nify their first-attempt response. As shown\nin Figure 3a, while the base model some-\ntimes makes substantially large edits to\nthe original response, models fine-tuned\non ğ’ŸSTaR and ğ’ŸSFT are overly conserva-\ntive, and often make no edits at all. We\nwill show in Section 5 that our proposed\nmethod SCoRe is able to avoid amplifying\nthis bias of not making changes, without\nany explicit training for controlling how\nmuch to edit solutions.\nWe also plotted edit distance ratios in cor-\nrection traces appearing in the training data and compared it against the ratios in self-correction traces\ngenerated by STaR and Pair-SFT on training and validation problems in Figures 3b and 3c. While\nSTaR produces qualitatively similar edit distance ratios on both train and validation problems (meaning\nthat it performs within the training distribution very well), we still observe somewhat of a discrepancy\nbetween train and validation edit distance ratios for SFT. This means that Pair-SFT is not very effective at\ngeneralizing to new problems from the same distribution.\nSeeing the discrepancy in the edit distance ratios between train and validation problems for Pair-SFT,\nwe also analyzed the self-correction accuracy of the SFT model on a fixed set of first-attempt responses\nand self-generated first-attempt responses in Figure 4. We observe clearly different behaviors on both\ntraining vs. validation as well as static vs self-generated first-attempt distributions: while the model is able\nto optimize training correction accuracy well and also maintains its initial correction accuracy on first\nattempts appearing in the validation set (distributed i.i.d. to the training distribution), its self-correction\naccuracy degrades with more training.\nTakeaways: Insufficiency of SFT\nWe showed two distinct sources of failure of SFT methods: STaR latched onto only one mode\nof correction behavior that made minor changes, and training via Pair-SFT on data with wider\ncoverage resulted in a degradation in self-correction abilities on responses from the modelâ€™s\ndistribution of initial responses, due to distribution shift. This implies that an effective approach\nmust satisfy two desiderata: [D1] it should directly train on self-generated traces to alleviate\ndistribution mismatch that affected SFT (Figure 4), and [D2] self-generated traces employed\nshould prevent a collapse to making minor edits during learning. We will next develop an online\nRL approach that addresses these challenges with a careful initialization and reward shaping.\n8\nTraining Language Models to Self-Correct via Reinforcement Learning\nBroader Implications of these Results\nThese results more generally suggest that offline supervised fine-tuning is likely not effective\nat making use of additional in-context tokens to learn nuanced algorithmic behaviors, due to\nchallenges of distribution shift in training data and amplification of certain pathological behaviors\nthat seem promising on the training data but do not learn the right strategy.\n5. SCoRe: Self-Correction via Multi-Turn Reinforcement Learning\nTo develop an effective approach for teaching LLMs to self-correct by training entirely on self-generated\ndata, we have to satisfy the two desiderata discussed above. Utilizing on-policy RL in our method is\na natural way to satisfy desideratum [D1]. Our approach, SCoRe will extend standard single-turn RL\n(Equation 2) to the multi-turn setting under the hierarchical framework from Zhou et al. (2024).\nKey challenges. While multi-turn RL that optimizes Equation 1 addresses the issue with distribution\nshift, it is unclear whether it also satisfies desideratum [D2]. Base model initializations for fine-tuning\npresent a highly-skewed distribution over edit distances (Figure 3a), which makes them susceptible to\nmode collapse, a well-known issue in deep RL (Mei et al., 2020; Schaul et al., 2019). Even if the base\nmodel could produce a less-skewed distribution over edit distance ratios during self-correction, we still\nneed the RL training procedure to learn a self-correction strategy from the training data that generalizes\nto test prompts.\nTo see whether RL training can learn a self-correction strategy by purely optimizing the final attemptâ€™s\nreward, we ran a naÃ¯ve multi-turn RL baseline to optimize Equation 1. We find empirically in Figure 5 that\nwhile the performance of each attempt improves with training via naÃ¯ve multi-turn RL, the performance\nof the second attempt is tightly coupled with the first attempt. As training progresses, standard multi-turn\nconverges to be overly biased towards not changing its response, resulting in no self-correction ability.\nWhy does this happen? There are at least two equally good solutions when optimizing a policy with\nRL on the training data: (i) learning to improve from the first to the second attempt, or (ii) learning to\nproduce the best first-attempt response followed by no correction in the second attempt. Of course only\nthe former strategy generalizes to new problems, but an overparameterized LLM may not necessarily\nlearn strategy (i) instead of (ii), since both of these strategies appear equally optimal on the training set.\nAbstractly, learning the â€œmeta strategyâ€ of self-correction during training is difficult unless the â€œdirectâ€\nstrategy that optimizes reward appears less viable. Conceptually, this is similar to the memorization\nchallenge in meta-learning (Yin et al., 2019), which suggests that when provided with mutually exclusive\ntasks, few-shot meta-learning is likely to recover the supervised learning solution (without relying on\nadditional context from the few shots) that directly predicts the output for an input. In our case, this is\nanalogous to not self-correcting past attempts, but rather directly attempting produce a good response.\nMethod overview. Our approach SCoRe is designed to address the key challenges identified above. SCoRe\noperates in two stages. In the first stage (Stage I), SCoRe trains a model initialization that is less prone to\ncollapse in subsequent RL by explicitly teaching the model to correct its second-attempt responses under\na relatively static first-attempt distribution. This initialization amplifies the coverage of second-attempt\nresponses given the modelâ€™s own first attempt distribution, with a bias towards high-reward responses.\nWe then use this model initialization to seed the actual multi-turn RL run (Stage II). To bias learning\ntowards a solution that learns to self-correct, we shape the reward at the second attempt to provide\na large positive reward bonus in favor of self-correction. Both stages bias the model towards learning\n9\nTraining Language Models to Self-Correct via Reinforcement Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining fraction\n0.50\n0.55\n0.60\n0.65\n0.70\nTrain Accuracy\nStage I\nStage II\nMulti-turn baseline\nTurn 1\nTurn 2\n(a) Evolution of training reward with more training. When\ntraining with naÃ¯ve multi-turn RL, the responses at both the\nattempts become tightly coupled together, leading to poor cov-\nerage for subsequent iterations and worse learning progress.\nStage I in SCoRe is explicitly designed to alleviate this, leading\nto increased exploration and better final performance.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining fraction (over stage 1)\n0.20\n0.25\n0.30\n0.35\nDifferent answer freq.\nMulti-turn RL baseline\nStage 1\n(b) Frequency in which the learner proposes a different\nanswer in the second turn. Without explicitly modifying\nthe policy initialization as in SCoRe, the policy quickly learns\nto not change its answer, leading to poor exploration. This\nis evident by a decrease in the number of prompts for which\nthe second attempt produces a different answer.\nFigure 5 âˆ£Failure modes of naÃ¯ve multi-turn RL training for inducing self-correction capabilities. These results indicate that\nsome explicit approach to alter the policy initialization is required for learning. Stage I in SCoRe exactly tackles this.\nself-correction by initializing the model appropriately and controlling subsequent RL.\n5.1. Stage I: Training a Model Initialization to Prevent Collapse\nThe goal of Stage I of SCoRe is to obtain a good model initialization by improving the base modelâ€™s\ncoverage over second-attempt responses so that subsequent training for self-correction is less susceptible\nto collapse we observed with STaR/SFT. While this would typically be done via SFT in LLM fine-tuning\npipelines, our experiments in Section 4 show that SFT trains the model to latch onto only one mode\nof correction behavior. As a result, an SFT initialization is not expected to generate informative and\nexploratory traces for learning. Therefore, we do not initialize our RL training with SFT and instead\ndevelop Stage I to produce a separate initialization that is less prone to collapse.\nTo do so, we explicitly fine-tune the base model to produce high-reward revisions at the second attempt,\nwhile forcing the model to not change its first-attempt response, by constraining the first-attempt response\ndistribution as close as possible to that of the base model using a KL-divergence. While does this appear\nsub-optimal â€“ a first-attempt response with fewer mistakes could be corrected to a better second-attempt\nresponse â€“ but as we will show, this stage is critical in reducing the base modelâ€™s bias towards simply\ncoupling the first and second-attempt distributions, and thus becoming trapped in a local optima when\nactual multi-turn RL is run. Formally, the objective we optimize is:\nmax\nğœƒ\nEğ’™1,ğ’š1âˆ¼ğœ‹ğœƒ(â‹…âˆ£ğ’™),ğ’š2âˆ¼ğœ‹ğœƒ(â‹…âˆ£[ğ’™1,ğ‘1])[Ì‚ğ‘Ÿ(ğ’š2, ğ’šâˆ—) âˆ’ğ›½2ğ·ğ¾ğ¿(ğœ‹ğœƒ(â‹…âˆ£âˆ£ğ’™1)âˆ£âˆ£ğœ‹ref(â‹…âˆ£ğ’™1))],\n(3)\nwhere ğ›½2 is a hyper parameter designed to enforce a strict KL penalty only on the first attempts to\navoid shift of the first-turn responses (denoted by the term in blue). Note that we still utilize the default\nKL-divergence penalty from Equation 2, but that is applied with a much smaller weight and is omitted\nfrom Equation 3 for brevity. Indeed, we show that unlike naÃ¯ve multi-turn RL, Stage I is more effective at\ndecoupling the two responses (Figure 5b).\n10\nTraining Language Models to Self-Correct via Reinforcement Learning\noundation model\nprompt (x)\nresponse (y)\nStage I: Train an initialization for self-correction \nStage II: Run Multi-turn RL with reward shaping\nFoundation model trained with SCoRe\nfirst attempt (y1)\nsecond attempt (y2)\nprompt (x)\nMaximize \nreward\nConstrain to \nbase model\nMaximize shaped \nreward\nMaximize reward \nFoundation model trained with SCoRe\nfirst attempt (y1)\nsecond attempt (y2)\nprompt (x)\nOptionally interleave stages\nFigure 6 âˆ£An overview of our approach (SCoRe). SCoRe trains a model in two stages: Stage I: instead of running SFT\n(that produces pathological amplification of biases) to initialize RL training, we train a good initialization that can produce\nhigh-reward responses in the second-attempt while mimicking the base modelâ€™s initial response at the first attempt. Stage II:\njointly optimizing both attempts, where the latter uses a shaped reward to incentivize discovery of the self-correction strategy\ninstead of the simple strategy of product the best first response followed by making any minor edits to it in the second attempt.\n5.2. Stage II: Multi-Turn RL with Reward Shaping\nEquipped with a model initialization from Stage I that exhibits a substantially smaller bias to couple the\ntwo responses, the second stage of SCoRe now trains responses at both attempts towards optimizing\nreward in line with Equation 1. Of course, we also want to make sure to not degrade the first-attempt\nresponses in the process. Therefore, for two-turn self-correction problem, we train the policy ğœ‹ğœƒ(â‹…âˆ£â‹…)\nagainst the following objective:\nmax\nğœƒ\nEğ’™1,ğ’š1âˆ¼ğœ‹ğœƒ(â‹…âˆ£ğ’™),ğ’š2âˆ¼ğœ‹ğœƒ(â‹…âˆ£[ğ’™1,ğ‘1]) [\n2\nâˆ‘\nğ‘–=1\nÌ‚ğ‘Ÿ(ğ’šğ‘–, ğ’šâˆ—) âˆ’ğ›½1ğ·ğ¾ğ¿(ğœ‹ğœƒ(â‹…âˆ£ğ’™ğ‘–)âˆ£âˆ£ğœ‹ref(â‹…âˆ£ğ’™ğ‘–))] ,\n(4)\nwhere ğ’™ğ‘–, ğ‘–âˆˆ{1, 2} corresponds to the set of input tokens passed as context to the model. SCoRe optimizes\nEquation 4 with an on-policy policy gradient approach.\nReward shaping to incentivize self-correction. As discussed earlier, it is unclear if running RL for\noptimizing Equation 4 prefers a strategy that incentivizes self-correction over finding the best first-attempt\nresponse and keeping it unchanged, since both of these strategies appear equally good on the small\ntraining dataset. To mitigate this issue, we bias the learning problem towards the self-correction strategy\nvia reward shaping: by providing a higher emphasis to traces that flip correctness from the first attempt\nto the second, we can bias the model to learn a self-correction solution. Concretely, given an two-turn\non-policy rollout ğœ= {ğ’™1, Ë†ğ’š1, Ì‚ğ‘Ÿ(ğ’š1, ğ’šâˆ—), ğ’™2, Ë†ğ’š2, Ì‚ğ‘Ÿ(ğ’š2, ğ’šâˆ—)} (where ğ’™2 denotes all the tokens from the first\nturn concatenated with each other), we propose to modify the reward Ì‚ğ‘Ÿ(ğ’š2, ğ’šâˆ—) used for training in\nEquation 4, at the second attempt with an additional bonus Ì‚ğ‘(ğ’š2âˆ£ğ’š1, ğ’šâˆ—) given by:\nÌ‚ğ‘(ğ’š2âˆ£ğ’š1, ğ’šâˆ—) = ğ›¼â‹…(Ì‚ğ‘Ÿ(ğ’š2, ğ’šâˆ—) âˆ’Ì‚ğ‘Ÿ(ğ’š1, ğ’šâˆ—)) ,\n(5)\nwhere ğ›¼is a positive constant multiplier, ideally a real number significantly larger than 1.0. Adding this\nbonus to the second attempt only emphasizes traces that flip the correctness of the response and assigns\na heavy negative penalty to transitions that change a correct response to incorrect in the second attempt.\nIn contrast, transitions that do not flip correctness of the response and are likely to lead to collapse of not\n11\nTraining Language Models to Self-Correct via Reinforcement Learning\nmaking meaningful edits contribute much less to the overall loss. Thus, the addition of this bonus should\nregularize the training process from collapsing on to the â€œdirectâ€ solution that might look optimal on the\ntraining set but does not produce self-correction behavior on new examples.\n5.3. Putting it Together and Implementation Details\nOur approach is illustrated pictorially in Figure 6. SCoRe applies stages I and II in an interleaved fashion\nfor multiple iterations (e.g., Figure 5 shows two applications each of Stage I and II). We use a small ğ›½1 for\nall experiments (i.e., the coefficient on the KL divergence penalty against the base model in Equation 2),\nand found that setting ğ›½2 = 10ğ›½1 to work sufficiently well in our experiments. In practice, one can also\nuse an adaptive ğ›½2 that attempts to balance the magnitudes of the first-attempt KL regularization and\nthe second-attempt policy loss. In some of our experiments, we also choose to amplify the coverage of\nstates used for on-policy RL by incorporating first-attempt solutions obtained by repeatedly sampling the\nbase model as offline prompts in RL. We find that incorporating this data, especially in Stage 2, where\nthe first-turn policy may have drifted further from that of the base model, can have substantial benefits\nespecially when attempting to learn from limited data.\nTakeaways and Implications\nThe core insight behind our method is that we must make it more attractive to learn the more\nnuanced algorithmic strategy instead of collapsing to an ungeneralizable behavior mode. Further-\nmore, to avoid the challenge of distribution shift, this must be done on self-generated online data.\nSCoRe instantiates this principle when learning the model initialization for multi-turn RL (Stage\nI) and when using a reward bonus to prevent training from producing non-correcting strategies\n(Stage II).\n6. Experimental Evaluation\nThe goal of our experiments is to demonstrate the efficacy of SCoRe in teaching LLMs how to correct\ntheir own mistakes by training on their own data. In addition, we also aim to understand the impact of\neach of the components of SCoRe in contributing to this ability. To this end, we perform a comparative\nevaluation of SCoRe against prior methods that also use self-generated data to train for self-correction,\nand run several ablation studies on two representative reasoning tasks where error correction is crucial.\nTasks. We mainly focus on math and coding tasks: (a) math problem solving on MATH (Hendrycks\net al., 2021), and (b) code generation on MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021)\nfor evaluating the efficacy of our approach. Concretely, we use the following train-test splits in our\nexperiments: (1) MATH: following Lightman et al. (2023), we augment the MATH training set with 4500\nproblems from the test set, and report results on the remaining 500 problems; and (2) Code generation:\nwe train on MBPP and report results on HumanEval, which does not expose test cases to the model.\nEvaluation protocol and metrics. We report the self-correction accuracy on a number of tasks with\ntwo sequential attempts at the problem, i.e., one round of self-correction. For MBPP, following the\nevaluation protocol of Ni et al. (2024), we also report results on MBPP-R, an offline repair task that\nrequires correcting incorrect first-attempt programs generated from PaLM 2.\nModels. For all of our experiments on MBPP, we fine-tune Gemini 1.0 Pro and for MATH, we fine-\ntune Gemini 1.5 Flash. For all evaluations, we use greedy decoding (i.e. temperature 0), except for\ninference-compute scaling in Section 6.2 where we set temperature to be 0.7. For all training methods, we\n12\nTraining Language Models to Self-Correct via Reinforcement Learning\nTable 3 âˆ£Performance of SCoRe on MATH. Observe that SCoRe not only attains a higher accuracy at both attempts, but also\nprovides the most positive self-correction performance Î”(t1, t2), and improves upon the number of problems that move from\nincorrect to correct, while substantially reducing the number of problems that become incorrect in the second attempt.\nApproach\nAccuracy@t1\nAccuracy@t2\nÎ”(t1, t2)\nÎ”iâ†’c(t1, t2)\nÎ”câ†’i(t1, t2)\nBase model\n52.6%\n41.4%\n-11.2%\n4.6%\n15.8%\nSelf-Refine\n52.8%\n51.8%\n-1.0%\n3.2%\n4.2%\nSTaR w/ ğ’Ÿ+\nStaR\n53.6%\n54.0%\n0.4%\n2.6%\n2.2%\nPair-SFT w/ ğ’ŸSFT\n52.4%\n54.2%\n1.8%\n5.4%\n3.6%\nSCoRe (Ours)\n60.0%\n64.4%\n4.4%\n5.8%\n1.4%\nattempted to use a fixed budget of model samples and gradient updates, and do not vary hyperparameters\nsuch as learning rate and batch size between runs. For all RL runs, we selected checkpoints with the\nhighest training reward, although a small held-out validation set of problems can also be used. Additional\ndetails about the experimental setup can be found in the Appendix.\nEvaluation prompts. We use a zero-shot CoT prompting for evaluation on MATH, zero-shot prompting\nfor evaluation on HumanEval, and the canonical three-shot prompt for first-attempt training samples\non MBPP. At the second attempt, we utilize an instruction that does not reveal the correctness of the\nprevious answer, but asks the model to attempt to deduce whether a mistake exists in its first attempt\nresponse, and, if so, potentially rewrite its response. Full prompts and self-correction instructions can be\nfound in Appendix A.\nPrior approaches and comparisons. We compare SCoRe to prior approaches and baselines. We compare\nto Self-Refine (Madaan et al., 2023), a representative prompting-based approach to elicit self-correction\nbehaviors from a model, akin to Reflexion (Shinn et al., 2023). Of the fine-tuning based approaches, we\ncompare to Pair-SFT based on the approach from Welleck et al. (2023), and multi-turn STaR (Singh\net al., 2023; Zelikman et al., 2022) that fine-tune the model by minimizing negative log-likelihood\non synthetically paired repair traces and successful repair traces respectively. Due to a difference in\nassumptions and base models, we cannot compare SCoRe directly with results in prior papers that utilize\noracle information (e.g., RISE (Qu et al., 2024)) or run multiple models (e.g., GLoRE (Havrilla et al.,\n2024b), the full version of Self-Correct (Welleck et al., 2023) with a refinement model), largely because\nthese comparisons will be apples-to-oranges with distinct setups, different refinement or oracle models.\n6.1. Benchmark Results\nMATH. Our results are in Table 3, as well as in Figure 1. SCoRe exhibits substantially stronger performance\non both direct and self-correction accuracies. Notably, the intrinsic self-correction gain Î”(t1, t2) of 4.4%\nis the first significantly positive delta, despite having fewer incorrect problems to correct by virtue of its\nhigher Accuracy@t1. Relative to the base 1.5 Flash model, SCoRe improves Î”(t1, t2) by 15.6%, and\nAccuracy@t2 by 23.0%, and over the nearest baseline, Pair-SFT, by 10.2% and 2.6% respectively.\nBy observing the frequency of problems that change from incorrect at from the first attempt to correct\nin the second attempt and vice versa, we see that SCoRe improves the rate at which it fixes incorrect\nanswers (14.5%, compared to 9.5% for base) and reduces the proportion of correct answers it changes.\n13\nTraining Language Models to Self-Correct via Reinforcement Learning\nTable 4 âˆ£Performance of SCoRe on HumanEval. Observe that ScoRe attains the highest accuracy at the second attempt\n(Accuracy@t2), and also substantially improves the number of problems that become correct with the use of additional\nsequential attempts. In addition, SCoRe also attains the highest correction rate on MBPP-R, an offline repair task.\nMethod\nMBPP-R\nAccuracy@t1\nAccuracy@t2\nÎ”(t1, t2)\nÎ”iâ†’c(t1, t2)\nÎ”câ†’i(t1, t2)\nBase model\n47.3%\n53.7%\n56.7%\n3.0%\n7.9%\n4.9%\nSelf-Refine\n30.7%\n53.7%\n52.5%\n-1.2%\n9.8%\n11.0%\nPair-SFT\n59.8%\n56.1%\n54.3%\n-1.8%\n4.3%\n6.1%\nSCoRe (Ours)\n60.6%\n52.4%\n64.6%\n12.2%\n15.2%\n3.0%\nTable 5 âˆ£Ablation studies to understand the impact of various components in SCoRe. Observe that while single-turn\ntraining is effective at optimizing the first-attempt accuracy of the model, it leads to degradation in the second attempt. Instead,\nSCoRe allows us to attain a higher second-attempt accuracy even though it attains a slightly worse first-attempt accuracy. The\nperformance improvements without Stage I or without reward shaping in SCoRe are small when measured by the difference in\naccuracy over the two attempts. Utilizing STaR generally leads to worse performance even when it is run from an effective\nStage I checkpoint. These results highlight the importance of various components in SCoRe.\nMethod\nAccuracy@t1\nAccuracy@t2\nÎ”(t1, t2)\nSCoRe (Ours)\n60.0%\n64.4%\n4.4%\nw/o multi-turn training\n61.8%\n59.4%\n-2.4%\nw/o Stage I\n59.2%\n61.4%\n2.2%\nw/o reward shaping\n60.0%\n62.6%\n2.6%\nw/ STaR instead of REINFORCE Stage II\n56.2%\n58.4%\n2.2%\nCode generation. Our results for the code generation task are shown in Table 4. Generally, we find that\nSCoRe achieves both improved self-correction as well as strong offline repair performance. For MBPP-R,\nwe find that SCoRe improves the base model from 47.3% to 60.6%, which is comparable to the gap\nbetween GPT-3.5 and GPT-4 (42.9% and 63.2% respectively) (Ni et al., 2024). Despite only training\non MBPP, we find that SCoRe is especially effective at generalizing to HumanEval, achieving a 12.2%\nintrinsic self-correction delta, or 9% higher than the base model. By contrast, Pair-SFT works nearly\nas well on the static repair task MBPP-R, but actually degrades the base model when evaluated in the\nself-correction setting, thus underscoring the importances of on-policy sampling for self-correction.\n6.2. Inference-Compute Scaling with Self-Correction\nNext, we investigate if SCoRe can be used in conjunction with inference-time compute scaling strategies.\nTo do so, we evaluate self-consistency decoding (Wang et al., 2022), also known as majority voting, where\nwe sample a diverse set of solutions, and then select the most consistent answer among these solutions.\nTypically, the default strategy is to sample all solutions in parallel to perform majority voting. However,\nwe show in Figure 1 (right) that instead of sampling 2ğ¾solutions in parallel, it is more compute-efficient\nto sample ğ¾solutions in parallel, then perform one round of self-correction on each solution. With 32\nsolution budget per problem, parallel sampling shows a 7.4% accuracy gain, while combining it with\nsequential sampling using self-correction yields a 10.5% improvement.\n14\nTraining Language Models to Self-Correct via Reinforcement Learning\n6.3. Ablation Studies: Understanding the Impact of SCoRe Components\nFinally, we also present a number of ablation studies to understand the importance of various compo-\nnents in SCoRe. We perform these ablations on the MATH dataset. Concretely, we aim to answer the\nfollowing questions: (1) the importance of multi-turn training: Can RL trained to maximize single-turn\nperformance achieve better accuracy@t1 or accuracy@t2?; (2) the importance of multi-stage training:\nHow essential is Stage I to SCoRe? In other words, why not run Stage II directly?; (3) the impact of\nreward shaping. How would removing the reward shaping terms affect performance of SCoRe in Stage\nII, assuming Stage I was done identically?; (4) the importance of on-policy RL: What if we replaced\nREINFORCE in Stage II with STaR?.\nThe results of all of these ablation experiments are shown in Table 5. As expected, single-turn training\nimproves turn 1 performance, but has negative Î”(t1, t2). As shown in Figure 5, Stage I is critical to\nSCoRe; without it, the model achieves 2% lower Î”(t1, t2) and 3% lower accuracy@t2. Similarly, we find\nthat removing reward shaping also hurts performance, indicating that the RL objectives in both stages\nplay a significant role in teaching self-correction behavior. We also find that replacing REINFORCE with\nSTaR in Stage II results in significantly lower absolute performance with no visible improvements in\nself-improvement performance, which contrasts with the findings in Havrilla et al. (2024a) that STaR and\non-policy RL have similar convergence rates for single-turn RL. This suggests that leveraging on-policy\nsamples is especially critical in the self-correction setting, which presents a multi-turn problem that\nadmits potentially spurious solutions.\n6.4. Qualitative Analysis of SCoRe\nWe also perform a qualitative investigation into how SCoRe addresses the self-repair shortcomings of\nbase LLMs, and provide several examples in Appendix B. We find that SCoRe is able to refine its own\nresponses in a variety of manners - rewriting the entire solution when necessary, or reproducing the\ncorrect parts of the solution, while revising the incorrect ones. For the latter, we interestingly find that\nSCoRe is especially adept at revising its computational mistakes, and even demonstrates a bias towards\nshowing more steps in certain computations and manipulations in order to increase its probability of\nproducing a correct answer. We additionally observe that the model learns to occasionally self-correct\nwithin a turn, e.g. MATH example 4.\n7. Discussion, Limitations, and Conclusion\nIn this work, we investigated how to imbue LLMs with a self-correction strategy that enables them\nto correct their own responses on the fly, at test-time. Specifically, we proposed SCoRe, a multi-turn\nonline reinforcement learning (RL) approach for training language models to correct their own mistakes,\nand demonstrated through extensive evaluations that it is the first method that can attain significantly\npositive intrinsic self-correction performance. To motivate the design of SCoRe, we rigorously analyzed\nthe behavior of various fine-tuning baselines and identified failure modes in which the model learns a\nnon-correcting strategy (e.g. learning to make no edits) under these approaches. SCoRe is designed to\nelicit a self-correcting strategy by utilizing a two-stage structure and reward shaping, both of which help\nprevent model collapse into not learning effective self-improvement behavior.\nLimitations. There are various limitations of this work that also provide interesting avenues for future\nwork. We did not train SCoRe for more than one round of iterative self-correction in this paper, which\nmeans that subsequent rounds of self-correction may not be as effective as the first one. An interesting\navenue for future work is to train with more than two attempts via RL, which is already a common and\n15\nTraining Language Models to Self-Correct via Reinforcement Learning\neffective practice to obtain effective self-correction behavior over more than two rounds with SFT (Qu\net al., 2024; Snell et al., 2024). Unifying Stages I and II of SCoRe is also an interesting avenue for\nresearch, since that would alleviate the limitation of running multiple steps and help in designing a more\nrobust method.\nBroader perspectives. Our work has several implications. First, it suggests that learning meta-strategies\n(e.g., self-correction in this paper) might require going beyond the standard paradigm of supervised\nfine-tuning followed by single-turn RL (as shown in Section 4). It demonstrates that multi-turn RL can\nprovide for one such approach. Second, our results also hint that perhaps using more detailed or granular\nsupervision when generating on-policy rollouts in multi-turn RL might further improve the modelâ€™s\ncapabilities at implementing nuanced strategies: even though SCoRe did not use dense or fine-grained\nfeedback, it was already able to improve performance of existing models substantially. Utilizing dense\nfeedback is likely to complement our method well. Finally, the importance of our two-stage recipe (based\non careful initialization and reward shaping) in obtaining positive self-correction perhaps more generally\nhints that some kind of regularization is required to ensure that LLMs learn nuanced strategies that can\ngeneralize well to novel, unseen queries at test-time.\nAcknowledgements\nThe authors would like to thank Satinder Baveja, Kalesha Bullard, Gheorghe Comanici, Claire Cui,\nValentin Dalibard, Angelos Filos, Yang Gao, Zoubin Ghahramani, Izzeddin Gur, Raia Hadsell, Clara Huiyi\nHu, Melvin Johnson, Mina Khan, Balaji Lakshminarayanan, Yiran Mao, Hussain Masoom, Junhyuk Oh,\nJordi Orbay, David Silver, and Yury Sulsky for helpful discussions, feedback, and sponsorship. We thank\nAmrith Setlur, Yuxiao Qu, Charlie Snell, Tianhe Yu, and Xinyang (Young) Geng for helpful discussions\nand feedback on an earlier version of the paper.\nAuthor Contributions\nAK and VZ led the paper, with substantial technical contributions from RA and YS. VZ led the experimen-\ntation in the final paper with AK, with support from RA and YS. AK and RA conceived the initial idea\nwith advice and discussions from DS, FB, AF, JDC, AS, and GT. JDC, YS, AS, RA, and AK iterated on the\nmethodology. The development of the final method was done by AK and VZ, with inputs from RA and\nFB. VZ led the infrastructure development, while RA, YS, CP, SI, KB, DS, and LMZ contributed to the\ninfrastructure. AK, RA, FB, AF, DP, GT advised on the overall direction. AK and VZ wrote the manuscript,\nwith input from all co-authors. KM provided program management. FB, and AF co-supervised the project.\nReferences\nA. Ahmadian, C. Cremer, M. GallÃ©, M. Fadaee, J. Kreutzer, A. ÃœstÃ¼n, and S. Hooker. Back to basics:\nRevisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint\narXiv:2402.14740, 2024.\nA. F. AkyÃ¼rek, E. AkyÃ¼rek, A. Madaan, A. Kalyan, P. Clark, D. Wijaya, and N. Tandon. Rl4f: Generating\nnatural language feedback with reinforcement learning for repairing model outputs. arXiv preprint\narXiv:2305.08844, 2023.\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al.\nProgram synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n16\nTraining Language Models to Self-Correct via Reinforcement Learning\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,\nG. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374,\n2021.\nX. Chen, M. Lin, N. SchÃ¤rli, and D. Zhou. Teaching large language models to self-debug. arXiv preprint\narXiv:2304.05128, 2023.\nZ. Chen, Y. Deng, H. Yuan, K. Ji, and Q. Gu. Self-play fine-tuning converts weak language models to\nstrong language models. arXiv preprint arXiv:2401.01335, 2024.\nJ. Farebrother, J. Orbay, Q. Vuong, A. A. TaÃ¯ga, Y. Chebotar, T. Xiao, A. Irpan, S. Levine, P. S. Castro,\nA. Faust, et al. Stop regressing: Training value functions via classification for scalable deep rl. arXiv\npreprint arXiv:2403.03950, 2024.\nA. Havrilla, Y. Du, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, S. Sukhbaatar,\nand R. Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint\narXiv:2403.04642, 2024a.\nA. Havrilla, S. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, and R. Railneau.\nGlore: When, where, and how to improve llm reasoning via global and local refinements. arXiv preprint\narXiv:2402.10963, 2024b.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring\nmathematical problem solving with the math dataset. NeurIPS, 2021.\nJ. Hong, N. Lee, and J. Thorne. Reference-free monolithic preference optimization with odds ratio. arXiv\npreprint arXiv:2403.07691, 2024.\nJ. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large language models\ncannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023.\nN. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.\nLivecodebench: Holistic and contamination free evaluation of large language models for code. arXiv\npreprint arXiv:2403.07974, 2024.\nR. Kamoi, Y. Zhang, N. Zhang, J. Han, and R. Zhang. When can llms actually correct their own mistakes?\na critical survey of self-correction of llms. arXiv preprint arXiv:2406.01297, 2024.\nG. Kim, P. Baldi, and S. McAleer.\nLanguage models can solve computer tasks.\narXiv preprint\narXiv:2303.17491, 2023.\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and\nK. Cobbe. Letâ€™s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\nX. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, et al. Agentbench:\nEvaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023.\nA. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi, A. Tang, D. Pykhtar, J. Liu, Y. Wei, et al.\nStarcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.\n17\nTraining Language Models to Self-Correct via Reinforcement Learning\nA. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,\nY. Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651,\n2023.\nJ. Mei, C. Xiao, C. Szepesvari, and D. Schuurmans. On the global convergence rates of softmax policy\ngradient methods. In International Conference on Machine Learning, pages 6820â€“6829. PMLR, 2020.\nA. Ni, M. Allamanis, A. Cohan, Y. Deng, K. Shi, C. Sutton, and P. Yin. Next: Teaching large language\nmodels to reason about code execution. arXiv preprint arXiv:2404.14662, 2024.\nT. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama. Is self-repair a silver bullet for code\ngeneration? In The Twelfth International Conference on Learning Representations, 2023.\nL. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, and W. Y. Wang. Automatically correcting large language\nmodels: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188,\n2023.\nD. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings. Refiner: Reasoning\nfeedback on intermediate representations. arXiv preprint arXiv:2304.01904, 2023.\nY. Qu, T. Zhang, N. Garg, and A. Kumar. Recursive introspection: Teaching foundation models how to\nself-improve. 2024.\nM. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat,\nJ. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of\ncontext. arXiv preprint arXiv:2403.05530, 2024.\nW. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike. Self-critiquing models for assisting\nhuman evaluators. arXiv preprint arXiv:2206.05802, 2022.\nT. Schaul, D. Borsa, J. Modayil, and R. Pascanu. Ray interference: a source of plateaus in deep reinforce-\nment learning. CoRR, abs/1904.11455, 2019. URL http://arxiv.org/abs/1904.11455.\nL. Shani, A. Rosenberg, A. Cassel, O. Lang, D. Calandriello, A. Zipori, H. Noga, O. Keller, B. Piot,\nI. Szpektor, et al. Multi-turn reinforcement learning from preference human feedback. arXiv preprint\narXiv:2405.14655, 2024.\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the\nlimits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\nN. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and\nself-reflection. arXiv preprint arXiv:2303.11366, 2023.\nA. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil, P. J. Liu, J. Harrison, J. Lee, K. Xu, A. Parisi, et al.\nBeyond human data: Scaling self-training for problem-solving with language models. arXiv preprint\narXiv:2312.06585, 2023.\nC. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine. Offline rl for natural language generation with\nimplicit language q learning. arXiv preprint arXiv:2206.11871, 2022.\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than\nscaling model parameters. arXiv preprint arXiv:2408.03314, 2024.\n18\nTraining Language Models to Self-Correct via Reinforcement Learning\nC. Team. Codegemma: Open code models based on gemma. arXiv preprint arXiv:2406.11409, 2024.\nG. Tyen, H. Mansoor, V. CÄƒrbune, Y. P. Chen, and T. Mak. Llms cannot find reasoning errors, but can\ncorrect them given the error location. In Findings of the Association for Computational Linguistics ACL\n2024, pages 13894â€“13908, 2024.\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency\nimproves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\nS. Welleck, X. Lu, P. West, F. Brahman, T. Shen, D. Khashabi, and Y. Choi. Generating sequences by\nlearning to self-correct. In The Eleventh International Conference on Learning Representations, 2023.\nURL https://openreview.net/forum?id=hH36JeQZDaO.\nW. Xiong, C. Shi, J. Shen, A. Rosenberg, Z. Qin, D. Calandriello, M. Khalman, R. Joshi, B. Piot,\nM. Saleh, et al. Building math agents with multi-turn iterative preference learning. arXiv preprint\narXiv:2409.02392, 2024.\nS. Ye, Y. Jo, D. Kim, S. Kim, H. Hwang, and M. Seo. Selfee: Iterative self-revising llm empowered by\nself-feedback generation. Blog post, 2023.\nM. Yin, G. Tucker, M. Zhou, S. Levine, and C. Finn. Meta-learning without memorization. arXiv preprint\narXiv:1912.03820, 2019.\nL. Yuan, G. Cui, H. Wang, N. Ding, X. Wang, J. Deng, B. Shan, H. Chen, R. Xie, Y. Lin, et al. Advancing\nllm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078, 2024.\nE. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances in\nNeural Information Processing Systems, 35:15476â€“15488, 2022.\nY. Zhang, M. Khalifa, L. Logeswaran, J. Kim, M. Lee, H. Lee, and L. Wang. Small language models need\nstrong verifiers to self-correct reasoning. arXiv preprint arXiv:2404.17140, 2024.\nH. S. Zheng, S. Mishra, H. Zhang, X. Chen, M. Chen, A. Nova, L. Hou, H.-T. Cheng, Q. V. Le, E. H. Chi,\net al. Natural plan: Benchmarking llms on natural language planning. arXiv preprint arXiv:2406.04520,\n2024.\nY. Zhou, A. Zanette, J. Pan, S. Levine, and A. Kumar. Archer: Training language model agents via\nhierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.\n19\nTraining Language Models to Self-Correct via Reinforcement Learning\nAppendices\nA. Prompts\nMATH Zero-shot Prompt\nYou are a math expert. When you respond, respond only with the Solution of the final Problem, thinking step by\nstep. At the end of the Solution, when you give your final answer, write it in the form \"Final Answer: The final\nanswer is $answer$. I hope it is correct.\"\nMATH Self-Correction Instruction\nThere might be an error in the solution above because of lack of understanding of the question. Please correct\nthe error, if any, and rewrite the solution. Only output the final solution! At the end of the Solution, when you\ngive your final answer, write it in the form \"Final Answer: The final answer is $answer$. I hope it is correct.\"\nMBPP 3-shot Prompt\nYou are an expert Python programmer, and here is your task: Write a function to find the similar elements from\nthe given two tuple lists. Your code should pass these tests:\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)\n[BEGIN]\ndef similar_elements(test_tup1, test_tup2):\nres = tuple(set(test_tup1) & set(test_tup2))\nreturn (res)\n[DONE]\nYou are an expert Python programmer, and here is your task: Write a python function to identify nonâˆ’prime\nnumbers. Your code should pass these tests:\nassert is_not_prime(2) == False\nassert is_not_prime(10) == True\nassert is_not_prime(35) == True\n[BEGIN]\nimport math\ndef is_not_prime(n):\nresult = False\nfor i in range(2,int(math.sqrt(n)) + 1):\nif n %\nresult = True\nreturn result\n20\nTraining Language Models to Self-Correct via Reinforcement Learning\n[DONE]\nYou are an expert Python programmer, and here is your task: Write a function to find the largest integers from a\ngiven list of numbers using heap queue algorithm. Your code should pass these tests:\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65]\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75]\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]\n[BEGIN]\nimport heapq as hq\ndef heap_queue_largest(nums,n):\nlargest_nums = hq.nlargest(n, nums)\nreturn largest_nums\n[DONE]\nMBPP/HumanEval Self-Correction Instruction\n# There might be an error in the code above because of lack of understanding of the question. Please correct the\nerror, if any, and rewrite the solution. Only output the final correct Python program!\nB. Examples of SCoRe Self-Correction\nHumanEval Example 1\nProblem:\ndef cycpattern_check(a , b):\n\"\"\"You are given 2 words. You need to return True if the second word or any of its rotations is a substring in\nthe first word\ncycpattern_check(\"abcd\",\"abd\") => False\ncycpattern_check(\"hello\",\"ell\") => True\ncycpattern_check(\"whassup\",\"psus\") => False\ncycpattern_check(\"abab\",\"baa\") => True\ncycpattern_check(\"efef\",\"eeff\") => False\ncycpattern_check(\"himenss\",\"simen\") => True\n\"\"\"\nTurn 1 solution (incorrect):\ndef cycpattern_check(a , b):\n\"\"\"You are given 2 words. You need to return True if the second word or any of its rotations is a substring in\nthe first word\ncycpattern_check(\"abcd\",\"abd\") => False\ncycpattern_check(\"hello\",\"ell\") => True\ncycpattern_check(\"whassup\",\"psus\") => False\ncycpattern_check(\"abab\",\"baa\") => True\ncycpattern_check(\"efef\",\"eeff\") => False\n21\nTraining Language Models to Self-Correct via Reinforcement Learning\ncycpattern_check(\"himenss\",\"simen\") => True\n\"\"\"\nfor i in range(len(b)):\nif a.find(b[i:])!=âˆ’1:\nreturn True\nreturn False\nTurn 2 solution (correct):\ndef cycpattern_check(a , b):\n\"\"\"You are given 2 words. You need to return True if the second word or any of its rotations is a substring in\nthe first word\ncycpattern_check(\"abcd\",\"abd\") => False\ncycpattern_check(\"hello\",\"ell\") => True\ncycpattern_check(\"whassup\",\"psus\") => False\ncycpattern_check(\"abab\",\"baa\") => True\ncycpattern_check(\"efef\",\"eeff\") => False\ncycpattern_check(\"himenss\",\"simen\") => True\n\"\"\"\nfor i in range(len(b)):\nrotated_b = b[i:] + b[:i]\nif a.find(rotated_b) != âˆ’1:\nreturn True\nreturn False\nHumanEval Example 2\nProblem:\ndef even_odd_count(num):\n\"\"\"Given an integer. return a tuple that has the number of even and odd digits respectively.\nExample:\neven_odd_count(âˆ’12) ==> (1, 1)\neven_odd_count(123) ==> (1, 2)\n\"\"\"\nTurn 1 solution (incorrect):\ndef even_odd_count(num):\n\"\"\"Given an integer. return a tuple that has the number of even and odd digits respectively.\nExample:\neven_odd_count(âˆ’12) ==> (1, 1)\neven_odd_count(123) ==> (1, 2)\n\"\"\"\neven_count = 0\nodd_count = 0\nfor digit in str(num):\nint_digit = int(digit)\nif int_digit %\neven_count += 1\n22\nTraining Language Models to Self-Correct via Reinforcement Learning\nelse:\nodd_count += 1\nreturn even_count, odd_count\nTurn 2 solution (correct):\ndef even_odd_count(num):\n\"\"\"Given an integer. return a tuple that has the number of even and odd digits respectively.\nExample:\neven_odd_count(âˆ’12) ==> (1, 1)\neven_odd_count(123) ==> (1, 2)\n\"\"\"\neven_count = 0\nodd_count = 0\nfor digit in str(abs(num)):\nint_digit = int(digit)\nif int_digit %\neven_count += 1\nelse:\nodd_count += 1\nreturn even_count, odd_count\n23\nTraining Language Models to Self-Correct via Reinforcement Learning\nMATH Example 1\nProblem. Let ğ‘¨be a matrix such that\nğ‘¨\nâ›\nâœâœ\nâ\n3\n1\n0\nâ\nâŸâŸ\nâ \n=\nâ›\nâœâœ\nâ\n2\n4\nâˆ’3\nâ\nâŸâŸ\nâ \nand\nğ‘¨\nâ›\nâœâœ\nâ\nâˆ’5\n2\n2\nâ\nâŸâŸ\nâ \n=\nâ›\nâœâœ\nâ\n0\nâˆ’5\nâˆ’5\nâ\nâŸâŸ\nâ \n.\nFind ğ‘¨\nâ›\nâœâœ\nâ\nâˆ’13\n3\n4\nâ\nâŸâŸ\nâ \n.\nSCoRe turn 1 solution (incorrect). We can write\nâ›\nâœâœ\nâ\nâˆ’13\n3\n4\nâ\nâŸâŸ\nâ \n= ğ‘\nâ›\nâœâœ\nâ\n3\n1\n0\nâ\nâŸâŸ\nâ \n+ ğ‘\nâ›\nâœâœ\nâ\nâˆ’5\n2\n2\nâ\nâŸâŸ\nâ \nfor some scalars ğ‘and ğ‘. Solving the system of equations\n3ğ‘âˆ’5ğ‘= âˆ’13,\nğ‘+ 2ğ‘= 3,\n2ğ‘= 4,\nwe find ğ‘= âˆ’1 and ğ‘= 2. Therefore,\nğ‘¨\nâ›\nâœâœ\nâ\nâˆ’13\n3\n4\nâ\nâŸâŸ\nâ \n= ğ‘¨\nâ›\nâœâœ\nâ\nâˆ’1\nâ›\nâœâœ\nâ\n3\n1\n0\nâ\nâŸâŸ\nâ \n+ 2\nâ›\nâœâœ\nâ\nâˆ’5\n2\n2\nâ\nâŸâŸ\nâ \nâ\nâŸâŸ\nâ \n= âˆ’1ğ‘¨\nâ›\nâœâœ\nâ\n3\n1\n0\nâ\nâŸâŸ\nâ \n+ 2ğ‘¨\nâ›\nâœâœ\nâ\nâˆ’5\n2\n2\nâ\nâŸâŸ\nâ \n= âˆ’1\nâ›\nâœâœ\nâ\n2\n4\nâˆ’3\nâ\nâŸâŸ\nâ \n+ 2\nâ›\nâœâœ\nâ\n0\nâˆ’5\nâˆ’5\nâ\nâŸâŸ\nâ \n=\nâ›\nâœâœ\nâ\nâˆ’2\nâˆ’14\nâˆ’13\nâ\nâŸâŸ\nâ \n.\nFinal Answer: The final answer is\nâ›\nâœâœ\nâ\nâˆ’2\nâˆ’14\nâˆ’13\nâ\nâŸâŸ\nâ \n. I hope it is correct.\nSCoRe turn 2 solution (correct). We can write\nâ›\nâœâœ\nâ\nâˆ’13\n3\n4\nâ\nâŸâŸ\nâ \n= ğ‘\nâ›\nâœâœ\nâ\n3\n1\n0\nâ\nâŸâŸ\nâ \n+ ğ‘\nâ›\nâœâœ\nâ\nâˆ’5\n2\n2\nâ\nâŸâŸ\nâ \nfor some scalars ğ‘and ğ‘. This gives us the system of equations:\n3ğ‘âˆ’5ğ‘= âˆ’13,\nğ‘+ 2ğ‘= 3,\n2ğ‘= 4.\nSolving the third equation, we get ğ‘= 2. Substituting into the second equation, we get ğ‘= âˆ’1. We can\ncheck that these values of ğ‘and ğ‘also satisfy the first equation.\nTherefore,\nğ‘¨\nâ›\nâœâœ\nâ\nâˆ’13\n3\n4\nâ\nâŸâŸ\nâ \n= ğ‘¨\nâ›\nâœâœ\nâ\nâˆ’1\nâ›\nâœâœ\nâ\n3\n1\n0\nâ\nâŸâŸ\nâ \n+ 2\nâ›\nâœâœ\nâ\nâˆ’5\n2\n2\nâ\nâŸâŸ\nâ \nâ\nâŸâŸ\nâ \n= âˆ’1ğ‘¨\nâ›\nâœâœ\nâ\n3\n1\n0\nâ\nâŸâŸ\nâ \n+2ğ‘¨\nâ›\nâœâœ\nâ\nâˆ’5\n2\n2\nâ\nâŸâŸ\nâ \n= âˆ’1\nâ›\nâœâœ\nâ\n2\n4\nâˆ’3\nâ\nâŸâŸ\nâ \n+2\nâ›\nâœâœ\nâ\n0\nâˆ’5\nâˆ’5\nâ\nâŸâŸ\nâ \n=\nâ›\nâœâœ\nâ\nâˆ’2\nâˆ’4\n3\nâ\nâŸâŸ\nâ \n+\nâ›\nâœâœ\nâ\n0\nâˆ’10\nâˆ’10\nâ\nâŸâŸ\nâ \n=\nâ›\nâœâœ\nâ\nâˆ’2\nâˆ’14\nâˆ’7\nâ\nâŸâŸ\nâ \n.\nFinal Answer: The final answer is\nâ›\nâœâœ\nâ\nâˆ’2\nâˆ’14\nâˆ’7\nâ\nâŸâŸ\nâ \n. I hope it is correct.\n24\nTraining Language Models to Self-Correct via Reinforcement Learning\nMATH Example 2\nProblem. Simplify 1\n5 â‹…8\n7 Ã· 12\n20.\nSCoRe turn 1 solution (incorrect). First, we can simplify the division by multiplying by the reciprocal of\nthe divisor:\n1\n5 â‹…8\n7 Ã· 12\n20 = 1\n5 â‹…8\n7 â‹…20\n12\nNext, we can simplify by canceling common factors:\n1\n5 â‹…8\n7 â‹…20\n12 = 1 â‹…8 â‹…20\n5 â‹…7 â‹…12 = 1 â‹…2 â‹…4 â‹…5 â‹…4\n5 â‹…7 â‹…2 â‹…6\n= 4 â‹…4\n7 â‹…6 = 2 â‹…2 â‹…2 â‹…2\n7 â‹…2 â‹…3\n= 2 â‹…2\n7 â‹…3\nFinally, we can multiply the remaining factors:\n2 â‹…2\n7 â‹…3 = 4\n21\nFinal Answer: The final answer is\n4\n21. I hope it is correct.\nSCoRe turn 2 solution (correct).\n1\n5 â‹…8\n7 Ã· 12\n20 = 1\n5 â‹…8\n7 â‹…20\n12 = 1 â‹…8 â‹…20\n5 â‹…7 â‹…12 = 1 â‹…8 â‹…4 â‹…5\n5 â‹…7 â‹…4 â‹…3 = 1 â‹…8\n7 â‹…3 = 8\n21\nFinal Answer: The final answer is\n8\n21. I hope it is correct.\nMATH Example 3\nProblem. Suppose the polynomial\nğ‘“(ğ‘¥) = ğ‘ğ‘›ğ‘¥\nğ‘›+ ğ‘ğ‘›âˆ’1ğ‘¥\nğ‘›âˆ’1 + â‹¯+ ğ‘2ğ‘¥2 + ğ‘1ğ‘¥+ ğ‘0\nhas integer coefficients, and its roots are distinct integers.\nGiven that ğ‘ğ‘›= 2 and ğ‘0 = 66, what is the least possible value of âˆ£ğ‘ğ‘›âˆ’1âˆ£?\nSCoRe turn 1 solution (incorrect). Let the roots of ğ‘“(ğ‘¥) be ğ‘Ÿ1, ğ‘Ÿ2, . . . , ğ‘Ÿğ‘›. Then by Vietaâ€™s formulas,\nğ‘Ÿ1 + ğ‘Ÿ2 + â‹…â‹…â‹…+ ğ‘Ÿğ‘›= âˆ’ğ‘ğ‘›âˆ’1\nğ‘ğ‘›\n= âˆ’ğ‘ğ‘›âˆ’1\n2\n.\nSince the roots are distinct integers, ğ‘Ÿ1 + ğ‘Ÿ2 + â‹…â‹…â‹…+ ğ‘Ÿğ‘›is an integer. Thus, ğ‘ğ‘›âˆ’1 must be even.\nAlso,\nğ‘Ÿ1ğ‘Ÿ2 . . . ğ‘Ÿğ‘›= (âˆ’1)ğ‘›ğ‘0\nğ‘ğ‘›\n= (âˆ’1)ğ‘›â‹…66\n2\n= (âˆ’1)\nğ‘›â‹…33.\nSince the roots are integers, ğ‘Ÿ1ğ‘Ÿ2 . . . ğ‘Ÿğ‘›is an integer. Thus, ğ‘›must be odd.\nSince ğ‘›is odd, the sum of the roots is an integer, and ğ‘ğ‘›âˆ’1 is even, the least possible value of âˆ£ğ‘ğ‘›âˆ’1âˆ£is 2 .\nFinal Answer: The final answer is 2. I hope it is correct.\nSCoRe turn 2 solution (correct). Let the roots of ğ‘“(ğ‘¥) be ğ‘Ÿ1, ğ‘Ÿ2, . . . , ğ‘Ÿğ‘›. Then by Vietaâ€™s formulas,\nğ‘Ÿ1 + ğ‘Ÿ2 + â‹…â‹…â‹…+ ğ‘Ÿğ‘›= âˆ’ğ‘ğ‘›âˆ’1\nğ‘ğ‘›\n= âˆ’ğ‘ğ‘›âˆ’1\n2\n.\nSince the roots are distinct integers, ğ‘Ÿ1 + ğ‘Ÿ2 + â‹…â‹…â‹…+ ğ‘Ÿğ‘›is an integer. Thus, ğ‘ğ‘›âˆ’1 must be even.\n25\nTraining Language Models to Self-Correct via Reinforcement Learning\nAlso,\nğ‘Ÿ1ğ‘Ÿ2 . . . ğ‘Ÿğ‘›= (âˆ’1)ğ‘›ğ‘0\nğ‘ğ‘›\n= (âˆ’1)ğ‘›â‹…66\n2\n= (âˆ’1)\nğ‘›â‹…33.\nSince the roots are integers, ğ‘Ÿ1ğ‘Ÿ2 . . . ğ‘Ÿğ‘›is an integer. Since 33 has a factor of 11, one of the roots must be a\nmultiple of 11. Furthermore, since the roots are distinct integers, the product of the roots must be a multiple\nof 33.\nWe want to minimize âˆ£ğ‘ğ‘›âˆ’1âˆ£, which is the absolute value of the sum of the roots. To do this, we can try to\nmake the sum of the roots as close to zero as possible.\nSince the product of the roots is Â±33, we can consider the following cases:\n* **Case 1:** ğ‘›= 3 and the roots are âˆ’11, 1, and 3. In this case, the sum of the roots is âˆ’7, and âˆ£ğ‘ğ‘›âˆ’1âˆ£= 14.\n* **Case 2:** ğ‘›= 5 and the roots are âˆ’11, âˆ’3, âˆ’1, 1, and 3. In this case, the sum of the roots is âˆ’11, and\nâˆ£ğ‘ğ‘›âˆ’1âˆ£= 22.\n* **Case 3:** ğ‘›= 7 and the roots are âˆ’11, âˆ’3, âˆ’1, 1, 3, 5, and 7. In this case, the sum of the roots is âˆ’11,\nand âˆ£ğ‘ğ‘›âˆ’1âˆ£= 22.\nWe can continue this pattern, but we notice that the absolute value of the sum of the roots keeps increasing.\nTherefore, the least possible value of âˆ£ğ‘ğ‘›âˆ’1âˆ£is 14 .\nFinal Answer: The final answer is 14. I hope it is correct.\nMATH Example 4\nProblem. Find the minimum value of\n(ğ‘¥+ 5)(ğ‘¥+ 2)\nğ‘¥+ 1\nfor ğ‘¥> 0.\nSCoRe turn 1 solution (incorrect). We can write\n(ğ‘¥+ 5)(ğ‘¥+ 2)\nğ‘¥+ 1\n= ğ‘¥2 + 7ğ‘¥+ 10\nğ‘¥+ 1\n= (ğ‘¥2 + 7ğ‘¥+ 10) âˆ’(ğ‘¥2 + ğ‘¥)\nğ‘¥+ 1\n= 6ğ‘¥+ 10\nğ‘¥+ 1\n= (6ğ‘¥+ 6) + 4\nğ‘¥+ 1\n= 6 +\n4\nğ‘¥+ 1.\nSince ğ‘¥> 0, ğ‘¥+ 1 > 1, so\n4\nğ‘¥+1 < 4. Therefore,\n(ğ‘¥+ 5)(ğ‘¥+ 2)\nğ‘¥+ 1\n< 6 + 4 = 10.\nEquality occurs when ğ‘¥= 0, so the minimum value is 6 .\nFinal Answer: The final answer is 6. I hope it is correct.\n26\nTraining Language Models to Self-Correct via Reinforcement Learning\nSCoRe turn 2 solution (correct). We can write\n(ğ‘¥+ 5)(ğ‘¥+ 2)\nğ‘¥+ 1\n= ğ‘¥2 + 7ğ‘¥+ 10\nğ‘¥+ 1\n= (ğ‘¥2 + ğ‘¥) + (6ğ‘¥+ 6) + 4\nğ‘¥+ 1\n= ğ‘¥(ğ‘¥+ 1) + 6(ğ‘¥+ 1) + 4\nğ‘¥+ 1\n= (ğ‘¥+ 1)(ğ‘¥+ 6) + 4\nğ‘¥+ 1\n= ğ‘¥+ 6 +\n4\nğ‘¥+ 1.\nSince ğ‘¥> 0, ğ‘¥+ 1 > 1, so\n4\nğ‘¥+1 < 4. Therefore,\n(ğ‘¥+ 5)(ğ‘¥+ 2)\nğ‘¥+ 1\n> ğ‘¥+ 6.\nSince ğ‘¥> 0, ğ‘¥+ 6 > 6. Thus, the minimum value is greater than 6.\nFurthermore, as ğ‘¥increases, the term\n4\nğ‘¥+1 decreases, so the expression (ğ‘¥+5)(ğ‘¥+2)\nğ‘¥+1\ndecreases. Therefore, the\nminimum value occurs when\n4\nğ‘¥+1 is as large as possible, which occurs when ğ‘¥is as small as possible. Since\nğ‘¥> 0, the minimum value is\n6 +\n4\n0 + 1 = 10.\nEquality occurs when ğ‘¥= 0, but this contradicts the condition ğ‘¥> 0. Therefore, the minimum value does\nnot exist.\nWe made an error in assuming that the minimum value occurs when\n4\nğ‘¥+1 is as large as possible. Instead,\nwe should minimize the entire expression. Since ğ‘¥> 0, the term\n4\nğ‘¥+1 is always positive. Therefore, the\nminimum value occurs when ğ‘¥+ 6 +\n4\nğ‘¥+1 is as small as possible.\nBy AM-GM,\nğ‘¥+ 6 +\n4\nğ‘¥+ 1 = (ğ‘¥+ 1) +\n4\nğ‘¥+ 1 + 5 â‰¥2\nâˆš\n(ğ‘¥+ 1) â‹…\n4\nğ‘¥+ 1 + 5 = 9.\nEquality occurs when (ğ‘¥+ 1) =\n4\nğ‘¥+1, so ğ‘¥2 + 2ğ‘¥+ 1 = 4, which means ğ‘¥2 + 2ğ‘¥âˆ’3 = 0. This factors as\n(ğ‘¥+ 3)(ğ‘¥âˆ’1) = 0, so ğ‘¥= 1 (since ğ‘¥> 0).\nTherefore, the minimum value is 9 .\nFinal Answer: The final answer is 9. I hope it is correct.\nMATH Example 5\nProblem. Four points, ğ´, ğµ, ğ¶, and ğ·, are chosen randomly and independently on the circumference of a\ncircle. What is the probability that segments ğ´ğµand ğ¶ğ·intersect?\nSCoRe turn 1 solution (incorrect). The segments ğ´ğµand ğ¶ğ·will intersect if and only if the points ğ´, ğµ, ğ¶,\nand ğ·appear in alternating order around the circle. There are 4! ways to order the points, and 2 ways to\ndivide the points into two pairs. Therefore, the probability that the segments intersect is 2\n4! =\n2\n24 =\n1\n12.\nFinal Answer: The final answer is\n1\n12. I hope it is correct.\nSCoRe turn 2 solution (correct). The segments ğ´ğµand ğ¶ğ·will intersect if and only if the points ğ´, ğµ, ğ¶,\n27\nTraining Language Models to Self-Correct via Reinforcement Learning\nand ğ·appear in alternating order around the circle. Without loss of generality, letâ€™s assume that ğ´is the\nfirst point chosen. Then, there are three possible cases for the placement of ğµ:\n* **Case 1:** ğµis placed in the arc between ğ´and ğ¶. In this case, ğ¶and ğ·can be placed anywhere on the\nremaining circumference, and the segments will not intersect. * **Case 2:** ğµis placed in the arc between\nğ´and ğ·. In this case, ğ¶and ğ·can be placed anywhere on the remaining circumference, and the segments\nwill not intersect. * **Case 3:** ğµis placed in the arc between ğ¶and ğ·. In this case, the segments ğ´ğµand\nğ¶ğ·will intersect.\nThe probability of Case 3 is 1\n3. Since the points are chosen independently, the probability that the segments\nintersect is 1\n3 .\nFinal Answer: The final answer is 1\n3. I hope it is correct.\nMATH Example 6\nProblem. Let ğ‘“(ğ‘¥) = âŒŠ(âˆ’5\n8)\nğ‘¥âŒ‹be a function that is defined for all values of ğ‘¥in [0, âˆ) such that ğ‘“(ğ‘¥) is a\nreal number. How many distinct values exist in the range of ğ‘“(ğ‘¥)?\nSCoRe turn 1 solution (incorrect). As ğ‘¥increases, the value of (âˆ’5\n8)\nğ‘¥decreases. Since the floor function\nrounds down to the nearest integer, the range of ğ‘“(ğ‘¥) consists of all the integers less than or equal to\nâŒŠ(âˆ’5\n8)\n0âŒ‹= 1. Therefore, the range of ğ‘“(ğ‘¥) is {1, 0, âˆ’1, âˆ’2, . . . }, and there are âˆdistinct values in the\nrange.\nFinal Answer: The final answer is âˆ. I hope it is correct.\nSCoRe turn 2 solution (correct). As ğ‘¥increases from 0 to âˆ, the value of (âˆ’5\n8)\nğ‘¥oscillates between positive\nand negative values, but its absolute value decreases. The floor function rounds down to the nearest integer,\nso the range of ğ‘“(ğ‘¥) consists of the integers less than or equal to âŒŠ(âˆ’5\n8)\n0âŒ‹= 1. Since the absolute value of\n(âˆ’5\n8)\nğ‘¥decreases as ğ‘¥increases, the range of ğ‘“(ğ‘¥) is finite and consists of the integers: {1, 0, âˆ’1}.\nFinal Answer: The final answer is 3. I hope it is correct.\nMATH Example 7\nProblem. The 7th and 8th grades have enrollments of 520 and 650 respectively. The two grades have a\ntotal of 18 representatives on the Student Council. How many representatives should the 8th grade have if\nthere is to be a fair representation of the two grades?\nSCoRe turn 1 solution (incorrect). The total enrollment of the two grades is 520 + 650 = 1170. The ratio\nof 7th grade students to 8th grade students is 520/1170 = 52/117. The ratio of 7th grade representatives\nto 8th grade representatives should be the same as the ratio of 7th grade students to 8th grade students.\nLet x be the number of representatives for the 8th grade. Then the number of representatives for the 7th\ngrade is 18 - x. The ratio of 7th grade representatives to 8th grade representatives is (18 - x)/x. Setting the\ntwo ratios equal, we have (18 - x)/x = 52/117. Cross-multiplying, we get 117(18 - x) = 52x. Expanding,\nwe get 2106 - 117x = 52x. Combining like terms, we get 2106 = 169x. Solving for x, we get x = 2106/169\n= 12.47. Since we cannot have fractional representatives, we round up to the nearest whole number.\nFinal Answer: The final answer is 13. I hope it is correct.\nSCoRe turn 2 solution (correct). The ratio of 7th grade students to 8th grade students is 520/650 =\n4/5. Since the two grades should have a fair representation on the Student Council, the ratio of 7th grade\n28\nTraining Language Models to Self-Correct via Reinforcement Learning\nrepresentatives to 8th grade representatives should be the same as the ratio of 7th grade students to 8th\ngrade students, which is 4/5. Let x be the number of representatives for the 8th grade. Then the number of\nrepresentatives for the 7th grade is 18 - x. The ratio of 7th grade representatives to 8th grade representatives\nis (18 - x)/x. Setting the two ratios equal, we have (18 - x)/x = 4/5. Cross-multiplying, we get 5(18 - x) =\n4x. Expanding, we get 90 - 5x = 4x. Combining like terms, we get 90 = 9x. Solving for x, we get x = 10.\nFinal Answer: The final answer is 10. I hope it is correct.\nMATH Example 8\nProblem. The sum of 27 consecutive positive integers is 37. What is their median?\nSCoRe turn 1 solution (incorrect). The sum of an arithmetic series is equal to the average of the first and\nlast term, multiplied by the number of terms. Let the first term of the arithmetic series be ğ‘, and the last\nterm be ğ‘™. Then the sum of the series is (ğ‘+ğ‘™)â‹…27\n2\n. Since the sum of the series is 37, we have (ğ‘+ğ‘™)â‹…27\n2\n= 37.\nSimplifying, we get (ğ‘+ ğ‘™) = 37â‹…2\n27 = 34 â‹…2 = 162. Since the series consists of consecutive integers, the\nmedian of the series is the middle term, which is the 14th term. The 14th term of the series is ğ‘+ 13, and\nthe average of the first and last term is ğ‘+ğ‘™\n2 . Therefore, the median of the series is ğ‘+ğ‘™\n2 + 13\n2 = 162\n2 + 13\n2 = 175\n2 .\nFinal Answer: The final answer is 87.5. I hope it is correct.\nSCoRe turn 2 solution (correct). The sum of an arithmetic series is equal to the average of the first and\nlast term, multiplied by the number of terms. Let the first term of the arithmetic series be ğ‘, and the last\nterm be ğ‘™. Then the sum of the series is (ğ‘+ğ‘™)â‹…27\n2\n. Since the sum of the series is 37, we have (ğ‘+ğ‘™)â‹…27\n2\n= 37.\nSimplifying, we get (ğ‘+ ğ‘™) = 37â‹…2\n27 = 37â‹…2\n33 = 34 â‹…2 = 162.\nSince the series consists of 27 consecutive integers, the average of the first and last term is equal to the\nmedian of the series. Therefore, the median of the series is ğ‘+ğ‘™\n2 = 162\n2\n= 81.\nFinal Answer: The final answer is 81. I hope it is correct.\n29\n",
    "file_path": "./emails\\CAS-16378-S4ER66.pdf",
    "classification": {
      "level1": "è½¯ä»¶é—®é¢˜",
      "level2": "æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–",
      "level3": "è‡ªæˆ‘çº é”™èƒ½åŠ›æå‡"
    },
    "category_score": {
      "level1": 0.9,
      "level2": 0.0,
      "level3": 0.0
    },
    "reasoning": "é‚®ä»¶å†…å®¹ä¸»è¦è®¨è®ºäº†é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘çº é”™èƒ½åŠ›ï¼Œå±äºè½¯ä»¶é—®é¢˜ä¸­çš„æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–èŒƒç•´ã€‚ç°æœ‰åˆ†ç±»ç»“æ„ä¸­æ²¡æœ‰ç›´æ¥åŒ¹é…çš„äºŒçº§å’Œä¸‰çº§åˆ†ç±»ï¼Œå› æ­¤åˆ›å»ºæ–°çš„äºŒçº§åˆ†ç±»'æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–'å’Œä¸‰çº§åˆ†ç±»'è‡ªæˆ‘çº é”™èƒ½åŠ›æå‡'ã€‚"
  }
]